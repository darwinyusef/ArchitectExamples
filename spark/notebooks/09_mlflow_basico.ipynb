{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 09 - MLflow B√°sico: Tracking y Experiment Management\n",
    "\n",
    "## üéØ Objetivos\n",
    "- Aprender los fundamentos de MLflow\n",
    "- Tracking de experimentos y m√©tricas\n",
    "- Logging de par√°metros, m√©tricas y artefactos\n",
    "- Comparaci√≥n de modelos\n",
    "- Uso de MLflow UI\n",
    "\n",
    "## üìö Tecnolog√≠as\n",
    "- **MLflow**: Experiment tracking, model registry\n",
    "- **Scikit-learn**: Modelos b√°sicos\n",
    "- **Pandas**: Manipulaci√≥n de datos\n",
    "\n",
    "## ‚≠ê Complejidad: B√°sico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Instalaci√≥n y Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar dependencias\n",
    "!pip install mlflow scikit-learn pandas numpy matplotlib seaborn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification, make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"‚úÖ MLflow version: {mlflow.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuraci√≥n de MLflow\n",
    "\n",
    "MLflow guarda los experimentos en un directorio local por defecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar tracking URI (local)\n",
    "mlflow.set_tracking_uri(\"./mlruns\")\n",
    "\n",
    "# Crear o establecer experimento\n",
    "experiment_name = \"mlflow_basico_tutorial\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "print(f\"üìä Experimento: {experiment_name}\")\n",
    "print(f\"üìÅ Tracking URI: {mlflow.get_tracking_uri()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Crear Dataset de Ejemplo\n",
    "\n",
    "Generamos un dataset sint√©tico de clasificaci√≥n binaria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar datos sint√©ticos\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=20,\n",
    "    n_informative=15,\n",
    "    n_redundant=5,\n",
    "    n_classes=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Split train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"üìä Train set: {X_train.shape}\")\n",
    "print(f\"üìä Test set: {X_test.shape}\")\n",
    "print(f\"üìä Class distribution: {np.bincount(y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ejemplo B√°sico: Un Solo Experimento\n",
    "\n",
    "Veamos c√≥mo trackear un experimento simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iniciar un run de MLflow\n",
    "with mlflow.start_run(run_name=\"logistic_regression_basic\"):\n",
    "    \n",
    "    # 1. Log de par√°metros\n",
    "    mlflow.log_param(\"model_type\", \"LogisticRegression\")\n",
    "    mlflow.log_param(\"max_iter\", 100)\n",
    "    mlflow.log_param(\"random_state\", 42)\n",
    "    mlflow.log_param(\"test_size\", 0.2)\n",
    "    \n",
    "    # 2. Entrenar modelo\n",
    "    model = LogisticRegression(max_iter=100, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # 3. Predicciones\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # 4. Calcular m√©tricas\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # 5. Log de m√©tricas\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    mlflow.log_metric(\"precision\", precision)\n",
    "    mlflow.log_metric(\"recall\", recall)\n",
    "    mlflow.log_metric(\"f1_score\", f1)\n",
    "    \n",
    "    # 6. Log del modelo\n",
    "    mlflow.sklearn.log_model(model, \"model\")\n",
    "    \n",
    "    # 7. Crear y guardar un artefacto (gr√°fico)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(range(len(y_test)), y_test, alpha=0.5, label='Real')\n",
    "    plt.scatter(range(len(y_pred)), y_pred, alpha=0.5, label='Predicci√≥n')\n",
    "    plt.xlabel('Muestras')\n",
    "    plt.ylabel('Clase')\n",
    "    plt.title('Predicciones vs Real')\n",
    "    plt.legend()\n",
    "    plt.savefig('predictions.png')\n",
    "    mlflow.log_artifact('predictions.png')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"‚úÖ Experimento completado!\")\n",
    "    print(f\"üìä Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"üìä Precision: {precision:.4f}\")\n",
    "    print(f\"üìä Recall: {recall:.4f}\")\n",
    "    print(f\"üìä F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparaci√≥n de M√∫ltiples Modelos\n",
    "\n",
    "Entrenemos varios modelos y comparemos sus resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir modelos a comparar\n",
    "models = {\n",
    "    \"LogisticRegression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"RandomForest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "# Entrenar y trackear cada modelo\n",
    "for model_name, model in models.items():\n",
    "    with mlflow.start_run(run_name=f\"{model_name}_comparison\"):\n",
    "        \n",
    "        # Log par√°metros\n",
    "        mlflow.log_param(\"model_type\", model_name)\n",
    "        mlflow.log_param(\"dataset_size\", len(X_train))\n",
    "        \n",
    "        # Entrenar\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predecir\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # M√©tricas\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        \n",
    "        # Log m√©tricas\n",
    "        mlflow.log_metric(\"accuracy\", accuracy)\n",
    "        mlflow.log_metric(\"precision\", precision)\n",
    "        mlflow.log_metric(\"recall\", recall)\n",
    "        mlflow.log_metric(\"f1_score\", f1)\n",
    "        \n",
    "        # Log modelo\n",
    "        mlflow.sklearn.log_model(model, \"model\")\n",
    "        \n",
    "        # Guardar resultados\n",
    "        results.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Accuracy\": accuracy,\n",
    "            \"Precision\": precision,\n",
    "            \"Recall\": recall,\n",
    "            \"F1-Score\": f1\n",
    "        })\n",
    "        \n",
    "        print(f\"‚úÖ {model_name} - Accuracy: {accuracy:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "# Mostrar tabla comparativa\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nüìä Comparaci√≥n de Modelos:\")\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Logging de Artefactos Complejos\n",
    "\n",
    "Guardemos visualizaciones y archivos adicionales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"artifacts_example\"):\n",
    "    \n",
    "    # Entrenar modelo\n",
    "    model = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # M√©tricas\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    \n",
    "    # 1. Guardar importancia de features\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': [f'feature_{i}' for i in range(X_train.shape[1])],\n",
    "        'importance': model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Gr√°fico de importancia\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(feature_importance['feature'][:10], feature_importance['importance'][:10])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title('Top 10 Feature Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('feature_importance.png')\n",
    "    mlflow.log_artifact('feature_importance.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Guardar CSV con importancia\n",
    "    feature_importance.to_csv('feature_importance.csv', index=False)\n",
    "    mlflow.log_artifact('feature_importance.csv')\n",
    "    \n",
    "    # 3. Guardar matriz de confusi√≥n\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('Real')\n",
    "    plt.xlabel('Predicci√≥n')\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    mlflow.log_artifact('confusion_matrix.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 4. Guardar metadata como JSON\n",
    "    import json\n",
    "    metadata = {\n",
    "        \"model\": \"RandomForestClassifier\",\n",
    "        \"n_estimators\": 50,\n",
    "        \"train_size\": len(X_train),\n",
    "        \"test_size\": len(X_test),\n",
    "        \"accuracy\": float(accuracy),\n",
    "        \"features\": X_train.shape[1]\n",
    "    }\n",
    "    \n",
    "    with open('metadata.json', 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    mlflow.log_artifact('metadata.json')\n",
    "    \n",
    "    print(\"‚úÖ Todos los artefactos guardados!\")\n",
    "    print(f\"üìä Top 5 features m√°s importantes:\")\n",
    "    print(feature_importance.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Uso de Tags y B√∫squeda de Experimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimento con tags\n",
    "with mlflow.start_run(run_name=\"tagged_experiment\") as run:\n",
    "    \n",
    "    # Agregar tags\n",
    "    mlflow.set_tag(\"team\", \"data-science\")\n",
    "    mlflow.set_tag(\"project\", \"tutorial-basico\")\n",
    "    mlflow.set_tag(\"environment\", \"development\")\n",
    "    mlflow.set_tag(\"version\", \"1.0\")\n",
    "    \n",
    "    # Entrenar modelo simple\n",
    "    model = LogisticRegression(random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Log m√©tricas\n",
    "    accuracy = accuracy_score(y_test, model.predict(X_test))\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    \n",
    "    # Guardar modelo\n",
    "    mlflow.sklearn.log_model(model, \"model\")\n",
    "    \n",
    "    print(f\"‚úÖ Run ID: {run.info.run_id}\")\n",
    "    print(f\"üìä Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. B√∫squeda y Recuperaci√≥n de Experimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buscar todos los runs del experimento actual\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "experiment = client.get_experiment_by_name(experiment_name)\n",
    "\n",
    "# Obtener todos los runs\n",
    "runs = client.search_runs(\n",
    "    experiment_ids=[experiment.experiment_id],\n",
    "    order_by=[\"metrics.accuracy DESC\"],\n",
    "    max_results=10\n",
    ")\n",
    "\n",
    "print(f\"üìä Total de runs encontrados: {len(runs)}\\n\")\n",
    "\n",
    "# Mostrar informaci√≥n de los mejores runs\n",
    "for i, run in enumerate(runs[:5], 1):\n",
    "    print(f\"\\n{i}. Run: {run.info.run_name}\")\n",
    "    print(f\"   Run ID: {run.info.run_id}\")\n",
    "    print(f\"   Status: {run.info.status}\")\n",
    "    \n",
    "    # Par√°metros\n",
    "    if run.data.params:\n",
    "        print(f\"   Par√°metros:\")\n",
    "        for key, value in run.data.params.items():\n",
    "            print(f\"      - {key}: {value}\")\n",
    "    \n",
    "    # M√©tricas\n",
    "    if run.data.metrics:\n",
    "        print(f\"   M√©tricas:\")\n",
    "        for key, value in run.data.metrics.items():\n",
    "            print(f\"      - {key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Cargar y Usar un Modelo Guardado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener el mejor run (por accuracy)\n",
    "best_run = runs[0]\n",
    "best_run_id = best_run.info.run_id\n",
    "\n",
    "print(f\"üèÜ Mejor modelo:\")\n",
    "print(f\"   Run ID: {best_run_id}\")\n",
    "print(f\"   Run Name: {best_run.info.run_name}\")\n",
    "print(f\"   Accuracy: {best_run.data.metrics.get('accuracy', 0):.4f}\")\n",
    "\n",
    "# Cargar el modelo\n",
    "model_uri = f\"runs:/{best_run_id}/model\"\n",
    "loaded_model = mlflow.sklearn.load_model(model_uri)\n",
    "\n",
    "print(f\"\\n‚úÖ Modelo cargado exitosamente!\")\n",
    "print(f\"üìä Tipo de modelo: {type(loaded_model).__name__}\")\n",
    "\n",
    "# Hacer predicciones con el modelo cargado\n",
    "predictions = loaded_model.predict(X_test[:5])\n",
    "print(f\"\\nüîÆ Predicciones de ejemplo: {predictions}\")\n",
    "print(f\"üìå Valores reales: {y_test[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Registro de Modelos (Model Registry)\n",
    "\n",
    "MLflow permite registrar modelos para producci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Registrar el mejor modelo\n",
    "model_name = \"tutorial_best_classifier\"\n",
    "\n",
    "try:\n",
    "    # Registrar modelo desde el run\n",
    "    model_uri = f\"runs:/{best_run_id}/model\"\n",
    "    registered_model = mlflow.register_model(model_uri, model_name)\n",
    "    \n",
    "    print(f\"‚úÖ Modelo registrado!\")\n",
    "    print(f\"üì¶ Nombre: {registered_model.name}\")\n",
    "    print(f\"üìå Versi√≥n: {registered_model.version}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Nota: {e}\")\n",
    "    print(f\"üí° El registro de modelos requiere un backend remoto (MySQL, PostgreSQL, etc.)\")\n",
    "    print(f\"üí° Para uso local, puedes cargar modelos directamente con el run_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Visualizaci√≥n con MLflow UI\n",
    "\n",
    "Para ver todos tus experimentos en una interfaz web, ejecuta en la terminal:\n",
    "\n",
    "```bash\n",
    "mlflow ui --port 5000\n",
    "```\n",
    "\n",
    "Luego abre en tu navegador: http://localhost:5000\n",
    "\n",
    "### Caracter√≠sticas de MLflow UI:\n",
    "- üìä Comparaci√≥n visual de experimentos\n",
    "- üìà Gr√°ficos de m√©tricas\n",
    "- üîç B√∫squeda y filtrado avanzado\n",
    "- üìÅ Navegaci√≥n de artefactos\n",
    "- üè∑Ô∏è Gesti√≥n de tags\n",
    "- üì¶ Registro de modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Resumen y Mejores Pr√°cticas\n",
    "\n",
    "### ‚úÖ Conceptos Clave:\n",
    "1. **Experiments**: Agrupan runs relacionados\n",
    "2. **Runs**: Una ejecuci√≥n individual de c√≥digo ML\n",
    "3. **Parameters**: Configuraci√≥n del modelo (hiperpar√°metros)\n",
    "4. **Metrics**: Resultados cuantitativos (accuracy, loss, etc.)\n",
    "5. **Artifacts**: Archivos generados (modelos, gr√°ficos, datos)\n",
    "6. **Tags**: Metadata adicional para organizaci√≥n\n",
    "\n",
    "### üí° Mejores Pr√°cticas:\n",
    "- ‚úÖ Usa nombres descriptivos para runs y experimentos\n",
    "- ‚úÖ Loggea todos los par√°metros relevantes\n",
    "- ‚úÖ Registra m√∫ltiples m√©tricas para an√°lisis completo\n",
    "- ‚úÖ Guarda artefactos importantes (modelos, gr√°ficos)\n",
    "- ‚úÖ Usa tags para organizar experimentos por proyecto/equipo\n",
    "- ‚úÖ Compara m√∫ltiples modelos en el mismo experimento\n",
    "- ‚úÖ Documenta tus experimentos con tags y descripciones\n",
    "\n",
    "### üöÄ Pr√≥ximos Pasos:\n",
    "- Integrar con frameworks de deep learning (PyTorch, TensorFlow)\n",
    "- Configurar backend remoto para Model Registry\n",
    "- Automatizar experimentos con Airflow\n",
    "- Deployar modelos con MLflow Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéâ Tutorial de MLflow B√°sico completado!\")\n",
    "print(f\"\\nüìä Estad√≠sticas finales:\")\n",
    "print(f\"   - Experimento: {experiment_name}\")\n",
    "print(f\"   - Total de runs: {len(runs)}\")\n",
    "print(f\"   - Mejor accuracy: {runs[0].data.metrics.get('accuracy', 0):.4f}\")\n",
    "print(f\"\\nüíª Para ver la UI ejecuta: mlflow ui --port 5000\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
