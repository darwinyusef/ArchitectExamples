{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ Tutorial Completo: Spark + Airflow + MLflow\n",
    "\n",
    "Este notebook te guiarÃ¡ paso a paso en el uso del pipeline de ML usando Spark para procesamiento, Airflow para orquestaciÃ³n y MLflow para tracking.\n",
    "\n",
    "## ğŸ“‹ Contenido\n",
    "1. [Setup Inicial](#setup)\n",
    "2. [IntroducciÃ³n a Spark](#spark-intro)\n",
    "3. [Procesamiento de Datos con Spark](#spark-processing)\n",
    "4. [Entrenamiento de Modelos con Spark MLlib](#spark-ml)\n",
    "5. [Tracking con MLflow](#mlflow)\n",
    "6. [OrquestaciÃ³n con Airflow](#airflow)\n",
    "7. [Pipeline Completo End-to-End](#pipeline)\n",
    "8. [Monitoreo y Debugging](#monitoring)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Inicial <a id=\"setup\"></a>\n",
    "\n",
    "### Verificar que los servicios estÃ©n corriendo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Verificar servicios\n",
    "services = {\n",
    "    \"Airflow\": \"http://localhost:8080/health\",\n",
    "    \"MLflow\": \"http://localhost:5000/health\",\n",
    "    \"Spark Master\": \"http://localhost:8081\"\n",
    "}\n",
    "\n",
    "print(\"ğŸ” Verificando servicios...\\n\")\n",
    "for name, url in services.items():\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        status = \"âœ… OK\" if response.status_code == 200 else f\"âš ï¸ {response.status_code}\"\n",
    "        print(f\"{name}: {status}\")\n",
    "    except Exception as e:\n",
    "        print(f\"{name}: âŒ No disponible - {str(e)[:50]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalar dependencias necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar paquetes si no estÃ¡n instalados\n",
    "!pip install -q pyspark mlflow pandas numpy matplotlib seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importar librerÃ­as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports principales\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.regression import LinearRegression, RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from requests.auth import HTTPBasicAuth\n",
    "\n",
    "# ConfiguraciÃ³n de visualizaciÃ³n\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"âœ… LibrerÃ­as importadas correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. IntroducciÃ³n a Spark <a id=\"spark-intro\"></a>\n",
    "\n",
    "### Â¿QuÃ© es Apache Spark?\n",
    "\n",
    "Apache Spark es un motor de procesamiento distribuido diseÃ±ado para:\n",
    "- **Procesamiento en memoria**: 100x mÃ¡s rÃ¡pido que MapReduce\n",
    "- **Escalabilidad**: De laptop a cluster de miles de nodos\n",
    "- **APIs unificadas**: Batch, streaming, ML, grafos\n",
    "- **Multi-lenguaje**: Scala, Java, Python, R\n",
    "\n",
    "### Arquitectura de Spark\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚          Driver Program                 â”‚\n",
    "â”‚     (SparkContext/SparkSession)         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "             â”‚\n",
    "             â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚        Cluster Manager                  â”‚\n",
    "â”‚  (Standalone / YARN / Kubernetes)       â”‚\n",
    "â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "     â”‚           â”‚            â”‚\n",
    "     â–¼           â–¼            â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚Worker   â”‚ â”‚Worker   â”‚ â”‚Worker   â”‚\n",
    "â”‚Executor â”‚ â”‚Executor â”‚ â”‚Executor â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Crear SesiÃ³n de Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Tutorial-Spark-Airflow\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"âœ… Spark Session creada\")\n",
    "print(f\"   Spark Version: {spark.version}\")\n",
    "print(f\"   Master: {spark.sparkContext.master}\")\n",
    "print(f\"   App Name: {spark.sparkContext.appName}\")\n",
    "print(f\"\\n   Spark UI: http://localhost:4040\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Procesamiento de Datos con Spark <a id=\"spark-processing\"></a>\n",
    "\n",
    "### 3.1 Crear Dataset de Ejemplo\n",
    "\n",
    "Vamos a crear un dataset de ventas simulado para demostrar las capacidades de Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar datos de ejemplo con pandas\n",
    "np.random.seed(42)\n",
    "\n",
    "n_records = 10000\n",
    "\n",
    "data = {\n",
    "    'fecha': pd.date_range('2023-01-01', periods=n_records, freq='H'),\n",
    "    'producto_id': np.random.randint(1, 101, n_records),\n",
    "    'categoria': np.random.choice(['ElectrÃ³nica', 'Ropa', 'Alimentos', 'Hogar'], n_records),\n",
    "    'cantidad': np.random.randint(1, 50, n_records),\n",
    "    'precio_unitario': np.random.uniform(10, 1000, n_records).round(2),\n",
    "    'descuento': np.random.choice([0, 5, 10, 15, 20], n_records),\n",
    "    'cliente_id': np.random.randint(1, 1001, n_records),\n",
    "    'region': np.random.choice(['Norte', 'Sur', 'Este', 'Oeste'], n_records)\n",
    "}\n",
    "\n",
    "df_pandas = pd.DataFrame(data)\n",
    "\n",
    "# Calcular columnas derivadas\n",
    "df_pandas['precio_con_descuento'] = df_pandas['precio_unitario'] * (1 - df_pandas['descuento']/100)\n",
    "df_pandas['total'] = df_pandas['cantidad'] * df_pandas['precio_con_descuento']\n",
    "df_pandas['mes'] = df_pandas['fecha'].dt.month\n",
    "df_pandas['dia_semana'] = df_pandas['fecha'].dt.dayofweek\n",
    "df_pandas['hora'] = df_pandas['fecha'].dt.hour\n",
    "\n",
    "print(f\"âœ… Dataset creado: {len(df_pandas):,} registros\")\n",
    "print(f\"   Rango de fechas: {df_pandas['fecha'].min()} a {df_pandas['fecha'].max()}\")\n",
    "print(f\"   Total de ventas: ${df_pandas['total'].sum():,.2f}\")\n",
    "\n",
    "df_pandas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Convertir a Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir pandas DataFrame a Spark DataFrame\n",
    "df_spark = spark.createDataFrame(df_pandas)\n",
    "\n",
    "print(\"âœ… DataFrame de Spark creado\")\n",
    "print(f\"   Particiones: {df_spark.rdd.getNumPartitions()}\")\n",
    "print(f\"   Registros: {df_spark.count():,}\")\n",
    "\n",
    "# Mostrar esquema\n",
    "print(\"\\nğŸ“‹ Esquema del DataFrame:\")\n",
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar primeras filas\n",
    "print(\"ğŸ“Š Primeras 5 filas:\")\n",
    "df_spark.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Operaciones BÃ¡sicas con Spark\n",
    "\n",
    "#### Filtrado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar ventas con descuento > 10%\n",
    "ventas_descuento = df_spark.filter(df_spark.descuento > 10)\n",
    "\n",
    "print(f\"ğŸ“Œ Ventas con descuento > 10%: {ventas_descuento.count():,}\")\n",
    "print(f\"   Porcentaje: {ventas_descuento.count() / df_spark.count() * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agregaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Ventas por categorÃ­a\n",
    "ventas_por_categoria = df_spark.groupBy('categoria').agg(\n",
    "    F.count('*').alias('num_ventas'),\n",
    "    F.sum('total').alias('total_ventas'),\n",
    "    F.avg('total').alias('promedio_venta'),\n",
    "    F.min('total').alias('min_venta'),\n",
    "    F.max('total').alias('max_venta')\n",
    ").orderBy(F.desc('total_ventas'))\n",
    "\n",
    "print(\"ğŸ“Š Ventas por CategorÃ­a:\")\n",
    "ventas_por_categoria.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear tabla de productos\n",
    "productos_data = [\n",
    "    (1, 'Laptop', 'ElectrÃ³nica', 'Premium'),\n",
    "    (2, 'Mouse', 'ElectrÃ³nica', 'BÃ¡sico'),\n",
    "    (3, 'Camisa', 'Ropa', 'BÃ¡sico'),\n",
    "    (4, 'Arroz', 'Alimentos', 'BÃ¡sico'),\n",
    "    (5, 'Refrigerador', 'Hogar', 'Premium')\n",
    "]\n",
    "\n",
    "productos_schema = ['producto_id', 'nombre', 'categoria', 'segmento']\n",
    "df_productos = spark.createDataFrame(productos_data, productos_schema)\n",
    "\n",
    "# Join con tabla de productos (solo para demostraciÃ³n con IDs 1-5)\n",
    "df_con_productos = df_spark.filter(F.col('producto_id') <= 5) \\\n",
    "    .join(df_productos, 'producto_id', 'left')\n",
    "\n",
    "print(\"ğŸ”— DataFrame con Join:\")\n",
    "df_con_productos.select('producto_id', 'nombre', 'cantidad', 'total', 'segmento').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Window Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Top 3 ventas por categorÃ­a\n",
    "window_spec = Window.partitionBy('categoria').orderBy(F.desc('total'))\n",
    "\n",
    "df_top_ventas = df_spark.withColumn('rank', F.row_number().over(window_spec)) \\\n",
    "    .filter(F.col('rank') <= 3) \\\n",
    "    .select('categoria', 'producto_id', 'total', 'rank') \\\n",
    "    .orderBy('categoria', 'rank')\n",
    "\n",
    "print(\"ğŸ† Top 3 Ventas por CategorÃ­a:\")\n",
    "df_top_ventas.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 AnÃ¡lisis Temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ventas por mes\n",
    "ventas_mensuales = df_spark.groupBy('mes').agg(\n",
    "    F.sum('total').alias('total_mes'),\n",
    "    F.count('*').alias('num_transacciones'),\n",
    "    F.avg('total').alias('ticket_promedio')\n",
    ").orderBy('mes')\n",
    "\n",
    "print(\"ğŸ“… Ventas Mensuales:\")\n",
    "ventas_mensuales.show(12)\n",
    "\n",
    "# Convertir a pandas para visualizaciÃ³n\n",
    "ventas_mensuales_pd = ventas_mensuales.toPandas()\n",
    "\n",
    "# Visualizar\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "axes[0].bar(ventas_mensuales_pd['mes'], ventas_mensuales_pd['total_mes'])\n",
    "axes[0].set_xlabel('Mes')\n",
    "axes[0].set_ylabel('Total Ventas ($)')\n",
    "axes[0].set_title('Ventas Totales por Mes')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].plot(ventas_mensuales_pd['mes'], ventas_mensuales_pd['ticket_promedio'], marker='o')\n",
    "axes[1].set_xlabel('Mes')\n",
    "axes[1].set_ylabel('Ticket Promedio ($)')\n",
    "axes[1].set_title('Ticket Promedio por Mes')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Guardar Datos Procesados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear directorio de datos si no existe\n",
    "!mkdir -p /tmp/spark_data\n",
    "\n",
    "# Guardar en formato Parquet (optimizado)\n",
    "output_path = \"/tmp/spark_data/ventas_procesadas.parquet\"\n",
    "df_spark.write.mode('overwrite').parquet(output_path)\n",
    "\n",
    "print(f\"âœ… Datos guardados en: {output_path}\")\n",
    "\n",
    "# Verificar que se guardÃ³ correctamente\n",
    "df_leido = spark.read.parquet(output_path)\n",
    "print(f\"   Registros leÃ­dos: {df_leido.count():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Entrenamiento de Modelos con Spark MLlib <a id=\"spark-ml\"></a>\n",
    "\n",
    "### 4.1 PreparaciÃ³n de Datos para ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear features para predecir el total de venta\n",
    "# Features: cantidad, precio_unitario, descuento, mes, dia_semana, hora\n",
    "\n",
    "# Primero, convertir categorÃ­as a numÃ©ricas\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "categoria_indexer = StringIndexer(inputCol='categoria', outputCol='categoria_idx')\n",
    "region_indexer = StringIndexer(inputCol='region', outputCol='region_idx')\n",
    "\n",
    "# Aplicar indexers\n",
    "df_ml = categoria_indexer.fit(df_spark).transform(df_spark)\n",
    "df_ml = region_indexer.fit(df_ml).transform(df_ml)\n",
    "\n",
    "# Seleccionar features numÃ©ricas\n",
    "feature_cols = [\n",
    "    'cantidad', 'precio_unitario', 'descuento', \n",
    "    'mes', 'dia_semana', 'hora',\n",
    "    'categoria_idx', 'region_idx'\n",
    "]\n",
    "\n",
    "# Ensamblar features en un vector\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol='features_raw')\n",
    "df_ml = assembler.transform(df_ml)\n",
    "\n",
    "# Escalar features\n",
    "scaler = StandardScaler(inputCol='features_raw', outputCol='features', withMean=True, withStd=True)\n",
    "scaler_model = scaler.fit(df_ml)\n",
    "df_ml = scaler_model.transform(df_ml)\n",
    "\n",
    "# Seleccionar solo lo necesario\n",
    "df_ml = df_ml.select('features', F.col('total').alias('label'))\n",
    "\n",
    "print(\"âœ… Features preparadas\")\n",
    "print(\"\\nğŸ“Š Muestra de datos:\")\n",
    "df_ml.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Dividir en Train/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split 80/20\n",
    "train_data, test_data = df_ml.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(f\"ğŸ“Š DivisiÃ³n de datos:\")\n",
    "print(f\"   Train: {train_data.count():,} registros\")\n",
    "print(f\"   Test:  {test_data.count():,} registros\")\n",
    "\n",
    "# Cache para mejor rendimiento\n",
    "train_data.cache()\n",
    "test_data.cache()\n",
    "\n",
    "print(\"\\nâœ… Datos cacheados en memoria\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Entrenar Modelo - RegresiÃ³n Lineal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear modelo de regresiÃ³n lineal\n",
    "lr = LinearRegression(\n",
    "    featuresCol='features',\n",
    "    labelCol='label',\n",
    "    maxIter=10,\n",
    "    regParam=0.01,\n",
    "    elasticNetParam=0.0\n",
    ")\n",
    "\n",
    "print(\"ğŸ”„ Entrenando modelo de RegresiÃ³n Lineal...\")\n",
    "lr_model = lr.fit(train_data)\n",
    "\n",
    "print(\"\\nâœ… Modelo entrenado\")\n",
    "print(f\"   Coeficientes: {len(lr_model.coefficients)}\")\n",
    "print(f\"   Intercepto: {lr_model.intercept:.2f}\")\n",
    "print(f\"   RMSE (train): {lr_model.summary.rootMeanSquaredError:.2f}\")\n",
    "print(f\"   RÂ² (train): {lr_model.summary.r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Evaluar Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predecir en test set\n",
    "predictions_lr = lr_model.transform(test_data)\n",
    "\n",
    "# Evaluar\n",
    "evaluator_rmse = RegressionEvaluator(labelCol='label', predictionCol='prediction', metricName='rmse')\n",
    "evaluator_r2 = RegressionEvaluator(labelCol='label', predictionCol='prediction', metricName='r2')\n",
    "evaluator_mae = RegressionEvaluator(labelCol='label', predictionCol='prediction', metricName='mae')\n",
    "\n",
    "rmse_lr = evaluator_rmse.evaluate(predictions_lr)\n",
    "r2_lr = evaluator_r2.evaluate(predictions_lr)\n",
    "mae_lr = evaluator_mae.evaluate(predictions_lr)\n",
    "\n",
    "print(\"ğŸ“Š MÃ©tricas del Modelo - RegresiÃ³n Lineal:\")\n",
    "print(f\"   RMSE: {rmse_lr:,.2f}\")\n",
    "print(f\"   MAE:  {mae_lr:,.2f}\")\n",
    "print(f\"   RÂ²:   {r2_lr:.4f}\")\n",
    "\n",
    "# Mostrar algunas predicciones\n",
    "print(\"\\nğŸ”® Predicciones vs Real:\")\n",
    "predictions_lr.select('label', 'prediction').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Entrenar Modelo - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear modelo Random Forest\n",
    "rf = RandomForestRegressor(\n",
    "    featuresCol='features',\n",
    "    labelCol='label',\n",
    "    numTrees=20,\n",
    "    maxDepth=5,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"ğŸ”„ Entrenando modelo Random Forest...\")\n",
    "rf_model = rf.fit(train_data)\n",
    "\n",
    "print(\"\\nâœ… Random Forest entrenado\")\n",
    "print(f\"   NÃºmero de Ã¡rboles: {rf_model.getNumTrees}\")\n",
    "print(f\"   Profundidad mÃ¡xima: {rf_model.getMaxDepth()}\")\n",
    "\n",
    "# Evaluar\n",
    "predictions_rf = rf_model.transform(test_data)\n",
    "\n",
    "rmse_rf = evaluator_rmse.evaluate(predictions_rf)\n",
    "r2_rf = evaluator_r2.evaluate(predictions_rf)\n",
    "mae_rf = evaluator_mae.evaluate(predictions_rf)\n",
    "\n",
    "print(\"\\nğŸ“Š MÃ©tricas del Modelo - Random Forest:\")\n",
    "print(f\"   RMSE: {rmse_rf:,.2f}\")\n",
    "print(f\"   MAE:  {mae_rf:,.2f}\")\n",
    "print(f\"   RÂ²:   {r2_rf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Comparar Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear tabla comparativa\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Modelo': ['RegresiÃ³n Lineal', 'Random Forest'],\n",
    "    'RMSE': [rmse_lr, rmse_rf],\n",
    "    'MAE': [mae_lr, mae_rf],\n",
    "    'RÂ²': [r2_lr, r2_rf]\n",
    "})\n",
    "\n",
    "print(\"ğŸ“Š ComparaciÃ³n de Modelos:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Visualizar comparaciÃ³n\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "metrics = ['RMSE', 'MAE', 'RÂ²']\n",
    "for i, metric in enumerate(metrics):\n",
    "    axes[i].bar(comparison_df['Modelo'], comparison_df[metric], color=['#3498db', '#e74c3c'])\n",
    "    axes[i].set_title(f'{metric} por Modelo')\n",
    "    axes[i].set_ylabel(metric)\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Determinar mejor modelo\n",
    "mejor_modelo = 'Random Forest' if r2_rf > r2_lr else 'RegresiÃ³n Lineal'\n",
    "print(f\"\\nğŸ† Mejor modelo: {mejor_modelo}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Tracking con MLflow <a id=\"mlflow\"></a>\n",
    "\n",
    "### 5.1 Configurar MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar MLflow\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "mlflow.set_experiment(\"spark-airflow-tutorial\")\n",
    "\n",
    "print(\"âœ… MLflow configurado\")\n",
    "print(f\"   Tracking URI: http://localhost:5000\")\n",
    "print(f\"   Experimento: spark-airflow-tutorial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Registrar Experimento - RegresiÃ³n Lineal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Registrar experimento con MLflow\n",
    "with mlflow.start_run(run_name=\"LinearRegression-Tutorial\") as run:\n",
    "    # Log de parÃ¡metros\n",
    "    mlflow.log_param(\"model_type\", \"LinearRegression\")\n",
    "    mlflow.log_param(\"maxIter\", 10)\n",
    "    mlflow.log_param(\"regParam\", 0.01)\n",
    "    mlflow.log_param(\"train_size\", train_data.count())\n",
    "    mlflow.log_param(\"test_size\", test_data.count())\n",
    "    mlflow.log_param(\"features\", feature_cols)\n",
    "    \n",
    "    # Log de mÃ©tricas\n",
    "    mlflow.log_metric(\"rmse\", rmse_lr)\n",
    "    mlflow.log_metric(\"mae\", mae_lr)\n",
    "    mlflow.log_metric(\"r2\", r2_lr)\n",
    "    \n",
    "    # Log del modelo\n",
    "    mlflow.spark.log_model(lr_model, \"model\")\n",
    "    \n",
    "    run_id_lr = run.info.run_id\n",
    "    \n",
    "    print(f\"âœ… Experimento registrado en MLflow\")\n",
    "    print(f\"   Run ID: {run_id_lr}\")\n",
    "    print(f\"   Ver en: http://localhost:5000/#/experiments/1/runs/{run_id_lr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Registrar Experimento - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Registrar Random Forest\n",
    "with mlflow.start_run(run_name=\"RandomForest-Tutorial\") as run:\n",
    "    # Log de parÃ¡metros\n",
    "    mlflow.log_param(\"model_type\", \"RandomForest\")\n",
    "    mlflow.log_param(\"numTrees\", 20)\n",
    "    mlflow.log_param(\"maxDepth\", 5)\n",
    "    mlflow.log_param(\"train_size\", train_data.count())\n",
    "    mlflow.log_param(\"test_size\", test_data.count())\n",
    "    mlflow.log_param(\"features\", feature_cols)\n",
    "    \n",
    "    # Log de mÃ©tricas\n",
    "    mlflow.log_metric(\"rmse\", rmse_rf)\n",
    "    mlflow.log_metric(\"mae\", mae_rf)\n",
    "    mlflow.log_metric(\"r2\", r2_rf)\n",
    "    \n",
    "    # Feature importance\n",
    "    importance_dict = {f\"feature_{i}\": float(imp) for i, imp in enumerate(rf_model.featureImportances)}\n",
    "    mlflow.log_dict(importance_dict, \"feature_importance.json\")\n",
    "    \n",
    "    # Log del modelo\n",
    "    mlflow.spark.log_model(rf_model, \"model\")\n",
    "    \n",
    "    run_id_rf = run.info.run_id\n",
    "    \n",
    "    print(f\"âœ… Experimento Random Forest registrado\")\n",
    "    print(f\"   Run ID: {run_id_rf}\")\n",
    "    print(f\"   Ver en: http://localhost:5000/#/experiments/1/runs/{run_id_rf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Comparar Experimentos en MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener todos los runs del experimento\n",
    "client = MlflowClient()\n",
    "experiment = client.get_experiment_by_name(\"spark-airflow-tutorial\")\n",
    "\n",
    "if experiment:\n",
    "    runs = client.search_runs(\n",
    "        experiment_ids=[experiment.experiment_id],\n",
    "        order_by=[\"metrics.r2 DESC\"]\n",
    "    )\n",
    "    \n",
    "    print(\"ğŸ“Š Experimentos en MLflow:\\n\")\n",
    "    for run in runs:\n",
    "        print(f\"Run: {run.info.run_name}\")\n",
    "        print(f\"  Run ID: {run.info.run_id}\")\n",
    "        print(f\"  RMSE: {run.data.metrics.get('rmse', 'N/A')}\")\n",
    "        print(f\"  RÂ²: {run.data.metrics.get('r2', 'N/A')}\")\n",
    "        print(f\"  Modelo: {run.data.params.get('model_type', 'N/A')}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"âš ï¸ Experimento no encontrado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. OrquestaciÃ³n con Airflow <a id=\"airflow\"></a>\n",
    "\n",
    "### 6.1 Listar DAGs Disponibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConfiguraciÃ³n de Airflow API\n",
    "AIRFLOW_API_URL = \"http://localhost:8080/api/v1\"\n",
    "AIRFLOW_AUTH = HTTPBasicAuth(\"airflow\", \"airflow\")\n",
    "\n",
    "# Listar DAGs\n",
    "try:\n",
    "    response = requests.get(f\"{AIRFLOW_API_URL}/dags\", auth=AIRFLOW_AUTH)\n",
    "    if response.status_code == 200:\n",
    "        dags = response.json()['dags']\n",
    "        print(f\"ğŸ“‹ DAGs disponibles ({len(dags)}):\")\n",
    "        print()\n",
    "        for dag in dags:\n",
    "            status = \"âœ… Activo\" if dag['is_active'] and not dag['is_paused'] else \"â¸ï¸ Pausado\"\n",
    "            print(f\"  {status} - {dag['dag_id']}\")\n",
    "            print(f\"    Schedule: {dag.get('schedule_interval', 'None')}\")\n",
    "            print(f\"    Tags: {', '.join(dag.get('tags', []))}\")\n",
    "            print()\n",
    "    else:\n",
    "        print(f\"âŒ Error: {response.status_code}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error conectando con Airflow: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Disparar DAG de ML Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disparar el DAG ml_pipeline\n",
    "dag_id = \"ml_pipeline\"\n",
    "\n",
    "try:\n",
    "    # Trigger DAG\n",
    "    trigger_url = f\"{AIRFLOW_API_URL}/dags/{dag_id}/dagRuns\"\n",
    "    payload = {\n",
    "        \"conf\": {\n",
    "            \"triggered_from\": \"jupyter_notebook\",\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    response = requests.post(\n",
    "        trigger_url,\n",
    "        json=payload,\n",
    "        auth=AIRFLOW_AUTH,\n",
    "        headers={\"Content-Type\": \"application/json\"}\n",
    "    )\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        result = response.json()\n",
    "        dag_run_id = result['dag_run_id']\n",
    "        print(f\"âœ… DAG disparado exitosamente\")\n",
    "        print(f\"   DAG ID: {dag_id}\")\n",
    "        print(f\"   Run ID: {dag_run_id}\")\n",
    "        print(f\"   Estado: {result['state']}\")\n",
    "        print(f\"\\n   Ver en Airflow: http://localhost:8080/dags/{dag_id}/grid\")\n",
    "    else:\n",
    "        print(f\"âŒ Error disparando DAG: {response.status_code}\")\n",
    "        print(f\"   Respuesta: {response.text}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Monitorear Estado del DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def get_dag_status(dag_id, max_checks=10):\n",
    "    \"\"\"Monitorea el estado de un DAG run\"\"\"\n",
    "    \n",
    "    # Obtener el Ãºltimo run\n",
    "    runs_url = f\"{AIRFLOW_API_URL}/dags/{dag_id}/dagRuns\"\n",
    "    response = requests.get(runs_url, auth=AIRFLOW_AUTH, params={\"limit\": 1})\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(\"âŒ No se pudo obtener el estado del DAG\")\n",
    "        return\n",
    "    \n",
    "    runs = response.json()['dag_runs']\n",
    "    if not runs:\n",
    "        print(\"âš ï¸ No hay runs para este DAG\")\n",
    "        return\n",
    "    \n",
    "    latest_run = runs[0]\n",
    "    dag_run_id = latest_run['dag_run_id']\n",
    "    \n",
    "    print(f\"ğŸ”„ Monitoreando DAG Run: {dag_run_id}\\n\")\n",
    "    \n",
    "    for i in range(max_checks):\n",
    "        # Obtener estado de tareas\n",
    "        tasks_url = f\"{AIRFLOW_API_URL}/dags/{dag_id}/dagRuns/{dag_run_id}/taskInstances\"\n",
    "        response = requests.get(tasks_url, auth=AIRFLOW_AUTH)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            tasks = response.json()['task_instances']\n",
    "            \n",
    "            print(f\"\\rğŸ“Š Check {i+1}/{max_checks}:\", end=\"\")\n",
    "            \n",
    "            states = {}\n",
    "            for task in tasks:\n",
    "                state = task['state']\n",
    "                states[state] = states.get(state, 0) + 1\n",
    "            \n",
    "            status_str = \" | \".join([f\"{k}: {v}\" for k, v in states.items()])\n",
    "            print(f\" {status_str}\", end=\"\")\n",
    "            \n",
    "            # Verificar si terminÃ³\n",
    "            if 'running' not in states and 'queued' not in states:\n",
    "                print(\"\\n\\nâœ… DAG completado\")\n",
    "                break\n",
    "            \n",
    "            time.sleep(5)\n",
    "        else:\n",
    "            print(f\"\\nâŒ Error obteniendo estado: {response.status_code}\")\n",
    "            break\n",
    "    \n",
    "    # Mostrar estado final\n",
    "    print(\"\\nğŸ“‹ Estado Final de Tareas:\\n\")\n",
    "    for task in tasks:\n",
    "        emoji = {\n",
    "            'success': 'âœ…',\n",
    "            'failed': 'âŒ',\n",
    "            'running': 'ğŸ”„',\n",
    "            'queued': 'â³',\n",
    "            'skipped': 'â­ï¸'\n",
    "        }.get(task['state'], 'â“')\n",
    "        \n",
    "        print(f\"{emoji} {task['task_id']}: {task['state']}\")\n",
    "        if task.get('duration'):\n",
    "            print(f\"   DuraciÃ³n: {task['duration']:.2f}s\")\n",
    "\n",
    "# Ejecutar monitoreo\n",
    "get_dag_status(\"ml_pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Crear DAG Personalizado ProgramÃ¡ticamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CÃ³digo de ejemplo para crear un DAG personalizado\n",
    "dag_code = '''\n",
    "from datetime import datetime, timedelta\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.operators.bash import BashOperator\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'tutorial',\n",
    "    'depends_on_past': False,\n",
    "    'start_date': datetime(2024, 1, 1),\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "dag = DAG(\n",
    "    'custom_spark_pipeline',\n",
    "    default_args=default_args,\n",
    "    description='Pipeline personalizado de Spark',\n",
    "    schedule_interval='@daily',\n",
    "    catchup=False,\n",
    "    tags=['spark', 'tutorial', 'custom'],\n",
    ")\n",
    "\n",
    "def load_data(**context):\n",
    "    \"\"\"Carga datos desde fuente\"\"\"\n",
    "    print(\"Cargando datos...\")\n",
    "    # AquÃ­ irÃ­a la lÃ³gica de carga\n",
    "    return {\"records\": 10000, \"status\": \"success\"}\n",
    "\n",
    "def process_with_spark(**context):\n",
    "    \"\"\"Procesa datos con Spark\"\"\"\n",
    "    from pyspark.sql import SparkSession\n",
    "    \n",
    "    spark = SparkSession.builder.appName(\"CustomPipeline\").getOrCreate()\n",
    "    print(f\"Spark Version: {spark.version}\")\n",
    "    # Procesamiento...\n",
    "    spark.stop()\n",
    "    return {\"processed\": True}\n",
    "\n",
    "def train_model(**context):\n",
    "    \"\"\"Entrena modelo ML\"\"\"\n",
    "    print(\"Entrenando modelo...\")\n",
    "    # LÃ³gica de entrenamiento\n",
    "    return {\"model_id\": \"model_v1.0\", \"accuracy\": 0.95}\n",
    "\n",
    "# Definir tareas\n",
    "t1 = PythonOperator(\n",
    "    task_id='load_data',\n",
    "    python_callable=load_data,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "t2 = PythonOperator(\n",
    "    task_id='process_spark',\n",
    "    python_callable=process_with_spark,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "t3 = PythonOperator(\n",
    "    task_id='train_model',\n",
    "    python_callable=train_model,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "t4 = BashOperator(\n",
    "    task_id='notify',\n",
    "    bash_command='echo \"Pipeline completado exitosamente\"',\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Dependencias: load -> process -> train -> notify\n",
    "t1 >> t2 >> t3 >> t4\n",
    "'''\n",
    "\n",
    "print(\"ğŸ“ CÃ³digo de DAG Personalizado:\")\n",
    "print(\"\\nPara crear este DAG:\")\n",
    "print(\"1. Copia el cÃ³digo en: airflow/dags/custom_spark_pipeline.py\")\n",
    "print(\"2. Espera ~30 segundos para que Airflow lo detecte\")\n",
    "print(\"3. Actualiza la UI de Airflow\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(dag_code)\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Pipeline Completo End-to-End <a id=\"pipeline\"></a>\n",
    "\n",
    "### 7.1 Definir Pipeline Completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_complete_pipeline(data_path=None, model_type='RandomForest'):\n",
    "    \"\"\"\n",
    "    Ejecuta el pipeline completo de ML:\n",
    "    1. Carga de datos\n",
    "    2. Procesamiento con Spark\n",
    "    3. Entrenamiento de modelo\n",
    "    4. EvaluaciÃ³n\n",
    "    5. Registro en MLflow\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"ğŸš€ Iniciando Pipeline Completo\\n\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Paso 1: Cargar datos\n",
    "    print(\"\\n[1/5] ğŸ“¥ Cargando datos...\")\n",
    "    if data_path:\n",
    "        df = spark.read.parquet(data_path)\n",
    "        print(f\"      âœ… Datos cargados desde {data_path}\")\n",
    "    else:\n",
    "        df = df_spark  # Usar datos del tutorial\n",
    "        print(f\"      âœ… Usando datos del tutorial\")\n",
    "    \n",
    "    print(f\"      Registros: {df.count():,}\")\n",
    "    \n",
    "    # Paso 2: Procesamiento\n",
    "    print(\"\\n[2/5] âš™ï¸ Procesando datos...\")\n",
    "    \n",
    "    # Indexar categorÃ­as\n",
    "    categoria_idx = StringIndexer(inputCol='categoria', outputCol='categoria_idx')\n",
    "    region_idx = StringIndexer(inputCol='region', outputCol='region_idx')\n",
    "    \n",
    "    df_processed = categoria_idx.fit(df).transform(df)\n",
    "    df_processed = region_idx.fit(df_processed).transform(df_processed)\n",
    "    \n",
    "    # Ensamblar features\n",
    "    feature_cols = ['cantidad', 'precio_unitario', 'descuento', \n",
    "                   'mes', 'dia_semana', 'hora', 'categoria_idx', 'region_idx']\n",
    "    assembler = VectorAssembler(inputCols=feature_cols, outputCol='features_raw')\n",
    "    df_processed = assembler.transform(df_processed)\n",
    "    \n",
    "    # Escalar\n",
    "    scaler = StandardScaler(inputCol='features_raw', outputCol='features', \n",
    "                           withMean=True, withStd=True)\n",
    "    scaler_model = scaler.fit(df_processed)\n",
    "    df_ml = scaler_model.transform(df_processed)\n",
    "    df_ml = df_ml.select('features', F.col('total').alias('label'))\n",
    "    \n",
    "    print(f\"      âœ… Datos procesados\")\n",
    "    \n",
    "    # Paso 3: Split\n",
    "    print(\"\\n[3/5] âœ‚ï¸ Dividiendo datos...\")\n",
    "    train, test = df_ml.randomSplit([0.8, 0.2], seed=42)\n",
    "    train.cache()\n",
    "    test.cache()\n",
    "    print(f\"      Train: {train.count():,} | Test: {test.count():,}\")\n",
    "    \n",
    "    # Paso 4: Entrenar\n",
    "    print(f\"\\n[4/5] ğŸ“ Entrenando modelo {model_type}...\")\n",
    "    \n",
    "    if model_type == 'LinearRegression':\n",
    "        model = LinearRegression(maxIter=10, regParam=0.01)\n",
    "    else:  # RandomForest\n",
    "        model = RandomForestRegressor(numTrees=20, maxDepth=5, seed=42)\n",
    "    \n",
    "    trained_model = model.fit(train)\n",
    "    print(f\"      âœ… Modelo entrenado\")\n",
    "    \n",
    "    # Paso 5: Evaluar\n",
    "    print(\"\\n[5/5] ğŸ“Š Evaluando modelo...\")\n",
    "    predictions = trained_model.transform(test)\n",
    "    \n",
    "    evaluator_rmse = RegressionEvaluator(labelCol='label', predictionCol='prediction', \n",
    "                                         metricName='rmse')\n",
    "    evaluator_r2 = RegressionEvaluator(labelCol='label', predictionCol='prediction', \n",
    "                                       metricName='r2')\n",
    "    evaluator_mae = RegressionEvaluator(labelCol='label', predictionCol='prediction', \n",
    "                                        metricName='mae')\n",
    "    \n",
    "    rmse = evaluator_rmse.evaluate(predictions)\n",
    "    r2 = evaluator_r2.evaluate(predictions)\n",
    "    mae = evaluator_mae.evaluate(predictions)\n",
    "    \n",
    "    print(f\"      RMSE: {rmse:,.2f}\")\n",
    "    print(f\"      RÂ²:   {r2:.4f}\")\n",
    "    print(f\"      MAE:  {mae:,.2f}\")\n",
    "    \n",
    "    # Paso 6: Registrar en MLflow\n",
    "    print(\"\\n[6/6] ğŸ“ Registrando en MLflow...\")\n",
    "    \n",
    "    with mlflow.start_run(run_name=f\"{model_type}-Pipeline-{datetime.now().strftime('%Y%m%d_%H%M%S')}\") as run:\n",
    "        mlflow.log_param(\"model_type\", model_type)\n",
    "        mlflow.log_param(\"train_size\", train.count())\n",
    "        mlflow.log_param(\"test_size\", test.count())\n",
    "        mlflow.log_param(\"features\", feature_cols)\n",
    "        \n",
    "        mlflow.log_metric(\"rmse\", rmse)\n",
    "        mlflow.log_metric(\"r2\", r2)\n",
    "        mlflow.log_metric(\"mae\", mae)\n",
    "        \n",
    "        mlflow.spark.log_model(trained_model, \"model\")\n",
    "        \n",
    "        run_id = run.info.run_id\n",
    "        print(f\"      âœ… Registrado con Run ID: {run_id}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"âœ… Pipeline completado exitosamente\\n\")\n",
    "    \n",
    "    return {\n",
    "        'model': trained_model,\n",
    "        'metrics': {'rmse': rmse, 'r2': r2, 'mae': mae},\n",
    "        'run_id': run_id\n",
    "    }\n",
    "\n",
    "print(\"âœ… FunciÃ³n de pipeline definida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Ejecutar Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar pipeline completo\n",
    "result = run_complete_pipeline(model_type='RandomForest')\n",
    "\n",
    "print(\"\\nğŸ“Š Resumen del Pipeline:\")\n",
    "print(f\"   Modelo: RandomForest\")\n",
    "print(f\"   RMSE: {result['metrics']['rmse']:,.2f}\")\n",
    "print(f\"   RÂ²: {result['metrics']['r2']:.4f}\")\n",
    "print(f\"   MLflow Run: {result['run_id']}\")\n",
    "print(f\"\\n   Ver en MLflow: http://localhost:5000/#/experiments/1/runs/{result['run_id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Monitoreo y Debugging <a id=\"monitoring\"></a>\n",
    "\n",
    "### 8.1 Monitorear Spark UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ” URLs de Monitoreo:\\n\")\n",
    "print(\"Spark:\")\n",
    "print(\"  â€¢ Spark UI (local): http://localhost:4040\")\n",
    "print(\"  â€¢ Spark Master UI: http://localhost:8081\")\n",
    "print()\n",
    "print(\"Airflow:\")\n",
    "print(\"  â€¢ Airflow UI: http://localhost:8080\")\n",
    "print(\"  â€¢ Usuario: airflow / ContraseÃ±a: airflow\")\n",
    "print()\n",
    "print(\"MLflow:\")\n",
    "print(\"  â€¢ MLflow UI: http://localhost:5000\")\n",
    "print()\n",
    "print(\"PostgreSQL:\")\n",
    "print(\"  â€¢ Host: localhost:5433\")\n",
    "print(\"  â€¢ Usuario: airflow / ContraseÃ±a: airflow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Verificar Recursos de Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# InformaciÃ³n de Spark Context\n",
    "sc = spark.sparkContext\n",
    "\n",
    "print(\"âš¡ InformaciÃ³n de Spark Context:\\n\")\n",
    "print(f\"  App Name: {sc.appName}\")\n",
    "print(f\"  Master: {sc.master}\")\n",
    "print(f\"  Spark Version: {sc.version}\")\n",
    "print(f\"  Default Parallelism: {sc.defaultParallelism}\")\n",
    "print(f\"  Default Min Partitions: {sc.defaultMinPartitions}\")\n",
    "print()\n",
    "print(f\"  ConfiguraciÃ³n actual:\")\n",
    "for key, value in sc.getConf().getAll():\n",
    "    if 'memory' in key.lower() or 'executor' in key.lower():\n",
    "        print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 EstadÃ­sticas de EjecuciÃ³n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener estadÃ­sticas de los Ãºltimos jobs\n",
    "status_tracker = sc.statusTracker()\n",
    "\n",
    "print(\"ğŸ“Š EstadÃ­sticas de Jobs:\\n\")\n",
    "active_jobs = status_tracker.getActiveJobIds()\n",
    "print(f\"  Jobs activos: {len(active_jobs)}\")\n",
    "\n",
    "# Pool info\n",
    "print(f\"\\n  Pool: {sc.getLocalProperty('spark.scheduler.pool') or 'default'}\")\n",
    "\n",
    "print(\"\\nâœ… Para mÃ¡s detalles, visita Spark UI: http://localhost:4040\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpiar datos cacheados\n",
    "try:\n",
    "    train_data.unpersist()\n",
    "    test_data.unpersist()\n",
    "    print(\"âœ… Cache limpiado\")\n",
    "except:\n",
    "    print(\"âš ï¸ No hay datos cacheados\")\n",
    "\n",
    "# Opcional: Detener Spark Session\n",
    "# spark.stop()\n",
    "# print(\"âœ… Spark Session detenida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ¯ Resumen del Tutorial\n",
    "\n",
    "### Lo que aprendiste:\n",
    "\n",
    "1. **Spark Fundamentals**:\n",
    "   - Crear SparkSession\n",
    "   - DataFrames y operaciones\n",
    "   - Agregaciones y Joins\n",
    "   - Window Functions\n",
    "   - Guardar/cargar datos en Parquet\n",
    "\n",
    "2. **Spark MLlib**:\n",
    "   - Feature engineering\n",
    "   - Entrenamiento de modelos (LR, RF)\n",
    "   - EvaluaciÃ³n de modelos\n",
    "   - ComparaciÃ³n de modelos\n",
    "\n",
    "3. **MLflow**:\n",
    "   - Tracking de experimentos\n",
    "   - Log de parÃ¡metros y mÃ©tricas\n",
    "   - Registro de modelos\n",
    "   - ComparaciÃ³n de runs\n",
    "\n",
    "4. **Airflow**:\n",
    "   - Listar DAGs\n",
    "   - Disparar DAGs programÃ¡ticamente\n",
    "   - Monitorear ejecuciÃ³n\n",
    "   - Crear DAGs personalizados\n",
    "\n",
    "5. **Pipeline End-to-End**:\n",
    "   - Integrar todos los componentes\n",
    "   - AutomatizaciÃ³n completa\n",
    "   - Monitoreo y debugging\n",
    "\n",
    "### PrÃ³ximos Pasos:\n",
    "\n",
    "1. **Experimenta**:\n",
    "   - Modifica hiperparÃ¡metros\n",
    "   - Prueba otros modelos (GBT, SVM)\n",
    "   - Agrega mÃ¡s features\n",
    "\n",
    "2. **Escala**:\n",
    "   - Conecta a cluster Spark real\n",
    "   - Procesa datasets mÃ¡s grandes\n",
    "   - Usa Spark Streaming\n",
    "\n",
    "3. **Productiviza**:\n",
    "   - Crea DAGs de producciÃ³n\n",
    "   - Implementa CI/CD\n",
    "   - Agrega alertas y notificaciones\n",
    "\n",
    "4. **Optimiza**:\n",
    "   - Tuning de performance Spark\n",
    "   - OptimizaciÃ³n de queries\n",
    "   - GestiÃ³n de recursos\n",
    "\n",
    "### Recursos Adicionales:\n",
    "\n",
    "- [Spark Documentation](https://spark.apache.org/docs/latest/)\n",
    "- [MLflow Documentation](https://mlflow.org/docs/latest/index.html)\n",
    "- [Airflow Documentation](https://airflow.apache.org/docs/)\n",
    "- [Spark MLlib Guide](https://spark.apache.org/docs/latest/ml-guide.html)\n",
    "\n",
    "---\n",
    "\n",
    "**Â¡Felicidades! Has completado el tutorial de Spark + Airflow + MLflow** ğŸ‰\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
