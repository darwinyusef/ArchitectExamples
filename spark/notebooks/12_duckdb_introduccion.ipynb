{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12 - DuckDB: SQL Analytics + Time Series + LLM Analysis\n",
    "\n",
    "## üéØ Objetivos\n",
    "- Introducci√≥n a DuckDB (SQL analytics in-process)\n",
    "- An√°lisis de series temporales\n",
    "- Forecasting con Prophet y ARIMA\n",
    "- Integraci√≥n con LLMs para an√°lisis de datos\n",
    "- Visualizaciones avanzadas\n",
    "\n",
    "## üìö Tecnolog√≠as\n",
    "- **DuckDB**: SQL OLAP database engine\n",
    "- **Prophet**: Time series forecasting (Facebook)\n",
    "- **statsmodels**: ARIMA, statistical models\n",
    "- **OpenAI/Anthropic**: LLM para an√°lisis\n",
    "- **Plotly**: Visualizaciones interactivas\n",
    "\n",
    "## ‚≠ê Complejidad: B√°sico/Intermedio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Instalaci√≥n y Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar dependencias\n",
    "!pip install duckdb pandas numpy matplotlib seaborn plotly prophet statsmodels scikit-learn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurar estilo\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(f\"‚úÖ DuckDB version: {duckdb.__version__}\")\n",
    "print(f\"‚úÖ Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Introducci√≥n a DuckDB\n",
    "\n",
    "DuckDB es un sistema de base de datos OLAP SQL in-process, similar a SQLite pero optimizado para analytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear conexi√≥n a DuckDB (en memoria)\n",
    "con = duckdb.connect(':memory:')\n",
    "\n",
    "# Tambi√©n podr√≠amos crear una base de datos persistente:\n",
    "# con = duckdb.connect('mi_base_datos.duckdb')\n",
    "\n",
    "print(\"‚úÖ Conexi√≥n a DuckDB establecida\")\n",
    "\n",
    "# Verificar versi√≥n\n",
    "version = con.execute(\"SELECT version()\").fetchone()\n",
    "print(f\"üìä DuckDB version: {version[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Operaciones B√°sicas con DuckDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear tabla desde SQL\n",
    "con.execute(\"\"\"\n",
    "    CREATE TABLE empleados (\n",
    "        id INTEGER,\n",
    "        nombre VARCHAR,\n",
    "        edad INTEGER,\n",
    "        departamento VARCHAR,\n",
    "        salario DECIMAL(10,2)\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Insertar datos\n",
    "con.execute(\"\"\"\n",
    "    INSERT INTO empleados VALUES\n",
    "        (1, 'Juan', 28, 'TI', 50000),\n",
    "        (2, 'Mar√≠a', 35, 'Ventas', 75000),\n",
    "        (3, 'Pedro', 42, 'TI', 60000),\n",
    "        (4, 'Ana', 29, 'Marketing', 55000),\n",
    "        (5, 'Luis', 38, 'Ventas', 70000)\n",
    "\"\"\")\n",
    "\n",
    "# Query simple\n",
    "result = con.execute(\"SELECT * FROM empleados\").df()\n",
    "print(\"üìä Tabla empleados:\")\n",
    "print(result)\n",
    "\n",
    "# Agregaci√≥n\n",
    "result = con.execute(\"\"\"\n",
    "    SELECT \n",
    "        departamento,\n",
    "        COUNT(*) as empleados,\n",
    "        AVG(edad) as edad_promedio,\n",
    "        AVG(salario) as salario_promedio\n",
    "    FROM empleados\n",
    "    GROUP BY departamento\n",
    "    ORDER BY salario_promedio DESC\n",
    "\"\"\").df()\n",
    "\n",
    "print(\"\\nüìä Resumen por departamento:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Integraci√≥n DuckDB + Pandas\n",
    "\n",
    "DuckDB puede consultar DataFrames de Pandas directamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear DataFrame de Pandas\n",
    "productos_df = pd.DataFrame({\n",
    "    'producto_id': [1, 2, 3, 4, 5],\n",
    "    'nombre': ['Laptop', 'Mouse', 'Teclado', 'Monitor', 'Aud√≠fonos'],\n",
    "    'precio': [1200, 25, 75, 350, 80],\n",
    "    'categoria': ['Electr√≥nica', 'Accesorios', 'Accesorios', 'Electr√≥nica', 'Accesorios']\n",
    "})\n",
    "\n",
    "print(\"üìä DataFrame de Pandas:\")\n",
    "print(productos_df)\n",
    "\n",
    "# Query directo sobre DataFrame de Pandas\n",
    "result = con.execute(\"\"\"\n",
    "    SELECT \n",
    "        categoria,\n",
    "        COUNT(*) as productos,\n",
    "        AVG(precio) as precio_promedio,\n",
    "        SUM(precio) as precio_total\n",
    "    FROM productos_df\n",
    "    GROUP BY categoria\n",
    "\"\"\").df()\n",
    "\n",
    "print(\"\\nüìä Query sobre DataFrame de Pandas:\")\n",
    "print(result)\n",
    "\n",
    "# Tambi√©n podemos usar la sintaxis alternativa\n",
    "result = duckdb.query(\"\"\"\n",
    "    SELECT * FROM productos_df WHERE precio > 50 ORDER BY precio DESC\n",
    "\"\"\").df()\n",
    "\n",
    "print(\"\\nüìä Filtrado con DuckDB:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generar Datos de Time Series\n",
    "\n",
    "Creamos datos sint√©ticos de series temporales para an√°lisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar serie temporal de ventas\n",
    "np.random.seed(42)\n",
    "\n",
    "# Fecha inicial: 2 a√±os atr√°s\n",
    "start_date = datetime.now() - timedelta(days=730)\n",
    "dates = pd.date_range(start=start_date, periods=730, freq='D')\n",
    "\n",
    "# Componentes de la serie temporal:\n",
    "# 1. Tendencia creciente\n",
    "trend = np.linspace(100, 300, len(dates))\n",
    "\n",
    "# 2. Estacionalidad semanal\n",
    "weekly_seasonality = 30 * np.sin(2 * np.pi * np.arange(len(dates)) / 7)\n",
    "\n",
    "# 3. Estacionalidad anual\n",
    "yearly_seasonality = 50 * np.sin(2 * np.pi * np.arange(len(dates)) / 365)\n",
    "\n",
    "# 4. Ruido aleatorio\n",
    "noise = np.random.normal(0, 15, len(dates))\n",
    "\n",
    "# 5. Eventos especiales (picos)\n",
    "special_events = np.zeros(len(dates))\n",
    "# Black Friday, Cyber Monday, etc.\n",
    "event_days = [60, 150, 240, 330, 425, 515, 605, 695]\n",
    "for day in event_days:\n",
    "    if day < len(dates):\n",
    "        special_events[day:day+3] = 100\n",
    "\n",
    "# Combinar todos los componentes\n",
    "sales = trend + weekly_seasonality + yearly_seasonality + noise + special_events\n",
    "sales = np.maximum(sales, 0)  # No ventas negativas\n",
    "\n",
    "# Crear DataFrame\n",
    "ts_df = pd.DataFrame({\n",
    "    'fecha': dates,\n",
    "    'ventas': sales,\n",
    "    'tendencia': trend,\n",
    "    'estacionalidad_semanal': weekly_seasonality,\n",
    "    'estacionalidad_anual': yearly_seasonality,\n",
    "    'ruido': noise\n",
    "})\n",
    "\n",
    "# Agregar features temporales\n",
    "ts_df['a√±o'] = ts_df['fecha'].dt.year\n",
    "ts_df['mes'] = ts_df['fecha'].dt.month\n",
    "ts_df['dia_semana'] = ts_df['fecha'].dt.dayofweek\n",
    "ts_df['dia_mes'] = ts_df['fecha'].dt.day\n",
    "ts_df['trimestre'] = ts_df['fecha'].dt.quarter\n",
    "\n",
    "print(f\"üìä Serie temporal creada: {len(ts_df)} d√≠as\")\n",
    "print(f\"üìä Rango: {ts_df['fecha'].min()} a {ts_df['fecha'].max()}\")\n",
    "print(f\"\\nüìä Primeras filas:\")\n",
    "print(ts_df.head(10))\n",
    "print(f\"\\nüìä Estad√≠sticas:\")\n",
    "print(ts_df[['ventas', 'tendencia']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. An√°lisis SQL de Time Series con DuckDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lisis mensual\n",
    "monthly_analysis = con.execute(\"\"\"\n",
    "    SELECT \n",
    "        a√±o,\n",
    "        mes,\n",
    "        COUNT(*) as dias,\n",
    "        ROUND(AVG(ventas), 2) as ventas_promedio,\n",
    "        ROUND(SUM(ventas), 2) as ventas_total,\n",
    "        ROUND(MIN(ventas), 2) as ventas_min,\n",
    "        ROUND(MAX(ventas), 2) as ventas_max,\n",
    "        ROUND(STDDEV(ventas), 2) as ventas_stddev\n",
    "    FROM ts_df\n",
    "    GROUP BY a√±o, mes\n",
    "    ORDER BY a√±o, mes\n",
    "\"\"\").df()\n",
    "\n",
    "print(\"üìä An√°lisis Mensual:\")\n",
    "print(monthly_analysis.head(12))\n",
    "\n",
    "# An√°lisis por d√≠a de la semana\n",
    "weekly_pattern = con.execute(\"\"\"\n",
    "    SELECT \n",
    "        dia_semana,\n",
    "        CASE dia_semana\n",
    "            WHEN 0 THEN 'Lunes'\n",
    "            WHEN 1 THEN 'Martes'\n",
    "            WHEN 2 THEN 'Mi√©rcoles'\n",
    "            WHEN 3 THEN 'Jueves'\n",
    "            WHEN 4 THEN 'Viernes'\n",
    "            WHEN 5 THEN 'S√°bado'\n",
    "            WHEN 6 THEN 'Domingo'\n",
    "        END as dia_nombre,\n",
    "        ROUND(AVG(ventas), 2) as ventas_promedio,\n",
    "        COUNT(*) as registros\n",
    "    FROM ts_df\n",
    "    GROUP BY dia_semana\n",
    "    ORDER BY dia_semana\n",
    "\"\"\").df()\n",
    "\n",
    "print(\"\\nüìä Patr√≥n Semanal:\")\n",
    "print(weekly_pattern)\n",
    "\n",
    "# Crecimiento mensual\n",
    "growth = con.execute(\"\"\"\n",
    "    WITH monthly AS (\n",
    "        SELECT \n",
    "            a√±o,\n",
    "            mes,\n",
    "            SUM(ventas) as ventas_total\n",
    "        FROM ts_df\n",
    "        GROUP BY a√±o, mes\n",
    "    )\n",
    "    SELECT \n",
    "        a√±o,\n",
    "        mes,\n",
    "        ventas_total,\n",
    "        LAG(ventas_total) OVER (ORDER BY a√±o, mes) as ventas_mes_anterior,\n",
    "        ROUND(\n",
    "            (ventas_total - LAG(ventas_total) OVER (ORDER BY a√±o, mes)) / \n",
    "            LAG(ventas_total) OVER (ORDER BY a√±o, mes) * 100, \n",
    "            2\n",
    "        ) as crecimiento_pct\n",
    "    FROM monthly\n",
    "    ORDER BY a√±o, mes\n",
    "\"\"\").df()\n",
    "\n",
    "print(\"\\nüìä Crecimiento Mensual:\")\n",
    "print(growth.head(12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualizaci√≥n de Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaci√≥n con Plotly (interactivo)\n",
    "fig = make_subplots(\n",
    "    rows=3, cols=1,\n",
    "    subplot_titles=('Ventas Diarias', 'Componentes de la Serie', 'Distribuci√≥n'),\n",
    "    specs=[[{\"secondary_y\": False}],\n",
    "           [{\"secondary_y\": False}],\n",
    "           [{\"secondary_y\": False}]],\n",
    "    vertical_spacing=0.1\n",
    ")\n",
    "\n",
    "# Serie temporal completa\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=ts_df['fecha'], y=ts_df['ventas'], \n",
    "               name='Ventas', line=dict(color='blue', width=1)),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Tendencia\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=ts_df['fecha'], y=ts_df['tendencia'], \n",
    "               name='Tendencia', line=dict(color='red', width=2, dash='dash')),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Componentes\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=ts_df['fecha'], y=ts_df['estacionalidad_semanal'], \n",
    "               name='Est. Semanal', line=dict(color='green')),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=ts_df['fecha'], y=ts_df['estacionalidad_anual'], \n",
    "               name='Est. Anual', line=dict(color='orange')),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# Histograma\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=ts_df['ventas'], name='Distribuci√≥n', \n",
    "                 marker=dict(color='purple'), nbinsx=50),\n",
    "    row=3, col=1\n",
    ")\n",
    "\n",
    "fig.update_layout(height=900, title_text=\"An√°lisis de Serie Temporal\", showlegend=True)\n",
    "fig.show()\n",
    "\n",
    "print(\"‚úÖ Gr√°ficos interactivos generados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patr√≥n semanal visualizado\n",
    "fig = px.bar(weekly_pattern, x='dia_nombre', y='ventas_promedio',\n",
    "             title='Ventas Promedio por D√≠a de la Semana',\n",
    "             labels={'dia_nombre': 'D√≠a', 'ventas_promedio': 'Ventas Promedio'},\n",
    "             color='ventas_promedio',\n",
    "             color_continuous_scale='Blues')\n",
    "fig.show()\n",
    "\n",
    "# Crecimiento mensual\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=list(range(len(growth))),\n",
    "    y=growth['crecimiento_pct'],\n",
    "    mode='lines+markers',\n",
    "    name='Crecimiento %',\n",
    "    line=dict(color='green', width=2)\n",
    "))\n",
    "fig.add_hline(y=0, line_dash=\"dash\", line_color=\"red\")\n",
    "fig.update_layout(\n",
    "    title='Crecimiento Mensual (%)',\n",
    "    xaxis_title='Mes',\n",
    "    yaxis_title='Crecimiento %',\n",
    "    height=400\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Forecasting con Prophet\n",
    "\n",
    "Prophet es una librer√≠a de Facebook para forecasting de series temporales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prophet import Prophet\n",
    "\n",
    "# Preparar datos para Prophet (requiere columnas 'ds' y 'y')\n",
    "prophet_df = ts_df[['fecha', 'ventas']].copy()\n",
    "prophet_df.columns = ['ds', 'y']\n",
    "\n",
    "# Crear y entrenar modelo\n",
    "print(\"üöÄ Entrenando modelo Prophet...\")\n",
    "model_prophet = Prophet(\n",
    "    daily_seasonality=True,\n",
    "    weekly_seasonality=True,\n",
    "    yearly_seasonality=True,\n",
    "    seasonality_mode='additive'\n",
    ")\n",
    "\n",
    "model_prophet.fit(prophet_df)\n",
    "print(\"‚úÖ Modelo Prophet entrenado\")\n",
    "\n",
    "# Crear dataframe para predicciones futuras (90 d√≠as)\n",
    "future = model_prophet.make_future_dataframe(periods=90)\n",
    "print(f\"üìä Prediciendo {90} d√≠as futuros...\")\n",
    "\n",
    "# Hacer predicciones\n",
    "forecast = model_prophet.predict(future)\n",
    "\n",
    "print(\"\\nüìä Predicciones:\")\n",
    "print(forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar predicciones con Prophet\n",
    "from prophet.plot import plot_plotly, plot_components_plotly\n",
    "\n",
    "# Gr√°fico de predicci√≥n\n",
    "fig1 = plot_plotly(model_prophet, forecast)\n",
    "fig1.update_layout(title='Predicci√≥n con Prophet (90 d√≠as)', height=500)\n",
    "fig1.show()\n",
    "\n",
    "# Componentes de la serie\n",
    "fig2 = plot_components_plotly(model_prophet, forecast)\n",
    "fig2.update_layout(height=800)\n",
    "fig2.show()\n",
    "\n",
    "print(\"‚úÖ Visualizaciones de Prophet generadas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Forecasting con ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# Preparar serie temporal\n",
    "ts_series = ts_df.set_index('fecha')['ventas']\n",
    "\n",
    "# Descomposici√≥n de la serie\n",
    "print(\"üìä Descomposici√≥n de la serie temporal...\")\n",
    "decomposition = seasonal_decompose(ts_series, model='additive', period=7)\n",
    "\n",
    "# Visualizar componentes\n",
    "fig, axes = plt.subplots(4, 1, figsize=(14, 10))\n",
    "\n",
    "decomposition.observed.plot(ax=axes[0], title='Serie Original')\n",
    "decomposition.trend.plot(ax=axes[1], title='Tendencia')\n",
    "decomposition.seasonal.plot(ax=axes[2], title='Estacionalidad')\n",
    "decomposition.resid.plot(ax=axes[3], title='Residuos')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Descomposici√≥n completada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar modelo ARIMA\n",
    "print(\"üöÄ Entrenando modelo ARIMA...\")\n",
    "\n",
    "# ARIMA(p,d,q) - usaremos (1,1,1) como ejemplo simple\n",
    "model_arima = ARIMA(ts_series, order=(1, 1, 1))\n",
    "fitted_model = model_arima.fit()\n",
    "\n",
    "print(\"‚úÖ Modelo ARIMA entrenado\")\n",
    "print(\"\\nüìä Resumen del modelo:\")\n",
    "print(fitted_model.summary())\n",
    "\n",
    "# Predicciones\n",
    "forecast_steps = 90\n",
    "forecast_arima = fitted_model.forecast(steps=forecast_steps)\n",
    "\n",
    "print(f\"\\nüìä Primeras 10 predicciones ARIMA:\")\n",
    "print(forecast_arima.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar predicciones ARIMA\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# √öltimos 180 d√≠as de datos reales\n",
    "plt.plot(ts_series[-180:], label='Datos Reales', color='blue')\n",
    "\n",
    "# Predicciones\n",
    "forecast_index = pd.date_range(\n",
    "    start=ts_series.index[-1] + timedelta(days=1),\n",
    "    periods=forecast_steps,\n",
    "    freq='D'\n",
    ")\n",
    "plt.plot(forecast_index, forecast_arima, label='Predicci√≥n ARIMA', \n",
    "         color='red', linestyle='--')\n",
    "\n",
    "plt.title('Predicci√≥n con ARIMA (90 d√≠as)', fontsize=14)\n",
    "plt.xlabel('Fecha')\n",
    "plt.ylabel('Ventas')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Visualizaci√≥n ARIMA completada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Comparaci√≥n de Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparar predicciones de Prophet vs ARIMA\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Obtener predicciones de Prophet para los mismos d√≠as\n",
    "forecast_future = forecast[forecast['ds'] > ts_df['fecha'].max()].head(90)\n",
    "\n",
    "# Crear visualizaci√≥n comparativa\n",
    "fig = go.Figure()\n",
    "\n",
    "# Datos hist√≥ricos (√∫ltimos 180 d√≠as)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=ts_df['fecha'][-180:],\n",
    "    y=ts_df['ventas'][-180:],\n",
    "    mode='lines',\n",
    "    name='Datos Hist√≥ricos',\n",
    "    line=dict(color='blue', width=2)\n",
    "))\n",
    "\n",
    "# Predicci√≥n Prophet\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=forecast_future['ds'],\n",
    "    y=forecast_future['yhat'],\n",
    "    mode='lines',\n",
    "    name='Prophet',\n",
    "    line=dict(color='green', width=2, dash='dash')\n",
    "))\n",
    "\n",
    "# Intervalo de confianza Prophet\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=forecast_future['ds'],\n",
    "    y=forecast_future['yhat_upper'],\n",
    "    mode='lines',\n",
    "    name='Prophet Upper',\n",
    "    line=dict(color='green', width=0),\n",
    "    showlegend=False\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=forecast_future['ds'],\n",
    "    y=forecast_future['yhat_lower'],\n",
    "    mode='lines',\n",
    "    name='Prophet Lower',\n",
    "    line=dict(color='green', width=0),\n",
    "    fill='tonexty',\n",
    "    fillcolor='rgba(0,255,0,0.1)',\n",
    "    showlegend=False\n",
    "))\n",
    "\n",
    "# Predicci√≥n ARIMA\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=forecast_index,\n",
    "    y=forecast_arima,\n",
    "    mode='lines',\n",
    "    name='ARIMA',\n",
    "    line=dict(color='red', width=2, dash='dot')\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Comparaci√≥n: Prophet vs ARIMA',\n",
    "    xaxis_title='Fecha',\n",
    "    yaxis_title='Ventas',\n",
    "    height=600,\n",
    "    hovermode='x unified'\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"‚úÖ Comparaci√≥n de modelos visualizada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Almacenar Resultados en DuckDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear DataFrame con predicciones\n",
    "predictions_df = pd.DataFrame({\n",
    "    'fecha': forecast_index,\n",
    "    'prediccion_prophet': forecast_future['yhat'].values,\n",
    "    'prophet_lower': forecast_future['yhat_lower'].values,\n",
    "    'prophet_upper': forecast_future['yhat_upper'].values,\n",
    "    'prediccion_arima': forecast_arima.values\n",
    "})\n",
    "\n",
    "# Registrar en DuckDB\n",
    "con.execute(\"CREATE TABLE IF NOT EXISTS ts_data AS SELECT * FROM ts_df\")\n",
    "con.execute(\"CREATE TABLE IF NOT EXISTS predictions AS SELECT * FROM predictions_df\")\n",
    "\n",
    "print(\"‚úÖ Datos guardados en DuckDB\")\n",
    "\n",
    "# Query combinada\n",
    "result = con.execute(\"\"\"\n",
    "    SELECT \n",
    "        p.fecha,\n",
    "        p.prediccion_prophet,\n",
    "        p.prediccion_arima,\n",
    "        ROUND((p.prediccion_prophet + p.prediccion_arima) / 2, 2) as prediccion_promedio\n",
    "    FROM predictions p\n",
    "    ORDER BY fecha\n",
    "    LIMIT 10\n",
    "\"\"\").df()\n",
    "\n",
    "print(\"\\nüìä Predicciones combinadas:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. An√°lisis con LLM (Generaci√≥n de Insights)\n",
    "\n",
    "Usaremos un LLM para analizar autom√°ticamente los datos y generar insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar resumen de datos para el LLM\n",
    "def generate_data_summary():\n",
    "    summary = f\"\"\"\n",
    "    RESUMEN DE DATOS DE VENTAS\n",
    "    ===========================\n",
    "    \n",
    "    Per√≠odo: {ts_df['fecha'].min()} a {ts_df['fecha'].max()}\n",
    "    Total de d√≠as: {len(ts_df)}\n",
    "    \n",
    "    ESTAD√çSTICAS GENERALES:\n",
    "    - Ventas promedio diarias: ${ts_df['ventas'].mean():.2f}\n",
    "    - Ventas m√≠nimas: ${ts_df['ventas'].min():.2f}\n",
    "    - Ventas m√°ximas: ${ts_df['ventas'].max():.2f}\n",
    "    - Desviaci√≥n est√°ndar: ${ts_df['ventas'].std():.2f}\n",
    "    \n",
    "    VENTAS POR D√çA DE LA SEMANA:\n",
    "    {weekly_pattern.to_string(index=False)}\n",
    "    \n",
    "    TENDENCIA:\n",
    "    - Ventas primer mes: ${monthly_analysis.iloc[0]['ventas_total']:.2f}\n",
    "    - Ventas √∫ltimo mes: ${monthly_analysis.iloc[-1]['ventas_total']:.2f}\n",
    "    - Crecimiento total: {((monthly_analysis.iloc[-1]['ventas_total'] / monthly_analysis.iloc[0]['ventas_total']) - 1) * 100:.2f}%\n",
    "    \n",
    "    PREDICCIONES (pr√≥ximos 30 d√≠as):\n",
    "    - Prophet: ${forecast_future['yhat'].head(30).mean():.2f} promedio\n",
    "    - ARIMA: ${forecast_arima.head(30).mean():.2f} promedio\n",
    "    \"\"\"\n",
    "    return summary\n",
    "\n",
    "data_summary = generate_data_summary()\n",
    "print(data_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n para an√°lisis con LLM (simulado)\n",
    "def analyze_with_llm(data_summary):\n",
    "    \"\"\"\n",
    "    En producci√≥n, aqu√≠ conectar√≠as con OpenAI, Anthropic, u otro LLM.\n",
    "    Por ahora, generamos un an√°lisis de ejemplo.\n",
    "    \n",
    "    Para usar un LLM real, descomenta y configura:\n",
    "    \n",
    "    # import openai\n",
    "    # openai.api_key = 'tu-api-key'\n",
    "    # response = openai.ChatCompletion.create(\n",
    "    #     model=\"gpt-4\",\n",
    "    #     messages=[\n",
    "    #         {\"role\": \"system\", \"content\": \"Eres un analista de datos experto.\"},\n",
    "    #         {\"role\": \"user\", \"content\": f\"Analiza estos datos: {data_summary}\"}\n",
    "    #     ]\n",
    "    # )\n",
    "    # return response.choices[0].message.content\n",
    "    \"\"\"\n",
    "    \n",
    "    # An√°lisis simulado basado en los datos\n",
    "    insights = f\"\"\"\n",
    "    ü§ñ AN√ÅLISIS GENERADO POR LLM\n",
    "    ============================\n",
    "    \n",
    "    üìä INSIGHTS CLAVE:\n",
    "    \n",
    "    1. TENDENCIA POSITIVA\n",
    "       Las ventas muestran una tendencia creciente sostenida a lo largo del per√≠odo analizado.\n",
    "       El crecimiento total sugiere un negocio en expansi√≥n.\n",
    "    \n",
    "    2. ESTACIONALIDAD SEMANAL\n",
    "       Se observa un patr√≥n claro en los d√≠as de la semana. Los d√≠as con mayores ventas\n",
    "       son {weekly_pattern.nlargest(1, 'ventas_promedio')['dia_nombre'].values[0]}, \n",
    "       mientras que {weekly_pattern.nsmallest(1, 'ventas_promedio')['dia_nombre'].values[0]} \n",
    "       tiene las ventas m√°s bajas.\n",
    "    \n",
    "    3. VARIABILIDAD\n",
    "       La desviaci√≥n est√°ndar indica una variabilidad moderada en las ventas diarias,\n",
    "       lo cual es normal en negocios retail.\n",
    "    \n",
    "    4. PREDICCIONES\n",
    "       Ambos modelos (Prophet y ARIMA) predicen una continuaci√≥n de la tendencia positiva.\n",
    "       Se recomienda:\n",
    "       - Aumentar inventario para los pr√≥ximos 30 d√≠as\n",
    "       - Preparar campa√±as de marketing para d√≠as de menor venta\n",
    "       - Monitorear eventos especiales que generan picos de venta\n",
    "    \n",
    "    5. RECOMENDACIONES\n",
    "       ‚úÖ Optimizar staffing para d√≠as de alta demanda\n",
    "       ‚úÖ Implementar promociones en d√≠as de baja venta\n",
    "       ‚úÖ Analizar factores externos que causan los picos de venta\n",
    "       ‚úÖ Considerar expansi√≥n dada la tendencia positiva\n",
    "    \n",
    "    üìà FORECAST SUMMARY:\n",
    "       Los pr√≥ximos 90 d√≠as muestran expectativas de crecimiento continuo.\n",
    "       El modelo Prophet sugiere mayor confianza con intervalos m√°s estrechos.\n",
    "    \"\"\"\n",
    "    \n",
    "    return insights\n",
    "\n",
    "# Generar an√°lisis\n",
    "llm_analysis = analyze_with_llm(data_summary)\n",
    "print(llm_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Guardar An√°lisis LLM en DuckDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear tabla de an√°lisis\n",
    "analysis_record = pd.DataFrame([{\n",
    "    'fecha_analisis': datetime.now(),\n",
    "    'periodo_inicio': ts_df['fecha'].min(),\n",
    "    'periodo_fin': ts_df['fecha'].max(),\n",
    "    'ventas_promedio': ts_df['ventas'].mean(),\n",
    "    'prediccion_30d_prophet': forecast_future['yhat'].head(30).mean(),\n",
    "    'prediccion_30d_arima': forecast_arima.head(30).mean(),\n",
    "    'insights_llm': llm_analysis\n",
    "}])\n",
    "\n",
    "con.execute(\"CREATE TABLE IF NOT EXISTS llm_analysis AS SELECT * FROM analysis_record\")\n",
    "\n",
    "print(\"‚úÖ An√°lisis LLM guardado en DuckDB\")\n",
    "\n",
    "# Recuperar an√°lisis\n",
    "stored_analysis = con.execute(\"\"\"\n",
    "    SELECT \n",
    "        fecha_analisis,\n",
    "        ROUND(ventas_promedio, 2) as ventas_promedio,\n",
    "        ROUND(prediccion_30d_prophet, 2) as pred_prophet,\n",
    "        ROUND(prediccion_30d_arima, 2) as pred_arima\n",
    "    FROM llm_analysis\n",
    "\"\"\").df()\n",
    "\n",
    "print(\"\\nüìä An√°lisis almacenado:\")\n",
    "print(stored_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Dashboard Resumen con DuckDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query compleja para dashboard\n",
    "dashboard_query = \"\"\"\n",
    "WITH stats AS (\n",
    "    SELECT \n",
    "        COUNT(*) as total_dias,\n",
    "        ROUND(AVG(ventas), 2) as ventas_avg,\n",
    "        ROUND(SUM(ventas), 2) as ventas_total,\n",
    "        ROUND(MIN(ventas), 2) as ventas_min,\n",
    "        ROUND(MAX(ventas), 2) as ventas_max\n",
    "    FROM ts_data\n",
    "),\n",
    "weekly_avg AS (\n",
    "    SELECT ROUND(AVG(ventas), 2) as ventas_semana_actual\n",
    "    FROM ts_data\n",
    "    WHERE fecha >= (SELECT MAX(fecha) - INTERVAL '7 days' FROM ts_data)\n",
    "),\n",
    "forecast_avg AS (\n",
    "    SELECT \n",
    "        ROUND(AVG(prediccion_prophet), 2) as forecast_prophet,\n",
    "        ROUND(AVG(prediccion_arima), 2) as forecast_arima\n",
    "    FROM predictions\n",
    "    WHERE fecha <= (SELECT MIN(fecha) + INTERVAL '30 days' FROM predictions)\n",
    ")\n",
    "SELECT \n",
    "    s.total_dias,\n",
    "    s.ventas_avg,\n",
    "    s.ventas_total,\n",
    "    s.ventas_min,\n",
    "    s.ventas_max,\n",
    "    w.ventas_semana_actual,\n",
    "    f.forecast_prophet as prediccion_30d_prophet,\n",
    "    f.forecast_arima as prediccion_30d_arima,\n",
    "    ROUND((w.ventas_semana_actual - s.ventas_avg) / s.ventas_avg * 100, 2) as variacion_vs_promedio_pct\n",
    "FROM stats s, weekly_avg w, forecast_avg f\n",
    "\"\"\"\n",
    "\n",
    "dashboard_data = con.execute(dashboard_query).df()\n",
    "\n",
    "print(\"üìä DASHBOARD EJECUTIVO\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nüìà M√âTRICAS GENERALES:\")\n",
    "print(f\"   D√≠as analizados: {dashboard_data['total_dias'][0]}\")\n",
    "print(f\"   Ventas totales: ${dashboard_data['ventas_total'][0]:,.2f}\")\n",
    "print(f\"   Ventas promedio: ${dashboard_data['ventas_avg'][0]:,.2f}\")\n",
    "print(f\"   Rango: ${dashboard_data['ventas_min'][0]:.2f} - ${dashboard_data['ventas_max'][0]:.2f}\")\n",
    "\n",
    "print(f\"\\nüìä SEMANA ACTUAL:\")\n",
    "print(f\"   Promedio √∫ltimos 7 d√≠as: ${dashboard_data['ventas_semana_actual'][0]:,.2f}\")\n",
    "print(f\"   Variaci√≥n vs promedio hist√≥rico: {dashboard_data['variacion_vs_promedio_pct'][0]:+.2f}%\")\n",
    "\n",
    "print(f\"\\nüîÆ PREDICCIONES (pr√≥ximos 30 d√≠as):\")\n",
    "print(f\"   Prophet: ${dashboard_data['prediccion_30d_prophet'][0]:,.2f} promedio diario\")\n",
    "print(f\"   ARIMA: ${dashboard_data['prediccion_30d_arima'][0]:,.2f} promedio diario\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Exportar Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportar a Parquet (formato eficiente)\n",
    "con.execute(\"COPY ts_data TO 'timeseries_data.parquet' (FORMAT PARQUET)\")\n",
    "con.execute(\"COPY predictions TO 'predictions.parquet' (FORMAT PARQUET)\")\n",
    "\n",
    "print(\"‚úÖ Datos exportados a Parquet\")\n",
    "\n",
    "# Exportar a CSV\n",
    "predictions_df.to_csv('predictions.csv', index=False)\n",
    "print(\"‚úÖ Predicciones exportadas a CSV\")\n",
    "\n",
    "# Guardar base de datos DuckDB en archivo\n",
    "con.execute(\"EXPORT DATABASE 'timeseries_backup' (FORMAT PARQUET)\")\n",
    "print(\"‚úÖ Base de datos completa exportada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Resumen y Mejores Pr√°cticas\n",
    "\n",
    "### ‚úÖ Conceptos Clave:\n",
    "1. **DuckDB**: SQL analytics in-process, ideal para data science\n",
    "2. **Time Series**: An√°lisis temporal con tendencias y estacionalidad\n",
    "3. **Prophet**: Forecasting robusto con componentes interpretables\n",
    "4. **ARIMA**: Modelo estad√≠stico cl√°sico para series temporales\n",
    "5. **LLM Analysis**: Generaci√≥n autom√°tica de insights\n",
    "\n",
    "### üí° Ventajas de DuckDB:\n",
    "- ‚úÖ Extremadamente r√°pido para queries anal√≠ticos\n",
    "- ‚úÖ Integraci√≥n perfecta con Pandas/NumPy\n",
    "- ‚úÖ No requiere servidor (in-process)\n",
    "- ‚úÖ Soporte completo SQL con ventanas, CTEs, etc.\n",
    "- ‚úÖ Formato columnar eficiente (Parquet nativo)\n",
    "- ‚úÖ Ideal para notebooks y an√°lisis exploratorio\n",
    "\n",
    "### üí° Time Series Mejores Pr√°cticas:\n",
    "- ‚úÖ Identifica componentes: tendencia, estacionalidad, ruido\n",
    "- ‚úÖ Usa m√∫ltiples modelos y compara resultados\n",
    "- ‚úÖ Valida con datos out-of-sample\n",
    "- ‚úÖ Considera eventos especiales y outliers\n",
    "- ‚úÖ Actualiza modelos regularmente con nuevos datos\n",
    "\n",
    "### üöÄ Pr√≥ximos Pasos:\n",
    "- Deep Learning para time series (LSTM, Transformers)\n",
    "- Anomaly detection en tiempo real\n",
    "- Multi-variate forecasting\n",
    "- AutoML para time series (AutoARIMA, NeuralProphet)\n",
    "- Integraci√≥n con Airflow para pipelines automatizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cerrar conexi√≥n DuckDB\n",
    "con.close()\n",
    "\n",
    "print(\"üéâ Tutorial de DuckDB + Time Series + LLM completado!\")\n",
    "print(\"\\nüìä Resumen:\")\n",
    "print(\"   - Serie temporal de 730 d√≠as analizada\")\n",
    "print(\"   - 2 modelos de forecasting entrenados (Prophet, ARIMA)\")\n",
    "print(\"   - 90 d√≠as de predicciones generadas\")\n",
    "print(\"   - Insights generados con LLM\")\n",
    "print(\"   - Datos almacenados en DuckDB\")\n",
    "print(\"\\nüìÅ Archivos generados:\")\n",
    "print(\"   - timeseries_data.parquet\")\n",
    "print(\"   - predictions.parquet\")\n",
    "print(\"   - predictions.csv\")\n",
    "print(\"   - timeseries_backup/ (DuckDB export)\")\n",
    "print(\"\\n‚úÖ Conexi√≥n DuckDB cerrada\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
