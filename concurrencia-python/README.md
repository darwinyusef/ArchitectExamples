# Concurrencia y Paralelismo en Python üöÄ

Sistema completo de ejemplos pr√°cticos sobre concurrencia, paralelismo y observabilidad en Python con FastAPI.

## üìã Tabla de Contenidos

- [Descripci√≥n](#descripci√≥n)
- [Conceptos Cubiertos](#conceptos-cubiertos)
- [Instalaci√≥n](#instalaci√≥n)
- [Uso R√°pido](#uso-r√°pido)
- [Endpoints Disponibles](#endpoints-disponibles)
- [Ejemplos Pr√°cticos](#ejemplos-pr√°cticos)
- [Observabilidad](#observabilidad)
- [GIL (Global Interpreter Lock)](#gil-global-interpreter-lock)
- [Cu√°ndo Usar Qu√©](#cu√°ndo-usar-qu√©)

## üéØ Descripci√≥n

Este proyecto es una gu√≠a pr√°ctica completa que demuestra todos los aspectos de la concurrencia y paralelismo en Python, incluyendo:

- Diferencias entre IO-bound y CPU-bound
- Threading vs Multiprocessing
- Async/await y event loops
- Problemas comunes: Race conditions, Deadlocks, Starvation
- Sincronizaci√≥n: Locks, Semaphores, Events
- Comunicaci√≥n: Queues (FIFO, LIFO, Priority)
- Patrones avanzados: run_in_executor, thread pools
- Observabilidad completa con m√©tricas

## ‚ú® Conceptos Cubiertos

### 1. IO-bound vs CPU-bound
- **IO-bound**: Operaciones que esperan I/O (red, disco, APIs)
- **CPU-bound**: C√°lculos intensivos que usan el procesador

### 2. Threading
- Threads b√°sicos
- Thread pools (ThreadPoolExecutor)
- Daemon threads
- Locks y sincronizaci√≥n

### 3. Multiprocessing
- Procesos independientes (evitan el GIL)
- Process pools
- Comunicaci√≥n entre procesos (Queues)

### 4. Async/await
- Coroutines y event loops
- asyncio.gather, as_completed
- Timeouts y cancellation
- Semaphores as√≠ncronos

### 5. Problemas de Concurrencia
- **Race Conditions**: Acceso concurrente a recursos
- **Deadlocks**: Bloqueo mutuo de recursos
- **Starvation**: Threads que nunca obtienen recursos

### 6. Sincronizaci√≥n
- **Locks**: Exclusi√≥n mutua
- **RLocks**: Locks reentrant
- **Semaphores**: Limitar concurrencia
- **Events**: Se√±alizaci√≥n entre threads

### 7. Queues
- Queue (FIFO)
- LifoQueue (LIFO/Stack)
- PriorityQueue
- Bounded queues

## üöÄ Instalaci√≥n

### Opci√≥n 1: Local

```bash
# Clonar el repositorio
cd concurrencia-python/backend

# Crear entorno virtual
python -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate

# Instalar dependencias
pip install -r requirements.txt

# Ejecutar servidor
python main.py
```

### Opci√≥n 2: Docker (Recomendado)

```bash
cd concurrencia-python
docker-compose up
```

El servidor estar√° disponible en: `http://localhost:8000`

## üìñ Uso R√°pido

### 1. Ver Documentaci√≥n Interactiva

Abre tu navegador en:
- Swagger UI: `http://localhost:8000/docs`
- ReDoc: `http://localhost:8000/redoc`

### 2. Probar Endpoints

```bash
# IO-bound con threading (recomendado)
curl -X POST http://localhost:8000/io-bound/threading \
  -H "Content-Type: application/json" \
  -d '{"iterations": 5}'

# CPU-bound con multiprocessing (recomendado)
curl -X POST http://localhost:8000/cpu-bound/multiprocessing \
  -H "Content-Type: application/json" \
  -d '{"iterations": 4}'

# Ver m√©tricas
curl http://localhost:8000/metrics/summary
```

## üåê Endpoints Disponibles

### IO-bound
- `POST /io-bound/sequential` - Secuencial (bloqueante) ‚ùå
- `POST /io-bound/threading` - Con threads ‚úÖ
- `POST /io-bound/async` - Con async/await ‚úÖ‚úÖ

### CPU-bound
- `POST /cpu-bound/sequential` - Secuencial
- `POST /cpu-bound/threading` - Con threads (no ayuda por GIL) ‚ùå
- `POST /cpu-bound/multiprocessing` - Con procesos ‚úÖ‚úÖ

### Race Conditions
- `POST /race-conditions/unsafe` - Demuestra race condition
- `POST /race-conditions/lock` - Soluci√≥n con Lock
- `POST /race-conditions/rlock` - Lock reentrant

### Deadlocks
- `POST /deadlocks/demonstrate` - Demuestra deadlock
- `POST /deadlocks/prevent` - Prevenci√≥n con orden consistente

### Threading
- `POST /threading/basic` - Threads b√°sicos
- `POST /threading/pool` - Thread pool
- `POST /threading/daemon` - Daemon threads

### Multiprocessing
- `POST /multiprocessing/basic` - Procesos b√°sicos
- `POST /multiprocessing/queue` - Comunicaci√≥n con queues
- `POST /multiprocessing/pool` - Process pool

### Async/await
- `POST /async/gather` - asyncio.gather
- `POST /async/run-in-executor` - Mezclar async con blocking
- `POST /async/timeout` - Timeouts
- `POST /async/as-completed` - Procesar conforme completan

### Queues
- `POST /queues/producer-consumer` - Patr√≥n productor-consumidor
- `POST /queues/priority` - Priority queue
- `POST /queues/lifo` - LIFO queue (stack)
- `POST /queues/bounded` - Queue con tama√±o m√°ximo

### Semaphores
- `POST /semaphores/rate-limit` - Rate limiting
- `POST /semaphores/threading` - Sem√°foro con threads
- `POST /semaphores/bounded` - BoundedSemaphore
- `POST /semaphores/connection-pool` - Pool de conexiones

### Observabilidad
- `GET /metrics/summary` - Resumen de m√©tricas
- `GET /metrics/operation/{name}` - Stats de operaci√≥n espec√≠fica
- `GET /metrics/system` - M√©tricas del sistema
- `POST /metrics/reset` - Limpiar m√©tricas
- `GET /prometheus/metrics` - M√©tricas formato Prometheus

## üí° Ejemplos Pr√°cticos

### Ejemplo 1: Comparar IO-bound

```python
import requests

# 1. Secuencial (lento - 5 segundos)
r1 = requests.post('http://localhost:8000/io-bound/sequential',
                   json={'iterations': 5})
print(f"Sequential: {r1.json()['duration']:.2f}s")

# 2. Threading (r√°pido - ~1 segundo)
r2 = requests.post('http://localhost:8000/io-bound/threading',
                   json={'iterations': 5})
print(f"Threading: {r2.json()['duration']:.2f}s")

# 3. Async (m√°s r√°pido - ~1 segundo)
r3 = requests.post('http://localhost:8000/io-bound/async',
                   json={'iterations': 5})
print(f"Async: {r3.json()['duration']:.2f}s")
```

### Ejemplo 2: Demostrar Race Condition

```python
# Sin Lock - resultados inconsistentes
r1 = requests.post('http://localhost:8000/race-conditions/unsafe',
                   json={'num_threads': 10, 'increments_per_thread': 1000})
print(f"Sin Lock: {r1.json()['final_counter']} (esperado: 10000)")

# Con Lock - resultados correctos
r2 = requests.post('http://localhost:8000/race-conditions/lock',
                   json={'num_threads': 10, 'increments_per_thread': 1000})
print(f"Con Lock: {r2.json()['final_counter']} (correcto)")
```

### Ejemplo 3: CPU-bound con GIL

```python
# Threading NO ayuda (por el GIL)
r1 = requests.post('http://localhost:8000/cpu-bound/threading',
                   json={'iterations': 4})
print(f"Threading: {r1.json()['duration']:.2f}s")

# Multiprocessing S√ç ayuda (evita GIL)
r2 = requests.post('http://localhost:8000/cpu-bound/multiprocessing',
                   json={'iterations': 4})
print(f"Multiprocessing: {r2.json()['duration']:.2f}s")
print(f"Speedup: {r1.json()['duration'] / r2.json()['duration']:.2f}x")
```

## üìä Observabilidad

El sistema incluye observabilidad completa:

```python
# Ver resumen de m√©tricas
r = requests.get('http://localhost:8000/metrics/summary')
print(r.json())

# M√©tricas del sistema
r = requests.get('http://localhost:8000/metrics/system')
print(r.json())
```

M√©tricas recolectadas:
- Duraci√≥n de ejecuciones
- Uso de CPU por operaci√≥n
- Uso de memoria
- N√∫mero de threads/procesos usados
- Tasa de √©xito/fallo
- Contadores personalizados

## üîí GIL (Global Interpreter Lock)

### ¬øQu√© es el GIL?

El GIL es un mutex que protege el acceso a objetos Python, permitiendo que solo un thread ejecute bytecode de Python a la vez.

### Implicaciones

1. **IO-bound**: El GIL se libera durante operaciones de I/O
   - ‚úÖ Threading funciona bien
   - ‚úÖ Async funciona excelente

2. **CPU-bound**: El GIL NO se libera durante c√°lculos
   - ‚ùå Threading NO ayuda (puede ser m√°s lento)
   - ‚úÖ Multiprocessing evita el GIL

### Ejemplo del GIL

```python
# CPU-bound con threading (malo)
def compute():
    total = 0
    for i in range(10_000_000):
        total += i * i
    return total

# Un thread: 1.0s
# Dos threads: 1.2s (m√°s lento por overhead del GIL)
# Dos procesos: 0.5s (evita el GIL)
```

## üéì Cu√°ndo Usar Qu√©

### IO-bound (APIs, DB, archivos)
1. **Async/await** ‚úÖ‚úÖ - Mejor opci√≥n
   - M√°s eficiente en memoria
   - Excelente para miles de conexiones
   ```python
   await asyncio.gather(*[fetch_api(i) for i in range(1000)])
   ```

2. **Threading** ‚úÖ - Buena opci√≥n
   - M√°s simple
   - Bueno para decenas/cientos de operaciones
   ```python
   with ThreadPoolExecutor() as executor:
       results = executor.map(fetch_api, range(100))
   ```

### CPU-bound (c√°lculos, procesamiento)
1. **Multiprocessing** ‚úÖ‚úÖ - √önica opci√≥n real
   - Evita el GIL
   - Usa todos los cores
   ```python
   with ProcessPoolExecutor() as executor:
       results = executor.map(calculate, data)
   ```

2. **Threading** ‚ùå - No usar
   - No ayuda por el GIL
   - Puede ser m√°s lento

### C√≥digo bloqueante en async
**run_in_executor** ‚úÖ
```python
loop = asyncio.get_event_loop()
result = await loop.run_in_executor(None, blocking_function)
```

### Limitar concurrencia
**Semaphore** ‚úÖ
```python
sem = asyncio.Semaphore(10)  # Max 10 concurrentes
async with sem:
    await operation()
```

### Comunicaci√≥n entre workers
**Queue** ‚úÖ
```python
queue = Queue()
# Producer
queue.put(item)
# Consumer
item = queue.get()
```

## üìö Recursos Adicionales

- [GUIA.md](./GUIA.md) - Gu√≠a detallada de conceptos
- [C√≥digo de ejemplos](./backend/examples/) - Implementaciones completas
- [FastAPI Docs](https://fastapi.tiangolo.com/)
- [Python asyncio](https://docs.python.org/3/library/asyncio.html)

## üêõ Problemas Comunes

### Race Condition
```python
# ‚ùå Incorrecto
counter += 1

# ‚úÖ Correcto
with lock:
    counter += 1
```

### Deadlock
```python
# ‚ùå Incorrecto
# Thread 1: lock_a ‚Üí lock_b
# Thread 2: lock_b ‚Üí lock_a

# ‚úÖ Correcto - orden consistente
# Thread 1: lock_a ‚Üí lock_b
# Thread 2: lock_a ‚Üí lock_b
```

### Blocking en Async
```python
# ‚ùå Incorrecto
async def bad():
    time.sleep(1)  # Bloquea el event loop

# ‚úÖ Correcto
async def good():
    await asyncio.sleep(1)  # No bloqueante
```

## üìù Licencia

MIT

## üë• Autor

Proyecto educativo para aprender concurrencia y paralelismo en Python.
