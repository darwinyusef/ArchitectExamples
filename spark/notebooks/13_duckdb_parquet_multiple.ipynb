{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13 - DuckDB: Procesamiento de M√∫ltiples Archivos Parquet\n",
    "\n",
    "## üéØ Objetivos\n",
    "- Trabajar con m√∫ltiples archivos Parquet\n",
    "- Particionamiento de datos\n",
    "- Queries eficientes sobre datasets particionados\n",
    "- Union y combinaci√≥n de archivos\n",
    "- Optimizaciones y mejores pr√°cticas\n",
    "- An√°lisis de datos distribuidos\n",
    "\n",
    "## üìö Tecnolog√≠as\n",
    "- **DuckDB**: SQL analytics engine\n",
    "- **Parquet**: Formato columnar\n",
    "- **Pandas**: Manipulaci√≥n de datos\n",
    "- **PyArrow**: Backend de Parquet\n",
    "\n",
    "## ‚≠ê Complejidad: Intermedio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Instalaci√≥n y Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar dependencias\n",
    "!pip install duckdb pandas numpy pyarrow matplotlib seaborn plotly faker -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"‚úÖ DuckDB version: {duckdb.__version__}\")\n",
    "print(f\"‚úÖ PyArrow version: {pa.__version__}\")\n",
    "print(f\"‚úÖ Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generar Datos de Prueba\n",
    "\n",
    "Crearemos datos sint√©ticos de e-commerce distribuidos en m√∫ltiples archivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "import random\n",
    "\n",
    "fake = Faker()\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Par√°metros\n",
    "NUM_CUSTOMERS = 1000\n",
    "NUM_PRODUCTS = 100\n",
    "NUM_TRANSACTIONS = 50000\n",
    "\n",
    "# Generar cat√°logo de productos\n",
    "categories = ['Electr√≥nica', 'Ropa', 'Hogar', 'Deportes', 'Libros', 'Juguetes', 'Alimentos']\n",
    "products = []\n",
    "for i in range(NUM_PRODUCTS):\n",
    "    products.append({\n",
    "        'product_id': i + 1,\n",
    "        'product_name': fake.catch_phrase(),\n",
    "        'category': random.choice(categories),\n",
    "        'price': round(random.uniform(10, 1000), 2),\n",
    "        'cost': round(random.uniform(5, 500), 2)\n",
    "    })\n",
    "\n",
    "products_df = pd.DataFrame(products)\n",
    "print(\"üì¶ Productos generados:\")\n",
    "print(products_df.head(10))\n",
    "\n",
    "# Generar clientes\n",
    "customers = []\n",
    "for i in range(NUM_CUSTOMERS):\n",
    "    customers.append({\n",
    "        'customer_id': i + 1,\n",
    "        'name': fake.name(),\n",
    "        'email': fake.email(),\n",
    "        'country': fake.country(),\n",
    "        'city': fake.city(),\n",
    "        'signup_date': fake.date_between(start_date='-2y', end_date='today')\n",
    "    })\n",
    "\n",
    "customers_df = pd.DataFrame(customers)\n",
    "print(\"\\nüë• Clientes generados:\")\n",
    "print(customers_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generar Transacciones Particionadas por Fecha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar transacciones para los √∫ltimos 2 a√±os\n",
    "start_date = datetime.now() - timedelta(days=730)\n",
    "end_date = datetime.now()\n",
    "\n",
    "transactions = []\n",
    "for i in range(NUM_TRANSACTIONS):\n",
    "    transaction_date = fake.date_time_between(start_date=start_date, end_date=end_date)\n",
    "    product = products_df.sample(1).iloc[0]\n",
    "    customer = customers_df.sample(1).iloc[0]\n",
    "    quantity = random.randint(1, 5)\n",
    "    \n",
    "    transactions.append({\n",
    "        'transaction_id': i + 1,\n",
    "        'transaction_date': transaction_date,\n",
    "        'customer_id': customer['customer_id'],\n",
    "        'product_id': product['product_id'],\n",
    "        'quantity': quantity,\n",
    "        'unit_price': product['price'],\n",
    "        'total_amount': round(quantity * product['price'], 2),\n",
    "        'payment_method': random.choice(['Credit Card', 'Debit Card', 'PayPal', 'Cash']),\n",
    "        'status': random.choice(['completed', 'completed', 'completed', 'pending', 'cancelled'])\n",
    "    })\n",
    "\n",
    "transactions_df = pd.DataFrame(transactions)\n",
    "\n",
    "# Agregar columnas de partici√≥n\n",
    "transactions_df['year'] = transactions_df['transaction_date'].dt.year\n",
    "transactions_df['month'] = transactions_df['transaction_date'].dt.month\n",
    "transactions_df['day'] = transactions_df['transaction_date'].dt.day\n",
    "\n",
    "print(f\"üí≥ Transacciones generadas: {len(transactions_df):,}\")\n",
    "print(f\"üìä Rango: {transactions_df['transaction_date'].min()} a {transactions_df['transaction_date'].max()}\")\n",
    "print(f\"\\nüìä Muestra de transacciones:\")\n",
    "print(transactions_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Guardar Datos en M√∫ltiples Archivos Parquet\n",
    "\n",
    "Crearemos estructura particionada por a√±o y mes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear directorios\n",
    "base_path = Path('data_parquet')\n",
    "if base_path.exists():\n",
    "    shutil.rmtree(base_path)\n",
    "base_path.mkdir(exist_ok=True)\n",
    "\n",
    "# Guardar productos y clientes (sin particionar)\n",
    "products_df.to_parquet(base_path / 'products.parquet', index=False)\n",
    "customers_df.to_parquet(base_path / 'customers.parquet', index=False)\n",
    "\n",
    "print(\"‚úÖ Guardados:\")\n",
    "print(f\"   - products.parquet\")\n",
    "print(f\"   - customers.parquet\")\n",
    "\n",
    "# Guardar transacciones particionadas por a√±o/mes\n",
    "transactions_path = base_path / 'transactions'\n",
    "transactions_path.mkdir(exist_ok=True)\n",
    "\n",
    "file_count = 0\n",
    "for year in transactions_df['year'].unique():\n",
    "    year_path = transactions_path / f'year={year}'\n",
    "    year_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    for month in transactions_df[transactions_df['year'] == year]['month'].unique():\n",
    "        month_path = year_path / f'month={month:02d}'\n",
    "        month_path.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Filtrar datos para esta partici√≥n\n",
    "        partition_data = transactions_df[\n",
    "            (transactions_df['year'] == year) & \n",
    "            (transactions_df['month'] == month)\n",
    "        ].copy()\n",
    "        \n",
    "        # Remover columnas de partici√≥n (est√°n en el path)\n",
    "        partition_data = partition_data.drop(columns=['year', 'month'])\n",
    "        \n",
    "        # Guardar\n",
    "        partition_file = month_path / 'data.parquet'\n",
    "        partition_data.to_parquet(partition_file, index=False)\n",
    "        file_count += 1\n",
    "        print(f\"‚úÖ {partition_file} ({len(partition_data)} registros)\")\n",
    "\n",
    "print(f\"\\nüìä Total de archivos Parquet creados: {file_count + 2}\")\n",
    "print(f\"üìä Estructura:\")\n",
    "print(f\"   data_parquet/\")\n",
    "print(f\"   ‚îú‚îÄ‚îÄ products.parquet\")\n",
    "print(f\"   ‚îú‚îÄ‚îÄ customers.parquet\")\n",
    "print(f\"   ‚îî‚îÄ‚îÄ transactions/\")\n",
    "print(f\"       ‚îú‚îÄ‚îÄ year=2023/month=01/data.parquet\")\n",
    "print(f\"       ‚îú‚îÄ‚îÄ year=2023/month=02/data.parquet\")\n",
    "print(f\"       ‚îî‚îÄ‚îÄ ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Leer M√∫ltiples Archivos Parquet con DuckDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conectar a DuckDB\n",
    "con = duckdb.connect(':memory:')\n",
    "\n",
    "print(\"‚úÖ DuckDB conectado\")\n",
    "\n",
    "# Leer archivo √∫nico\n",
    "result = con.execute(\"\"\"\n",
    "    SELECT * FROM 'data_parquet/products.parquet' LIMIT 5\n",
    "\"\"\").df()\n",
    "\n",
    "print(\"\\nüì¶ Productos (archivo √∫nico):\")\n",
    "print(result)\n",
    "\n",
    "# Leer TODOS los archivos de transacciones con glob pattern\n",
    "result = con.execute(\"\"\"\n",
    "    SELECT COUNT(*) as total_transactions\n",
    "    FROM 'data_parquet/transactions/**/*.parquet'\n",
    "\"\"\").df()\n",
    "\n",
    "print(f\"\\nüí≥ Total transacciones (m√∫ltiples archivos): {result['total_transactions'][0]:,}\")\n",
    "\n",
    "# Ver muestra\n",
    "result = con.execute(\"\"\"\n",
    "    SELECT * \n",
    "    FROM 'data_parquet/transactions/**/*.parquet'\n",
    "    LIMIT 10\n",
    "\"\"\").df()\n",
    "\n",
    "print(\"\\nüìä Muestra de transacciones:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Queries con Partici√≥n Pushdown\n",
    "\n",
    "DuckDB puede leer solo las particiones necesarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query que solo lee particiones espec√≠ficas\n",
    "# DuckDB detecta el filtro year=2024, month=1 y solo lee esos archivos\n",
    "result = con.execute(\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as transactions,\n",
    "        SUM(total_amount) as total_revenue,\n",
    "        AVG(total_amount) as avg_transaction\n",
    "    FROM read_parquet('data_parquet/transactions/year=2024/month=01/*.parquet')\n",
    "\"\"\").df()\n",
    "\n",
    "print(\"üìä Estad√≠sticas Enero 2024 (solo 1 archivo):\")\n",
    "print(result)\n",
    "\n",
    "# Query m√°s complejo con m√∫ltiples meses\n",
    "result = con.execute(\"\"\"\n",
    "    SELECT \n",
    "        DATE_TRUNC('month', transaction_date) as month,\n",
    "        COUNT(*) as transactions,\n",
    "        ROUND(SUM(total_amount), 2) as revenue,\n",
    "        ROUND(AVG(total_amount), 2) as avg_transaction,\n",
    "        COUNT(DISTINCT customer_id) as unique_customers\n",
    "    FROM 'data_parquet/transactions/**/*.parquet'\n",
    "    GROUP BY month\n",
    "    ORDER BY month DESC\n",
    "    LIMIT 12\n",
    "\"\"\").df()\n",
    "\n",
    "print(\"\\nüìä √öltimos 12 meses:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Joins entre M√∫ltiples Archivos Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join transacciones con productos y clientes\n",
    "result = con.execute(\"\"\"\n",
    "    SELECT \n",
    "        t.transaction_id,\n",
    "        t.transaction_date,\n",
    "        c.name as customer_name,\n",
    "        c.country,\n",
    "        p.product_name,\n",
    "        p.category,\n",
    "        t.quantity,\n",
    "        t.total_amount,\n",
    "        t.status\n",
    "    FROM 'data_parquet/transactions/**/*.parquet' t\n",
    "    JOIN 'data_parquet/customers.parquet' c ON t.customer_id = c.customer_id\n",
    "    JOIN 'data_parquet/products.parquet' p ON t.product_id = p.product_id\n",
    "    WHERE t.status = 'completed'\n",
    "    ORDER BY t.transaction_date DESC\n",
    "    LIMIT 10\n",
    "\"\"\").df()\n",
    "\n",
    "print(\"üìä Transacciones completas (con joins):\")\n",
    "print(result)\n",
    "\n",
    "# An√°lisis de ventas por categor√≠a y pa√≠s\n",
    "result = con.execute(\"\"\"\n",
    "    SELECT \n",
    "        p.category,\n",
    "        c.country,\n",
    "        COUNT(*) as transactions,\n",
    "        SUM(t.quantity) as units_sold,\n",
    "        ROUND(SUM(t.total_amount), 2) as revenue\n",
    "    FROM 'data_parquet/transactions/**/*.parquet' t\n",
    "    JOIN 'data_parquet/customers.parquet' c ON t.customer_id = c.customer_id\n",
    "    JOIN 'data_parquet/products.parquet' p ON t.product_id = p.product_id\n",
    "    WHERE t.status = 'completed'\n",
    "    GROUP BY p.category, c.country\n",
    "    ORDER BY revenue DESC\n",
    "    LIMIT 20\n",
    "\"\"\").df()\n",
    "\n",
    "print(\"\\nüìä Ventas por Categor√≠a y Pa√≠s (Top 20):\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. An√°lisis de Performance: Single vs Multiple Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Test 1: Leer todo de m√∫ltiples archivos particionados\n",
    "start = time.time()\n",
    "result1 = con.execute(\"\"\"\n",
    "    SELECT COUNT(*), SUM(total_amount)\n",
    "    FROM 'data_parquet/transactions/**/*.parquet'\n",
    "\"\"\").df()\n",
    "time1 = time.time() - start\n",
    "\n",
    "print(f\"‚è±Ô∏è Query sobre archivos particionados: {time1*1000:.2f}ms\")\n",
    "print(f\"üìä Resultado: {result1}\")\n",
    "\n",
    "# Test 2: Leer solo una partici√≥n\n",
    "start = time.time()\n",
    "result2 = con.execute(\"\"\"\n",
    "    SELECT COUNT(*), SUM(total_amount)\n",
    "    FROM 'data_parquet/transactions/year=2024/month=01/*.parquet'\n",
    "\"\"\").df()\n",
    "time2 = time.time() - start\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è Query sobre 1 partici√≥n: {time2*1000:.2f}ms\")\n",
    "print(f\"üìä Resultado: {result2}\")\n",
    "\n",
    "# Crear un archivo √∫nico para comparaci√≥n\n",
    "single_file_path = base_path / 'transactions_single.parquet'\n",
    "transactions_df.to_parquet(single_file_path, index=False)\n",
    "\n",
    "# Test 3: Leer archivo √∫nico grande\n",
    "start = time.time()\n",
    "result3 = con.execute(\"\"\"\n",
    "    SELECT COUNT(*), SUM(total_amount)\n",
    "    FROM 'data_parquet/transactions_single.parquet'\n",
    "\"\"\").df()\n",
    "time3 = time.time() - start\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è Query sobre archivo √∫nico: {time3*1000:.2f}ms\")\n",
    "print(f\"üìä Resultado: {result3}\")\n",
    "\n",
    "print(f\"\\nüìä Comparaci√≥n de Performance:\")\n",
    "print(f\"   Archivos particionados: {time1*1000:.2f}ms\")\n",
    "print(f\"   Una partici√≥n: {time2*1000:.2f}ms ({time1/time2:.2f}x m√°s lento)\")\n",
    "print(f\"   Archivo √∫nico: {time3*1000:.2f}ms\")\n",
    "print(f\"\\nüí° Particionamiento permite leer solo datos necesarios!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Crear Vistas sobre M√∫ltiples Parquet Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear vistas para simplificar queries\n",
    "con.execute(\"\"\"\n",
    "    CREATE OR REPLACE VIEW transactions_view AS\n",
    "    SELECT * FROM 'data_parquet/transactions/**/*.parquet'\n",
    "\"\"\")\n",
    "\n",
    "con.execute(\"\"\"\n",
    "    CREATE OR REPLACE VIEW products_view AS\n",
    "    SELECT * FROM 'data_parquet/products.parquet'\n",
    "\"\"\")\n",
    "\n",
    "con.execute(\"\"\"\n",
    "    CREATE OR REPLACE VIEW customers_view AS\n",
    "    SELECT * FROM 'data_parquet/customers.parquet'\n",
    "\"\"\")\n",
    "\n",
    "# Crear vista materializada con joins\n",
    "con.execute(\"\"\"\n",
    "    CREATE OR REPLACE VIEW sales_detailed AS\n",
    "    SELECT \n",
    "        t.transaction_id,\n",
    "        t.transaction_date,\n",
    "        t.customer_id,\n",
    "        c.name as customer_name,\n",
    "        c.country,\n",
    "        c.city,\n",
    "        t.product_id,\n",
    "        p.product_name,\n",
    "        p.category,\n",
    "        p.price as product_price,\n",
    "        p.cost as product_cost,\n",
    "        t.quantity,\n",
    "        t.total_amount,\n",
    "        t.payment_method,\n",
    "        t.status,\n",
    "        (t.total_amount - (p.cost * t.quantity)) as profit\n",
    "    FROM transactions_view t\n",
    "    JOIN customers_view c ON t.customer_id = c.customer_id\n",
    "    JOIN products_view p ON t.product_id = p.product_id\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úÖ Vistas creadas\")\n",
    "\n",
    "# Usar vistas para queries m√°s simples\n",
    "result = con.execute(\"\"\"\n",
    "    SELECT \n",
    "        category,\n",
    "        COUNT(*) as transactions,\n",
    "        ROUND(SUM(total_amount), 2) as revenue,\n",
    "        ROUND(SUM(profit), 2) as profit,\n",
    "        ROUND(SUM(profit) / SUM(total_amount) * 100, 2) as profit_margin_pct\n",
    "    FROM sales_detailed\n",
    "    WHERE status = 'completed'\n",
    "    GROUP BY category\n",
    "    ORDER BY revenue DESC\n",
    "\"\"\").df()\n",
    "\n",
    "print(\"\\nüìä An√°lisis de Rentabilidad por Categor√≠a:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Agregaciones Complejas con Window Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lisis de tendencias mensuales con window functions\n",
    "result = con.execute(\"\"\"\n",
    "    WITH monthly_sales AS (\n",
    "        SELECT \n",
    "            DATE_TRUNC('month', transaction_date) as month,\n",
    "            category,\n",
    "            SUM(total_amount) as revenue\n",
    "        FROM sales_detailed\n",
    "        WHERE status = 'completed'\n",
    "        GROUP BY month, category\n",
    "    )\n",
    "    SELECT \n",
    "        month,\n",
    "        category,\n",
    "        ROUND(revenue, 2) as revenue,\n",
    "        ROUND(LAG(revenue) OVER (PARTITION BY category ORDER BY month), 2) as prev_month_revenue,\n",
    "        ROUND(\n",
    "            (revenue - LAG(revenue) OVER (PARTITION BY category ORDER BY month)) / \n",
    "            LAG(revenue) OVER (PARTITION BY category ORDER BY month) * 100, \n",
    "            2\n",
    "        ) as growth_pct,\n",
    "        ROUND(AVG(revenue) OVER (PARTITION BY category ORDER BY month ROWS BETWEEN 2 PRECEDING AND CURRENT ROW), 2) as moving_avg_3m\n",
    "    FROM monthly_sales\n",
    "    ORDER BY category, month DESC\n",
    "\"\"\").df()\n",
    "\n",
    "print(\"üìä Tendencias Mensuales por Categor√≠a:\")\n",
    "print(result.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Exportar Resultados a Nuevos Parquet Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear directorio de analytics\n",
    "analytics_path = base_path / 'analytics'\n",
    "analytics_path.mkdir(exist_ok=True)\n",
    "\n",
    "# Exportar an√°lisis de categor√≠as\n",
    "con.execute(\"\"\"\n",
    "    COPY (\n",
    "        SELECT \n",
    "            category,\n",
    "            COUNT(*) as transactions,\n",
    "            SUM(total_amount) as revenue,\n",
    "            SUM(profit) as profit,\n",
    "            AVG(total_amount) as avg_transaction\n",
    "        FROM sales_detailed\n",
    "        WHERE status = 'completed'\n",
    "        GROUP BY category\n",
    "    ) TO 'data_parquet/analytics/category_summary.parquet' (FORMAT PARQUET)\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úÖ Exportado: category_summary.parquet\")\n",
    "\n",
    "# Exportar top customers\n",
    "con.execute(\"\"\"\n",
    "    COPY (\n",
    "        SELECT \n",
    "            customer_id,\n",
    "            customer_name,\n",
    "            country,\n",
    "            COUNT(*) as total_orders,\n",
    "            SUM(total_amount) as total_spent,\n",
    "            AVG(total_amount) as avg_order_value,\n",
    "            MAX(transaction_date) as last_purchase\n",
    "        FROM sales_detailed\n",
    "        WHERE status = 'completed'\n",
    "        GROUP BY customer_id, customer_name, country\n",
    "        ORDER BY total_spent DESC\n",
    "        LIMIT 100\n",
    "    ) TO 'data_parquet/analytics/top_customers.parquet' (FORMAT PARQUET)\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úÖ Exportado: top_customers.parquet\")\n",
    "\n",
    "# Exportar an√°lisis temporal\n",
    "con.execute(\"\"\"\n",
    "    COPY (\n",
    "        SELECT \n",
    "            DATE_TRUNC('day', transaction_date) as date,\n",
    "            COUNT(*) as transactions,\n",
    "            SUM(total_amount) as revenue,\n",
    "            COUNT(DISTINCT customer_id) as unique_customers,\n",
    "            COUNT(DISTINCT product_id) as unique_products\n",
    "        FROM sales_detailed\n",
    "        WHERE status = 'completed'\n",
    "        GROUP BY date\n",
    "        ORDER BY date\n",
    "    ) TO 'data_parquet/analytics/daily_summary.parquet' (FORMAT PARQUET)\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úÖ Exportado: daily_summary.parquet\")\n",
    "\n",
    "# Listar archivos creados\n",
    "print(\"\\nüìÅ Archivos analytics creados:\")\n",
    "for f in analytics_path.glob('*.parquet'):\n",
    "    size_mb = f.stat().st_size / (1024 * 1024)\n",
    "    print(f\"   {f.name} ({size_mb:.2f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Visualizaciones con Plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer datos de analytics\n",
    "category_df = con.execute(\"\"\"\n",
    "    SELECT * FROM 'data_parquet/analytics/category_summary.parquet'\n",
    "    ORDER BY revenue DESC\n",
    "\"\"\").df()\n",
    "\n",
    "# Gr√°fico de barras - Revenue por categor√≠a\n",
    "fig = px.bar(\n",
    "    category_df, \n",
    "    x='category', \n",
    "    y='revenue',\n",
    "    title='Revenue por Categor√≠a',\n",
    "    labels={'category': 'Categor√≠a', 'revenue': 'Revenue ($)'},\n",
    "    color='profit',\n",
    "    color_continuous_scale='Viridis'\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Leer daily summary\n",
    "daily_df = con.execute(\"\"\"\n",
    "    SELECT * FROM 'data_parquet/analytics/daily_summary.parquet'\n",
    "    ORDER BY date\n",
    "\"\"\").df()\n",
    "\n",
    "# Time series de revenue\n",
    "fig = px.line(\n",
    "    daily_df,\n",
    "    x='date',\n",
    "    y='revenue',\n",
    "    title='Revenue Diario',\n",
    "    labels={'date': 'Fecha', 'revenue': 'Revenue ($)'}\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# An√°lisis de top customers\n",
    "top_customers_df = con.execute(\"\"\"\n",
    "    SELECT * FROM 'data_parquet/analytics/top_customers.parquet'\n",
    "    LIMIT 10\n",
    "\"\"\").df()\n",
    "\n",
    "fig = px.bar(\n",
    "    top_customers_df,\n",
    "    x='customer_name',\n",
    "    y='total_spent',\n",
    "    title='Top 10 Clientes por Gasto Total',\n",
    "    labels={'customer_name': 'Cliente', 'total_spent': 'Gasto Total ($)'},\n",
    "    color='total_orders',\n",
    "    color_continuous_scale='Blues'\n",
    ")\n",
    "fig.update_xaxis(tickangle=45)\n",
    "fig.show()\n",
    "\n",
    "print(\"‚úÖ Visualizaciones generadas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Union de M√∫ltiples Parquet con Diferentes Schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simular archivos con schemas diferentes (ej: datos de diferentes fuentes)\n",
    "legacy_path = base_path / 'legacy'\n",
    "legacy_path.mkdir(exist_ok=True)\n",
    "\n",
    "# Schema antiguo (sin algunas columnas)\n",
    "legacy_transactions = transactions_df[[\n",
    "    'transaction_id', 'transaction_date', 'customer_id', \n",
    "    'product_id', 'total_amount'\n",
    "]].head(100).copy()\n",
    "\n",
    "legacy_transactions.to_parquet(legacy_path / 'old_transactions.parquet', index=False)\n",
    "\n",
    "# Nuevo schema (con todas las columnas)\n",
    "new_transactions = transactions_df[[\n",
    "    'transaction_id', 'transaction_date', 'customer_id', \n",
    "    'product_id', 'quantity', 'unit_price', 'total_amount', \n",
    "    'payment_method', 'status'\n",
    "]].tail(100).copy()\n",
    "\n",
    "new_transactions.to_parquet(legacy_path / 'new_transactions.parquet', index=False)\n",
    "\n",
    "print(\"‚úÖ Archivos con diferentes schemas creados\")\n",
    "\n",
    "# Union con schema matching autom√°tico\n",
    "result = con.execute(\"\"\"\n",
    "    SELECT \n",
    "        transaction_id,\n",
    "        transaction_date,\n",
    "        customer_id,\n",
    "        product_id,\n",
    "        total_amount,\n",
    "        COALESCE(payment_method, 'Unknown') as payment_method,\n",
    "        COALESCE(status, 'Unknown') as status,\n",
    "        'legacy' as source\n",
    "    FROM 'data_parquet/legacy/old_transactions.parquet'\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    SELECT \n",
    "        transaction_id,\n",
    "        transaction_date,\n",
    "        customer_id,\n",
    "        product_id,\n",
    "        total_amount,\n",
    "        payment_method,\n",
    "        status,\n",
    "        'new' as source\n",
    "    FROM 'data_parquet/legacy/new_transactions.parquet'\n",
    "    \n",
    "    ORDER BY transaction_date\n",
    "\"\"\").df()\n",
    "\n",
    "print(\"\\nüìä Union de schemas diferentes:\")\n",
    "print(result.head(10))\n",
    "print(f\"\\nüìä Total registros: {len(result)}\")\n",
    "print(f\"üìä Fuentes: {result['source'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Metadata y Schema Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener schema de un archivo Parquet\n",
    "schema_info = con.execute(\"\"\"\n",
    "    DESCRIBE SELECT * FROM 'data_parquet/products.parquet'\n",
    "\"\"\").df()\n",
    "\n",
    "print(\"üìä Schema de products.parquet:\")\n",
    "print(schema_info)\n",
    "\n",
    "# Metadata de archivo Parquet usando PyArrow\n",
    "parquet_file = pq.ParquetFile('data_parquet/products.parquet')\n",
    "print(\"\\nüìä Metadata del archivo:\")\n",
    "print(f\"   N√∫mero de row groups: {parquet_file.num_row_groups}\")\n",
    "print(f\"   N√∫mero de filas: {parquet_file.metadata.num_rows}\")\n",
    "print(f\"   N√∫mero de columnas: {parquet_file.metadata.num_columns}\")\n",
    "print(f\"   Tama√±o serializado: {parquet_file.metadata.serialized_size} bytes\")\n",
    "\n",
    "# Estad√≠sticas de columnas\n",
    "print(\"\\nüìä Estad√≠sticas de columnas:\")\n",
    "for i in range(parquet_file.metadata.num_columns):\n",
    "    col = parquet_file.metadata.row_group(0).column(i)\n",
    "    print(f\"   {col.path_in_schema}: {col.statistics}\")\n",
    "\n",
    "# Listar todos los archivos y tama√±os\n",
    "result = con.execute(\"\"\"\n",
    "    SELECT \n",
    "        filename,\n",
    "        file_size,\n",
    "        file_modified_time\n",
    "    FROM read_parquet(\n",
    "        'data_parquet/**/*.parquet',\n",
    "        filename=true,\n",
    "        file_row_number=false,\n",
    "        hive_partitioning=false\n",
    "    )\n",
    "    GROUP BY filename, file_size, file_modified_time\n",
    "    ORDER BY file_size DESC\n",
    "\"\"\").df()\n",
    "\n",
    "print(\"\\nüìÅ Todos los archivos Parquet:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Best Practices y Optimizaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üí° MEJORES PR√ÅCTICAS PARA PARQUET + DUCKDB\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n1Ô∏è‚É£ PARTICIONAMIENTO:\")\n",
    "print(\"   ‚úÖ Particiona por columnas frecuentemente filtradas (fecha, regi√≥n, etc.)\")\n",
    "print(\"   ‚úÖ Evita demasiadas particiones peque√±as (<100MB cada una)\")\n",
    "print(\"   ‚úÖ Usa Hive-style partitioning (year=2024/month=01/)\")\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£ COMPRESI√ìN:\")\n",
    "print(\"   ‚úÖ Usa SNAPPY para balance velocidad/compresi√≥n\")\n",
    "print(\"   ‚úÖ Usa GZIP para m√°xima compresi√≥n (m√°s lento)\")\n",
    "print(\"   ‚úÖ Usa ZSTD para mejor compresi√≥n moderna\")\n",
    "\n",
    "print(\"\\n3Ô∏è‚É£ SCHEMA:\")\n",
    "print(\"   ‚úÖ Usa tipos de datos apropiados (INT32 vs INT64)\")\n",
    "print(\"   ‚úÖ Considera diccionarios para strings repetitivos\")\n",
    "print(\"   ‚úÖ Mant√©n schemas consistentes entre archivos\")\n",
    "\n",
    "print(\"\\n4Ô∏è‚É£ QUERIES:\")\n",
    "print(\"   ‚úÖ Usa projection pushdown (SELECT solo columnas necesarias)\")\n",
    "print(\"   ‚úÖ Usa predicate pushdown (WHERE en particiones)\")\n",
    "print(\"   ‚úÖ Evita SELECT * en datasets grandes\")\n",
    "\n",
    "print(\"\\n5Ô∏è‚É£ ESCRITURA:\")\n",
    "print(\"   ‚úÖ Escribe batches grandes (no muchos archivos peque√±os)\")\n",
    "print(\"   ‚úÖ Usa row groups de ~128MB\")\n",
    "print(\"   ‚úÖ Considera ordenar datos antes de escribir\")\n",
    "\n",
    "# Demostrar projection pushdown\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üìä DEMO: Projection Pushdown\\n\")\n",
    "\n",
    "# Sin projection (lee todo)\n",
    "start = time.time()\n",
    "result = con.execute(\"\"\"\n",
    "    SELECT transaction_id, total_amount\n",
    "    FROM (\n",
    "        SELECT *\n",
    "        FROM 'data_parquet/transactions/**/*.parquet'\n",
    "    )\n",
    "    LIMIT 1000\n",
    "\"\"\").df()\n",
    "time_without = time.time() - start\n",
    "\n",
    "# Con projection (solo lee columnas necesarias)\n",
    "start = time.time()\n",
    "result = con.execute(\"\"\"\n",
    "    SELECT transaction_id, total_amount\n",
    "    FROM 'data_parquet/transactions/**/*.parquet'\n",
    "    LIMIT 1000\n",
    "\"\"\").df()\n",
    "time_with = time.time() - start\n",
    "\n",
    "print(f\"Sin projection pushdown: {time_without*1000:.2f}ms\")\n",
    "print(f\"Con projection pushdown: {time_with*1000:.2f}ms\")\n",
    "print(f\"Mejora: {time_without/time_with:.2f}x m√°s r√°pido\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Resumen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estad√≠sticas finales\n",
    "print(\"üéâ RESUMEN DEL TUTORIAL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Contar archivos\n",
    "parquet_files = list(base_path.rglob('*.parquet'))\n",
    "total_size = sum(f.stat().st_size for f in parquet_files)\n",
    "\n",
    "print(f\"\\nüìÅ Archivos creados: {len(parquet_files)}\")\n",
    "print(f\"üíæ Tama√±o total: {total_size / (1024*1024):.2f} MB\")\n",
    "\n",
    "# Estad√≠sticas de datos\n",
    "stats = con.execute(\"\"\"\n",
    "    SELECT \n",
    "        (SELECT COUNT(*) FROM 'data_parquet/products.parquet') as productos,\n",
    "        (SELECT COUNT(*) FROM 'data_parquet/customers.parquet') as clientes,\n",
    "        (SELECT COUNT(*) FROM 'data_parquet/transactions/**/*.parquet') as transacciones,\n",
    "        (SELECT SUM(total_amount) FROM 'data_parquet/transactions/**/*.parquet' WHERE status = 'completed') as revenue_total\n",
    "\"\"\").df()\n",
    "\n",
    "print(f\"\\nüìä Datos procesados:\")\n",
    "print(f\"   Productos: {stats['productos'][0]:,}\")\n",
    "print(f\"   Clientes: {stats['clientes'][0]:,}\")\n",
    "print(f\"   Transacciones: {stats['transacciones'][0]:,}\")\n",
    "print(f\"   Revenue total: ${stats['revenue_total'][0]:,.2f}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Conceptos aprendidos:\")\n",
    "print(f\"   - Particionamiento de datos\")\n",
    "print(f\"   - Lectura de m√∫ltiples archivos Parquet\")\n",
    "print(f\"   - Queries distribuidos con DuckDB\")\n",
    "print(f\"   - Joins entre archivos\")\n",
    "print(f\"   - Window functions\")\n",
    "print(f\"   - Optimizaciones de performance\")\n",
    "print(f\"   - Schema evolution\")\n",
    "\n",
    "# Cerrar conexi√≥n\n",
    "con.close()\n",
    "print(\"\\n‚úÖ Conexi√≥n DuckDB cerrada\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
