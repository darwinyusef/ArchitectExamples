{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark + Clustering + PostgreSQL + MLflow\n",
    "\n",
    "## Objetivos\n",
    "- Algoritmos de clustering con Spark ML\n",
    "- Almacenamiento de resultados en PostgreSQL\n",
    "- Evaluaci√≥n de modelos de clustering\n",
    "- Tracking con MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.clustering import KMeans, GaussianMixture, BisectingKMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraci√≥n\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('Clustering-Postgres-MLflow') \\\n",
    "    .master('local[*]') \\\n",
    "    .config('spark.jars', '/path/to/postgresql-jdbc.jar') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "mlflow.set_tracking_uri('http://localhost:5000')\n",
    "mlflow.set_experiment('spark-clustering-postgres')\n",
    "\n",
    "# PostgreSQL connection\n",
    "POSTGRES_URL = 'postgresql://spark_user:spark_password@localhost:5432/spark_ml_db'\n",
    "engine = create_engine(POSTGRES_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generar Datos Sint√©ticos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Generar clusters\n",
    "n_samples = 10000\n",
    "n_features = 10\n",
    "n_clusters = 5\n",
    "\n",
    "X, y_true = make_blobs(\n",
    "    n_samples=n_samples,\n",
    "    n_features=n_features,\n",
    "    centers=n_clusters,\n",
    "    cluster_std=1.5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Crear DataFrame\n",
    "feature_cols = [f'feature_{i}' for i in range(n_features)]\n",
    "df_pandas = pd.DataFrame(X, columns=feature_cols)\n",
    "df_pandas['customer_id'] = range(len(df_pandas))\n",
    "\n",
    "df_spark = spark.createDataFrame(df_pandas)\n",
    "\n",
    "print(f'Dataset: {df_spark.count()} clientes, {n_features} features')\n",
    "df_spark.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preparar Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensamblar features\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol='features_raw')\n",
    "df_assembled = assembler.transform(df_spark)\n",
    "\n",
    "# Escalar\n",
    "scaler = StandardScaler(inputCol='features_raw', outputCol='features', withMean=True, withStd=True)\n",
    "scaler_model = scaler.fit(df_assembled)\n",
    "df_scaled = scaler_model.transform(df_assembled)\n",
    "\n",
    "print('‚úì Features preparadas y escaladas')\n",
    "df_scaled.select('customer_id', 'features').show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar m√∫ltiples modelos con diferentes K\n",
    "k_values = [3, 4, 5, 6, 7, 8]\n",
    "results = []\n",
    "\n",
    "evaluator = ClusteringEvaluator(featuresCol='features', metricName='silhouette')\n",
    "\n",
    "for k in k_values:\n",
    "    with mlflow.start_run(run_name=f'kmeans-k{k}'):\n",
    "        # Log params\n",
    "        mlflow.log_params({'algorithm': 'KMeans', 'k': k, 'max_iter': 20})\n",
    "        \n",
    "        # Train\n",
    "        kmeans = KMeans(featuresCol='features', predictionCol='cluster', k=k, maxIter=20, seed=42)\n",
    "        model = kmeans.fit(df_scaled)\n",
    "        \n",
    "        # Predict\n",
    "        predictions = model.transform(df_scaled)\n",
    "        \n",
    "        # Evaluate\n",
    "        silhouette = evaluator.evaluate(predictions)\n",
    "        \n",
    "        # Cost (Within Set Sum of Squared Errors)\n",
    "        wssse = model.summary.trainingCost\n",
    "        \n",
    "        # Cluster sizes\n",
    "        cluster_counts = predictions.groupBy('cluster').count().orderBy('cluster').collect()\n",
    "        cluster_sizes = {row['cluster']: row['count'] for row in cluster_counts}\n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metrics({\n",
    "            'silhouette_score': silhouette,\n",
    "            'wssse': wssse,\n",
    "            'n_iterations': model.summary.numIter\n",
    "        })\n",
    "        \n",
    "        # Log cluster sizes\n",
    "        for cluster_id, size in cluster_sizes.items():\n",
    "            mlflow.log_metric(f'cluster_{cluster_id}_size', size)\n",
    "        \n",
    "        # Save model\n",
    "        mlflow.spark.log_model(model, f'kmeans-model-k{k}')\n",
    "        \n",
    "        results.append({\n",
    "            'k': k,\n",
    "            'silhouette': silhouette,\n",
    "            'wssse': wssse,\n",
    "            'model': model\n",
    "        })\n",
    "        \n",
    "        print(f'K={k}: Silhouette={silhouette:.4f}, WSSSE={wssse:.2f}')\n",
    "\n",
    "# Encontrar mejor K\n",
    "best_result = max(results, key=lambda x: x['silhouette'])\n",
    "print(f'\\n‚úì Mejor K: {best_result[\"k\"]} (Silhouette: {best_result[\"silhouette\"]:.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualizaci√≥n: Elbow Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear gr√°ficos\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Elbow plot (WSSSE)\n",
    "k_vals = [r['k'] for r in results]\n",
    "wssse_vals = [r['wssse'] for r in results]\n",
    "silhouette_vals = [r['silhouette'] for r in results]\n",
    "\n",
    "axes[0].plot(k_vals, wssse_vals, marker='o', linewidth=2, markersize=8, color='steelblue')\n",
    "axes[0].set_xlabel('N√∫mero de Clusters (K)', fontsize=12)\n",
    "axes[0].set_ylabel('WSSSE', fontsize=12)\n",
    "axes[0].set_title('Elbow Method - WSSSE', fontweight='bold', fontsize=14)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Silhouette plot\n",
    "axes[1].plot(k_vals, silhouette_vals, marker='s', linewidth=2, markersize=8, color='coral')\n",
    "axes[1].axhline(y=max(silhouette_vals), color='green', linestyle='--', alpha=0.5, label='M√°ximo')\n",
    "axes[1].set_xlabel('N√∫mero de Clusters (K)', fontsize=12)\n",
    "axes[1].set_ylabel('Silhouette Score', fontsize=12)\n",
    "axes[1].set_title('Silhouette Score vs K', fontweight='bold', fontsize=14)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('clustering_evaluation.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Gaussian Mixture Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GMM clustering\n",
    "with mlflow.start_run(run_name='gaussian-mixture-model'):\n",
    "    k = best_result['k']\n",
    "    \n",
    "    mlflow.log_params({'algorithm': 'GMM', 'k': k, 'max_iter': 100})\n",
    "    \n",
    "    # Train\n",
    "    gmm = GaussianMixture(featuresCol='features', predictionCol='cluster', k=k, maxIter=100, seed=42)\n",
    "    gmm_model = gmm.fit(df_scaled)\n",
    "    \n",
    "    # Predict\n",
    "    gmm_predictions = gmm_model.transform(df_scaled)\n",
    "    \n",
    "    # Evaluate\n",
    "    gmm_silhouette = evaluator.evaluate(gmm_predictions)\n",
    "    \n",
    "    # Log likelihood\n",
    "    log_likelihood = gmm_model.summary.logLikelihood\n",
    "    \n",
    "    mlflow.log_metrics({\n",
    "        'silhouette_score': gmm_silhouette,\n",
    "        'log_likelihood': log_likelihood\n",
    "    })\n",
    "    \n",
    "    mlflow.spark.log_model(gmm_model, 'gmm-model')\n",
    "    \n",
    "    print(f'GMM - Silhouette: {gmm_silhouette:.4f}')\n",
    "    print(f'GMM - Log Likelihood: {log_likelihood:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Guardar Resultados en PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usar mejor modelo\n",
    "best_model = best_result['model']\n",
    "final_predictions = best_model.transform(df_scaled)\n",
    "\n",
    "# Seleccionar columnas relevantes\n",
    "df_to_save = final_predictions.select('customer_id', 'cluster', *feature_cols)\n",
    "\n",
    "# Convertir a Pandas\n",
    "df_results = df_to_save.toPandas()\n",
    "\n",
    "# Guardar en PostgreSQL\n",
    "try:\n",
    "    df_results.to_sql(\n",
    "        'customer_clusters',\n",
    "        engine,\n",
    "        if_exists='replace',\n",
    "        index=False,\n",
    "        method='multi',\n",
    "        chunksize=1000\n",
    "    )\n",
    "    print(f'‚úì {len(df_results)} registros guardados en PostgreSQL (tabla: customer_clusters)')\n",
    "except Exception as e:\n",
    "    print(f'‚ö† Error al guardar en PostgreSQL: {e}')\n",
    "    print('Aseg√∫rate de que PostgreSQL est√© corriendo y las credenciales sean correctas')\n",
    "\n",
    "# Tambi√©n guardar estad√≠sticas de clusters\n",
    "cluster_stats = df_results.groupby('cluster').agg({\n",
    "    'customer_id': 'count',\n",
    "    **{col: ['mean', 'std'] for col in feature_cols}\n",
    "}).reset_index()\n",
    "\n",
    "cluster_stats.columns = ['_'.join(col).strip('_') for col in cluster_stats.columns.values]\n",
    "\n",
    "try:\n",
    "    cluster_stats.to_sql(\n",
    "        'cluster_statistics',\n",
    "        engine,\n",
    "        if_exists='replace',\n",
    "        index=False\n",
    "    )\n",
    "    print(f'‚úì Estad√≠sticas de {len(cluster_stats)} clusters guardadas')\n",
    "except Exception as e:\n",
    "    print(f'‚ö† Error: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Leer desde PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer datos con Spark JDBC\n",
    "try:\n",
    "    df_from_postgres = spark.read \\\n",
    "        .format('jdbc') \\\n",
    "        .option('url', 'jdbc:postgresql://localhost:5432/spark_ml_db') \\\n",
    "        .option('dbtable', 'customer_clusters') \\\n",
    "        .option('user', 'spark_user') \\\n",
    "        .option('password', 'spark_password') \\\n",
    "        .option('driver', 'org.postgresql.Driver') \\\n",
    "        .load()\n",
    "    \n",
    "    print('‚úì Datos le√≠dos desde PostgreSQL:')\n",
    "    df_from_postgres.show(10)\n",
    "    \n",
    "    # Distribuci√≥n de clusters\n",
    "    print('\\nüìä Distribuci√≥n de Clusters:')\n",
    "    df_from_postgres.groupBy('cluster').count().orderBy('cluster').show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f'‚ö† Error al leer desde PostgreSQL: {e}')\n",
    "    print('Necesitas descargar postgresql-jdbc.jar y configurar la ruta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. An√°lisis de Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analizar caracter√≠sticas de cada cluster\n",
    "print('\\nüìä AN√ÅLISIS DE CLUSTERS\\n' + '='*60)\n",
    "\n",
    "for cluster_id in range(best_result['k']):\n",
    "    cluster_data = df_results[df_results['cluster'] == cluster_id]\n",
    "    \n",
    "    print(f'\\nCluster {cluster_id}:')\n",
    "    print(f'  Tama√±o: {len(cluster_data)} clientes ({len(cluster_data)/len(df_results)*100:.1f}%)')\n",
    "    \n",
    "    # Top 3 features m√°s distintivas\n",
    "    feature_means = cluster_data[feature_cols].mean()\n",
    "    global_means = df_results[feature_cols].mean()\n",
    "    deviations = (feature_means - global_means).abs().sort_values(ascending=False)\n",
    "    \n",
    "    print('  Top 3 features distintivas:')\n",
    "    for i, (feat, dev) in enumerate(deviations.head(3).items(), 1):\n",
    "        print(f'    {i}. {feat}: {feature_means[feat]:.2f} (global: {global_means[feat]:.2f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualizaci√≥n PCA de Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Reducir a 2D con PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(df_results[feature_cols])\n",
    "\n",
    "# Visualizar\n",
    "plt.figure(figsize=(12, 8))\n",
    "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=df_results['cluster'], \n",
    "                     cmap='viridis', alpha=0.6, s=30)\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)', fontsize=12)\n",
    "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)', fontsize=12)\n",
    "plt.title('Visualizaci√≥n de Clusters (PCA)', fontweight='bold', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('clusters_pca.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f'‚úì Varianza explicada: {sum(pca.explained_variance_ratio_)*100:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Queries SQL de Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Queries √∫tiles para an√°lisis\n",
    "queries = {\n",
    "    'Clientes por cluster': \"\"\"\n",
    "        SELECT cluster, COUNT(*) as num_customers\n",
    "        FROM customer_clusters\n",
    "        GROUP BY cluster\n",
    "        ORDER BY cluster;\n",
    "    \"\"\",\n",
    "    \n",
    "    'Clientes del cluster m√°s grande': \"\"\"\n",
    "        SELECT cluster, COUNT(*) as num_customers\n",
    "        FROM customer_clusters\n",
    "        GROUP BY cluster\n",
    "        ORDER BY num_customers DESC\n",
    "        LIMIT 1;\n",
    "    \"\"\",\n",
    "    \n",
    "    'Muestra de clientes de cada cluster': \"\"\"\n",
    "        SELECT *\n",
    "        FROM (\n",
    "            SELECT *, ROW_NUMBER() OVER (PARTITION BY cluster ORDER BY customer_id) as rn\n",
    "            FROM customer_clusters\n",
    "        ) t\n",
    "        WHERE rn <= 5\n",
    "        ORDER BY cluster, customer_id;\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "print('\\nüìù Queries SQL de Ejemplo:\\n')\n",
    "for name, query in queries.items():\n",
    "    print(f'{name}:')\n",
    "    print(query)\n",
    "    print()\n",
    "\n",
    "# Ejecutar query de ejemplo\n",
    "try:\n",
    "    result = pd.read_sql(queries['Clientes por cluster'], engine)\n",
    "    print('\\nResultado:')\n",
    "    print(result)\n",
    "except Exception as e:\n",
    "    print(f'‚ö† Error: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "### Aprendizajes Clave\n",
    "\n",
    "**Clustering con Spark:**\n",
    "- ‚úÖ K-Means escalable para grandes datasets\n",
    "- ‚úÖ GMM para clusters probabil√≠sticos\n",
    "- ‚úÖ Evaluaci√≥n con Silhouette Score\n",
    "- ‚úÖ Elbow method para selecci√≥n de K\n",
    "\n",
    "**Integraci√≥n PostgreSQL:**\n",
    "- ‚úÖ Persistencia de resultados\n",
    "- ‚úÖ Queries SQL para an√°lisis\n",
    "- ‚úÖ JDBC para lectura/escritura\n",
    "- ‚úÖ Escalabilidad con particionamiento\n",
    "\n",
    "**MLflow Tracking:**\n",
    "- ‚úÖ Comparaci√≥n de modelos\n",
    "- ‚úÖ Versionado de experimentos\n",
    "- ‚úÖ Reproducibilidad\n",
    "\n",
    "### Casos de Uso\n",
    "- Segmentaci√≥n de clientes\n",
    "- Detecci√≥n de anomal√≠as\n",
    "- An√°lisis de comportamiento\n",
    "- Recomendaciones personalizadas\n",
    "\n",
    "### Ejercicios\n",
    "1. Implementar DBSCAN distribuido\n",
    "2. Clustering jer√°rquico con BisectingKMeans\n",
    "3. Feature engineering para mejorar clusters\n",
    "4. Crear dashboard en Grafana con datos de PostgreSQL\n",
    "5. Implementar clustering incremental para streaming data\n",
    "\n",
    "### Pr√≥ximos Pasos\n",
    "1. Implementar pipeline de actualizaci√≥n autom√°tica\n",
    "2. A√±adir monitoreo de drift de clusters\n",
    "3. Crear API para asignaci√≥n de clusters en tiempo real\n",
    "4. Integrar con sistemas de CRM\n",
    "5. Implementar A/B testing de diferentes estrategias de clustering"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
