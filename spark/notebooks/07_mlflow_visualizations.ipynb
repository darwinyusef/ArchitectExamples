{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLflow + Visualizaciones: Matplotlib y Seaborn\n",
    "\n",
    "## Objetivos\n",
    "- Integrar visualizaciones con MLflow\n",
    "- Guardar gráficos como artifacts\n",
    "- Crear dashboards interactivos\n",
    "- Best practices para visualización en ML\n",
    "\n",
    "## Tipos de Visualizaciones\n",
    "1. **Métricas de entrenamiento** - Loss curves, accuracy\n",
    "2. **Evaluación de modelos** - Confusion matrix, ROC curves\n",
    "3. **Feature importance** - Importancia de variables\n",
    "4. **Datos** - Distribuciones, correlaciones\n",
    "5. **Predicciones** - Actual vs Predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup e Importaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Core\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification, make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, classification_report, roc_curve, auc,\n",
    "    precision_recall_curve, accuracy_score, f1_score\n",
    ")\n",
    "\n",
    "# Visualización\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# MLflow\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# Configurar estilo\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print('✓ Importaciones completadas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configurar MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar MLflow\n",
    "mlflow.set_tracking_uri('http://localhost:5000')\n",
    "mlflow.set_experiment('visualization-examples')\n",
    "\n",
    "# Crear cliente\n",
    "client = MlflowClient()\n",
    "\n",
    "print(f'✓ MLflow URI: {mlflow.get_tracking_uri()}')\n",
    "print(f'✓ Experiment: visualization-examples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generar Datos de Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset de clasificación\n",
    "X, y = make_classification(\n",
    "    n_samples=2000,\n",
    "    n_features=20,\n",
    "    n_informative=15,\n",
    "    n_redundant=5,\n",
    "    n_classes=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Crear DataFrame para visualizaciones\n",
    "feature_names = [f'feature_{i}' for i in range(X.shape[1])]\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "df['target'] = y\n",
    "\n",
    "print(f'✓ Dataset: {X.shape[0]} muestras, {X.shape[1]} features')\n",
    "print(f'✓ Clases: {np.unique(y, return_counts=True)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualización 1: Distribución de Datos con Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data_distribution(df, save_path='data_distribution.png'):\n",
    "    \"\"\"Visualizar distribución de features\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    fig.suptitle('Distribución de Features', fontsize=16, fontweight='bold', y=1.02)\n",
    "    \n",
    "    # Seleccionar 6 features para visualizar\n",
    "    features_to_plot = feature_names[:6]\n",
    "    \n",
    "    for idx, (ax, feature) in enumerate(zip(axes.flat, features_to_plot)):\n",
    "        # Histograma por clase\n",
    "        for class_label in [0, 1]:\n",
    "            data_class = df[df['target'] == class_label][feature]\n",
    "            ax.hist(data_class, bins=30, alpha=0.6, label=f'Class {class_label}')\n",
    "        \n",
    "        ax.set_xlabel(feature, fontsize=10)\n",
    "        ax.set_ylabel('Frecuencia', fontsize=10)\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return save_path\n",
    "\n",
    "# Crear y guardar gráfico\n",
    "with mlflow.start_run(run_name='data-exploration'):\n",
    "    # Crear visualización\n",
    "    plot_path = plot_data_distribution(df)\n",
    "    \n",
    "    # Guardar en MLflow\n",
    "    mlflow.log_artifact(plot_path, artifact_path='plots')\n",
    "    \n",
    "    # Log estadísticas\n",
    "    mlflow.log_params({\n",
    "        'n_samples': len(df),\n",
    "        'n_features': len(feature_names),\n",
    "        'n_classes': len(np.unique(y))\n",
    "    })\n",
    "    \n",
    "    print('✓ Gráfico guardado en MLflow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualización 2: Matriz de Correlación con Seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_correlation_matrix(df, save_path='correlation_matrix.png'):\n",
    "    \"\"\"Visualizar matriz de correlación con seaborn\"\"\"\n",
    "    \n",
    "    # Calcular correlación (solo features numéricas)\n",
    "    corr_matrix = df[feature_names[:10]].corr()  # Primeras 10 features\n",
    "    \n",
    "    # Crear figura\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Heatmap con seaborn\n",
    "    sns.heatmap(\n",
    "        corr_matrix,\n",
    "        annot=True,\n",
    "        fmt='.2f',\n",
    "        cmap='coolwarm',\n",
    "        center=0,\n",
    "        square=True,\n",
    "        linewidths=1,\n",
    "        cbar_kws={'shrink': 0.8}\n",
    "    )\n",
    "    \n",
    "    plt.title('Matriz de Correlación de Features', fontsize=14, fontweight='bold', pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return save_path\n",
    "\n",
    "# Crear y guardar\n",
    "with mlflow.start_run(run_name='correlation-analysis'):\n",
    "    plot_path = plot_correlation_matrix(df)\n",
    "    mlflow.log_artifact(plot_path, artifact_path='plots')\n",
    "    \n",
    "    # Log correlaciones más altas\n",
    "    corr_matrix = df[feature_names[:10]].corr()\n",
    "    upper_triangle = np.triu(corr_matrix, k=1)\n",
    "    max_corr = np.max(np.abs(upper_triangle))\n",
    "    mlflow.log_metric('max_correlation', max_corr)\n",
    "    \n",
    "    print(f'✓ Correlación máxima: {max_corr:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Entrenar Modelo para Visualizaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar Random Forest\n",
    "with mlflow.start_run(run_name='model-training') as run:\n",
    "    # Modelo\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Entrenar\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predicciones\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Métricas\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    mlflow.log_params({\n",
    "        'n_estimators': 100,\n",
    "        'max_depth': 10\n",
    "    })\n",
    "    \n",
    "    mlflow.log_metrics({\n",
    "        'accuracy': accuracy,\n",
    "        'f1_score': f1\n",
    "    })\n",
    "    \n",
    "    mlflow.sklearn.log_model(model, 'random-forest')\n",
    "    \n",
    "    run_id = run.info.run_id\n",
    "    \n",
    "    print(f'✓ Modelo entrenado')\n",
    "    print(f'  Accuracy: {accuracy:.4f}')\n",
    "    print(f'  F1-Score: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualización 3: Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, save_path='confusion_matrix.png'):\n",
    "    \"\"\"Matriz de confusión mejorada con seaborn\"\"\"\n",
    "    \n",
    "    # Calcular matriz\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Normalizar\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    # Crear figura con dos subplots\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Matriz absoluta\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        annot=True,\n",
    "        fmt='d',\n",
    "        cmap='Blues',\n",
    "        ax=axes[0],\n",
    "        cbar_kws={'label': 'Count'}\n",
    "    )\n",
    "    axes[0].set_title('Confusion Matrix (Counts)', fontweight='bold')\n",
    "    axes[0].set_xlabel('Predicted')\n",
    "    axes[0].set_ylabel('Actual')\n",
    "    \n",
    "    # Matriz normalizada\n",
    "    sns.heatmap(\n",
    "        cm_normalized,\n",
    "        annot=True,\n",
    "        fmt='.2%',\n",
    "        cmap='Greens',\n",
    "        ax=axes[1],\n",
    "        cbar_kws={'label': 'Percentage'}\n",
    "    )\n",
    "    axes[1].set_title('Confusion Matrix (Normalized)', fontweight='bold')\n",
    "    axes[1].set_xlabel('Predicted')\n",
    "    axes[1].set_ylabel('Actual')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return save_path\n",
    "\n",
    "# Crear y guardar\n",
    "with mlflow.start_run(run_id=run_id):\n",
    "    plot_path = plot_confusion_matrix(y_test, y_pred)\n",
    "    mlflow.log_artifact(plot_path, artifact_path='evaluation')\n",
    "    \n",
    "    print('✓ Confusion matrix guardada')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualización 4: ROC Curve y Precision-Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_and_pr_curves(y_true, y_pred_proba, save_path='roc_pr_curves.png'):\n",
    "    \"\"\"Curvas ROC y Precision-Recall\"\"\"\n",
    "    \n",
    "    # Calcular curvas\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_pred_proba)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    \n",
    "    # Crear figura\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # ROC Curve\n",
    "    axes[0].plot(fpr, tpr, color='darkorange', lw=2,\n",
    "                label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "    axes[0].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')\n",
    "    axes[0].set_xlim([0.0, 1.0])\n",
    "    axes[0].set_ylim([0.0, 1.05])\n",
    "    axes[0].set_xlabel('False Positive Rate', fontsize=12)\n",
    "    axes[0].set_ylabel('True Positive Rate', fontsize=12)\n",
    "    axes[0].set_title('ROC Curve', fontweight='bold', fontsize=14)\n",
    "    axes[0].legend(loc='lower right')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Precision-Recall Curve\n",
    "    axes[1].plot(recall, precision, color='green', lw=2,\n",
    "                label=f'PR curve (AUC = {pr_auc:.3f})')\n",
    "    axes[1].set_xlim([0.0, 1.0])\n",
    "    axes[1].set_ylim([0.0, 1.05])\n",
    "    axes[1].set_xlabel('Recall', fontsize=12)\n",
    "    axes[1].set_ylabel('Precision', fontsize=12)\n",
    "    axes[1].set_title('Precision-Recall Curve', fontweight='bold', fontsize=14)\n",
    "    axes[1].legend(loc='lower left')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return save_path, roc_auc, pr_auc\n",
    "\n",
    "# Crear y guardar\n",
    "with mlflow.start_run(run_id=run_id):\n",
    "    plot_path, roc_auc, pr_auc = plot_roc_and_pr_curves(y_test, y_pred_proba)\n",
    "    \n",
    "    mlflow.log_artifact(plot_path, artifact_path='evaluation')\n",
    "    mlflow.log_metrics({\n",
    "        'roc_auc': roc_auc,\n",
    "        'pr_auc': pr_auc\n",
    "    })\n",
    "    \n",
    "    print(f'✓ ROC AUC: {roc_auc:.4f}')\n",
    "    print(f'✓ PR AUC: {pr_auc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualización 5: Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(model, feature_names, top_n=15, save_path='feature_importance.png'):\n",
    "    \"\"\"Visualizar importancia de features\"\"\"\n",
    "    \n",
    "    # Obtener importancias\n",
    "    importances = model.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1][:top_n]\n",
    "    \n",
    "    # Crear DataFrame\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': [feature_names[i] for i in indices],\n",
    "        'importance': importances[indices]\n",
    "    })\n",
    "    \n",
    "    # Crear figura\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Barplot horizontal\n",
    "    sns.barplot(\n",
    "        data=importance_df,\n",
    "        y='feature',\n",
    "        x='importance',\n",
    "        palette='viridis',\n",
    "        ax=axes[0]\n",
    "    )\n",
    "    axes[0].set_title(f'Top {top_n} Feature Importances', fontweight='bold', fontsize=14)\n",
    "    axes[0].set_xlabel('Importance', fontsize=12)\n",
    "    axes[0].set_ylabel('Feature', fontsize=12)\n",
    "    axes[0].grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # Cumulative importance\n",
    "    cumulative_importance = np.cumsum(importances[indices])\n",
    "    axes[1].plot(range(1, top_n + 1), cumulative_importance, \n",
    "                marker='o', linewidth=2, markersize=8, color='darkblue')\n",
    "    axes[1].axhline(y=0.95, color='red', linestyle='--', label='95% threshold')\n",
    "    axes[1].set_title('Cumulative Feature Importance', fontweight='bold', fontsize=14)\n",
    "    axes[1].set_xlabel('Number of Features', fontsize=12)\n",
    "    axes[1].set_ylabel('Cumulative Importance', fontsize=12)\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return save_path, importance_df\n",
    "\n",
    "# Crear y guardar\n",
    "with mlflow.start_run(run_id=run_id):\n",
    "    plot_path, importance_df = plot_feature_importance(model, feature_names)\n",
    "    \n",
    "    mlflow.log_artifact(plot_path, artifact_path='analysis')\n",
    "    \n",
    "    # Guardar CSV de importancias\n",
    "    csv_path = 'feature_importance.csv'\n",
    "    importance_df.to_csv(csv_path, index=False)\n",
    "    mlflow.log_artifact(csv_path, artifact_path='analysis')\n",
    "    \n",
    "    # Log top feature\n",
    "    mlflow.log_param('top_feature', importance_df.iloc[0]['feature'])\n",
    "    mlflow.log_metric('top_feature_importance', importance_df.iloc[0]['importance'])\n",
    "    \n",
    "    print('✓ Feature importance guardada')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualización 6: Comparación de Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models_visualization(models_results, save_path='models_comparison.png'):\n",
    "    \"\"\"Comparar múltiples modelos visualmente\"\"\"\n",
    "    \n",
    "    # Crear DataFrame\n",
    "    df_results = pd.DataFrame(models_results)\n",
    "    \n",
    "    # Crear figura\n",
    "    fig = plt.figure(figsize=(16, 10))\n",
    "    gs = gridspec.GridSpec(2, 2, figure=fig)\n",
    "    \n",
    "    # 1. Accuracy comparison\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    sns.barplot(data=df_results, x='model', y='accuracy', palette='Set2', ax=ax1)\n",
    "    ax1.set_title('Accuracy por Modelo', fontweight='bold', fontsize=12)\n",
    "    ax1.set_ylim([df_results['accuracy'].min() - 0.05, 1.0])\n",
    "    ax1.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # 2. F1-Score comparison\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    sns.barplot(data=df_results, x='model', y='f1_score', palette='Set3', ax=ax2)\n",
    "    ax2.set_title('F1-Score por Modelo', fontweight='bold', fontsize=12)\n",
    "    ax2.set_ylim([df_results['f1_score'].min() - 0.05, 1.0])\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # 3. Training time\n",
    "    ax3 = fig.add_subplot(gs[1, 0])\n",
    "    sns.barplot(data=df_results, x='model', y='train_time', palette='coolwarm', ax=ax3)\n",
    "    ax3.set_title('Tiempo de Entrenamiento (s)', fontweight='bold', fontsize=12)\n",
    "    ax3.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # 4. Radar chart de métricas\n",
    "    ax4 = fig.add_subplot(gs[1, 1], projection='polar')\n",
    "    \n",
    "    metrics = ['accuracy', 'f1_score', 'precision', 'recall']\n",
    "    angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()\n",
    "    angles += angles[:1]\n",
    "    \n",
    "    for idx, row in df_results.iterrows():\n",
    "        values = [row[m] for m in metrics]\n",
    "        values += values[:1]\n",
    "        ax4.plot(angles, values, 'o-', linewidth=2, label=row['model'])\n",
    "        ax4.fill(angles, values, alpha=0.15)\n",
    "    \n",
    "    ax4.set_xticks(angles[:-1])\n",
    "    ax4.set_xticklabels(metrics)\n",
    "    ax4.set_ylim(0, 1)\n",
    "    ax4.set_title('Radar de Métricas', fontweight='bold', fontsize=12, pad=20)\n",
    "    ax4.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "    ax4.grid(True)\n",
    "    \n",
    "    plt.suptitle('Comparación de Modelos', fontsize=16, fontweight='bold', y=1.00)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return save_path\n",
    "\n",
    "# Entrenar múltiples modelos\n",
    "models = {\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'GradientBoosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    import time\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    from sklearn.metrics import precision_score, recall_score\n",
    "    \n",
    "    results.append({\n",
    "        'model': name,\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'f1_score': f1_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred),\n",
    "        'recall': recall_score(y_test, y_pred),\n",
    "        'train_time': train_time\n",
    "    })\n",
    "\n",
    "# Visualizar comparación\n",
    "with mlflow.start_run(run_name='models-comparison'):\n",
    "    plot_path = compare_models_visualization(results)\n",
    "    mlflow.log_artifact(plot_path, artifact_path='comparison')\n",
    "    \n",
    "    # Log resultados\n",
    "    for result in results:\n",
    "        for key, value in result.items():\n",
    "            if key != 'model':\n",
    "                mlflow.log_metric(f\"{result['model']}_{key}\", value)\n",
    "    \n",
    "    print('✓ Comparación de modelos guardada')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Visualización 7: Gráficos Interactivos con Plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interactive_plots(df, y_test, y_pred_proba):\n",
    "    \"\"\"Crear visualizaciones interactivas con Plotly\"\"\"\n",
    "    \n",
    "    # 1. Distribución 3D de features\n",
    "    fig1 = px.scatter_3d(\n",
    "        df,\n",
    "        x='feature_0',\n",
    "        y='feature_1',\n",
    "        z='feature_2',\n",
    "        color='target',\n",
    "        title='Distribución 3D de Features',\n",
    "        labels={'target': 'Class'},\n",
    "        color_continuous_scale='Viridis'\n",
    "    )\n",
    "    fig1.write_html('3d_distribution.html')\n",
    "    \n",
    "    # 2. ROC Curve interactiva\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    fig2 = go.Figure()\n",
    "    fig2.add_trace(go.Scatter(\n",
    "        x=fpr,\n",
    "        y=tpr,\n",
    "        mode='lines',\n",
    "        name=f'ROC (AUC={roc_auc:.3f})',\n",
    "        line=dict(color='darkorange', width=3),\n",
    "        hovertemplate='<b>FPR</b>: %{x:.3f}<br><b>TPR</b>: %{y:.3f}'\n",
    "    ))\n",
    "    fig2.add_trace(go.Scatter(\n",
    "        x=[0, 1],\n",
    "        y=[0, 1],\n",
    "        mode='lines',\n",
    "        name='Random',\n",
    "        line=dict(color='navy', width=2, dash='dash')\n",
    "    ))\n",
    "    fig2.update_layout(\n",
    "        title='Interactive ROC Curve',\n",
    "        xaxis_title='False Positive Rate',\n",
    "        yaxis_title='True Positive Rate',\n",
    "        hovermode='closest'\n",
    "    )\n",
    "    fig2.write_html('roc_curve_interactive.html')\n",
    "    \n",
    "    # 3. Dashboard de métricas\n",
    "    fig3 = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=('Distribution', 'Correlation', 'Feature Importance', 'Predictions'),\n",
    "        specs=[[{'type': 'histogram'}, {'type': 'heatmap'}],\n",
    "               [{'type': 'bar'}, {'type': 'scatter'}]]\n",
    "    )\n",
    "    \n",
    "    # Histogram\n",
    "    for class_label in [0, 1]:\n",
    "        fig3.add_trace(\n",
    "            go.Histogram(\n",
    "                x=df[df['target'] == class_label]['feature_0'],\n",
    "                name=f'Class {class_label}',\n",
    "                opacity=0.7\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "    \n",
    "    # Heatmap\n",
    "    corr = df[feature_names[:5]].corr()\n",
    "    fig3.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=corr.values,\n",
    "            x=corr.columns,\n",
    "            y=corr.columns,\n",
    "            colorscale='RdBu'\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    fig3.update_layout(height=800, showlegend=True, title_text='ML Dashboard')\n",
    "    fig3.write_html('ml_dashboard.html')\n",
    "    \n",
    "    return ['3d_distribution.html', 'roc_curve_interactive.html', 'ml_dashboard.html']\n",
    "\n",
    "# Crear gráficos interactivos\n",
    "with mlflow.start_run(run_name='interactive-plots'):\n",
    "    html_files = create_interactive_plots(df, y_test, y_pred_proba)\n",
    "    \n",
    "    for html_file in html_files:\n",
    "        mlflow.log_artifact(html_file, artifact_path='interactive')\n",
    "    \n",
    "    print('✓ Gráficos interactivos guardados en MLflow')\n",
    "    print('  Puedes visualizarlos en MLflow UI -> Artifacts -> interactive/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Best Practices: Template de Visualización Completa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_model_report(model, X_train, X_test, y_train, y_test, \n",
    "                               feature_names, experiment_name='complete-report'):\n",
    "    \"\"\"Reporte completo de modelo con todas las visualizaciones\"\"\"\n",
    "    \n",
    "    with mlflow.start_run(run_name=experiment_name) as run:\n",
    "        # 1. Entrenar modelo\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        model.fit(X_train, y_train)\n",
    "        train_time = time.time() - start_time\n",
    "        \n",
    "        # 2. Predicciones\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        # 3. Métricas\n",
    "        from sklearn.metrics import (\n",
    "            accuracy_score, f1_score, precision_score, recall_score,\n",
    "            roc_auc_score\n",
    "        )\n",
    "        \n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'f1_score': f1_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred),\n",
    "            'recall': recall_score(y_test, y_pred),\n",
    "            'roc_auc': roc_auc_score(y_test, y_pred_proba),\n",
    "            'train_time': train_time\n",
    "        }\n",
    "        \n",
    "        mlflow.log_metrics(metrics)\n",
    "        \n",
    "        # 4. Visualizaciones\n",
    "        plots = {}\n",
    "        \n",
    "        # Confusion Matrix\n",
    "        plots['confusion_matrix'] = plot_confusion_matrix(\n",
    "            y_test, y_pred, 'report_confusion_matrix.png'\n",
    "        )\n",
    "        \n",
    "        # ROC & PR Curves\n",
    "        plots['roc_pr'], _, _ = plot_roc_and_pr_curves(\n",
    "            y_test, y_pred_proba, 'report_roc_pr.png'\n",
    "        )\n",
    "        \n",
    "        # Feature Importance\n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            plots['feature_importance'], _ = plot_feature_importance(\n",
    "                model, feature_names, save_path='report_feature_importance.png'\n",
    "            )\n",
    "        \n",
    "        # Log all plots\n",
    "        for plot_name, plot_path in plots.items():\n",
    "            mlflow.log_artifact(plot_path, artifact_path='report')\n",
    "        \n",
    "        # 5. Guardar modelo\n",
    "        mlflow.sklearn.log_model(model, 'model')\n",
    "        \n",
    "        # 6. Crear resumen HTML\n",
    "        html_report = f\"\"\"\n",
    "        <html>\n",
    "        <head><title>Model Report</title></head>\n",
    "        <body>\n",
    "            <h1>Model Performance Report</h1>\n",
    "            <h2>Metrics</h2>\n",
    "            <ul>\n",
    "                <li>Accuracy: {metrics['accuracy']:.4f}</li>\n",
    "                <li>F1-Score: {metrics['f1_score']:.4f}</li>\n",
    "                <li>Precision: {metrics['precision']:.4f}</li>\n",
    "                <li>Recall: {metrics['recall']:.4f}</li>\n",
    "                <li>ROC-AUC: {metrics['roc_auc']:.4f}</li>\n",
    "            </ul>\n",
    "        </body>\n",
    "        </html>\n",
    "        \"\"\"\n",
    "        \n",
    "        with open('model_report.html', 'w') as f:\n",
    "            f.write(html_report)\n",
    "        \n",
    "        mlflow.log_artifact('model_report.html', artifact_path='report')\n",
    "        \n",
    "        print('\\n' + '='*60)\n",
    "        print('REPORTE COMPLETO GENERADO')\n",
    "        print('='*60)\n",
    "        print(f'Run ID: {run.info.run_id}')\n",
    "        print(f'\\nMétricas:')\n",
    "        for metric, value in metrics.items():\n",
    "            print(f'  {metric}: {value:.4f}')\n",
    "        print(f'\\nVisualiza el reporte en MLflow UI:')\n",
    "        print(f'http://localhost:5000/#/experiments/{run.info.experiment_id}/runs/{run.info.run_id}')\n",
    "        print('='*60)\n",
    "        \n",
    "        return run.info.run_id\n",
    "\n",
    "# Ejecutar reporte completo\n",
    "model_report = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "run_id = comprehensive_model_report(\n",
    "    model_report, X_train, X_test, y_train, y_test, \n",
    "    feature_names, 'comprehensive-ml-report'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Limpiar Archivos Temporales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Listar archivos generados\n",
    "image_files = glob.glob('*.png')\n",
    "html_files = glob.glob('*.html')\n",
    "csv_files = glob.glob('*.csv')\n",
    "\n",
    "print('Archivos generados:')\n",
    "print(f'  Imágenes: {len(image_files)}')\n",
    "print(f'  HTML: {len(html_files)}')\n",
    "print(f'  CSV: {len(csv_files)}')\n",
    "print('\\nTodos están guardados en MLflow!')\n",
    "print('\\nPara limpiar archivos locales (opcional):\")\n",
    "print('  import os; [os.remove(f) for f in glob.glob(\"*.png\") + glob.glob(\"*.html\")]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen y Best Practices\n",
    "\n",
    "### Tipos de Visualizaciones Creadas:\n",
    "\n",
    "1. **Exploración de Datos**\n",
    "   - Distribuciones de features\n",
    "   - Matrices de correlación\n",
    "   - Gráficos 3D interactivos\n",
    "\n",
    "2. **Evaluación de Modelos**\n",
    "   - Confusion Matrix\n",
    "   - ROC Curve & PR Curve\n",
    "   - Métricas comparativas\n",
    "\n",
    "3. **Interpretabilidad**\n",
    "   - Feature importance\n",
    "   - Cumulative importance\n",
    "   - Análisis de contribución\n",
    "\n",
    "4. **Comparaciones**\n",
    "   - Multi-model comparisons\n",
    "   - Radar charts\n",
    "   - Dashboards interactivos\n",
    "\n",
    "### Mejores Prácticas:\n",
    "\n",
    "✅ **Siempre guardar visualizaciones en MLflow**\n",
    "```python\n",
    "mlflow.log_artifact(plot_path, artifact_path='plots')\n",
    "```\n",
    "\n",
    "✅ **Usar nombres descriptivos**\n",
    "```python\n",
    "save_path = 'confusion_matrix_test_set.png'\n",
    "```\n",
    "\n",
    "✅ **Alta resolución para publicación**\n",
    "```python\n",
    "plt.savefig(path, dpi=150, bbox_inches='tight')\n",
    "```\n",
    "\n",
    "✅ **Combinar estáticas e interactivas**\n",
    "- Matplotlib/Seaborn: Para reportes PDF\n",
    "- Plotly: Para exploración interactiva\n",
    "\n",
    "✅ **Organizar artifacts por categoría**\n",
    "```python\n",
    "mlflow.log_artifact(path, artifact_path='evaluation')  # vs 'exploration', 'comparison'\n",
    "```\n",
    "\n",
    "### Comandos Útiles:\n",
    "\n",
    "```python\n",
    "# Buscar runs con visualizaciones\n",
    "client = MlflowClient()\n",
    "runs = client.search_runs(experiment_ids=['1'])\n",
    "\n",
    "# Descargar artifacts\n",
    "client.download_artifacts(run_id, 'plots', dst_path='./downloads')\n",
    "\n",
    "# Listar artifacts\n",
    "artifacts = client.list_artifacts(run_id)\n",
    "for artifact in artifacts:\n",
    "    print(artifact.path)\n",
    "```\n",
    "\n",
    "### Recursos:\n",
    "- [Matplotlib Gallery](https://matplotlib.org/stable/gallery/index.html)\n",
    "- [Seaborn Examples](https://seaborn.pydata.org/examples/index.html)\n",
    "- [Plotly Charts](https://plotly.com/python/)\n",
    "- [MLflow Artifacts](https://mlflow.org/docs/latest/tracking.html#logging-artifacts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
