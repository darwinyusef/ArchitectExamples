{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11 - Apache Spark BÃ¡sico: Data Processing y ETL\n",
    "\n",
    "## ðŸŽ¯ Objetivos\n",
    "- Entender los fundamentos de Apache Spark\n",
    "- Trabajar con DataFrames y transformaciones\n",
    "- Realizar operaciones ETL (Extract, Transform, Load)\n",
    "- SQL en Spark\n",
    "- Agregaciones y joins\n",
    "- OptimizaciÃ³n bÃ¡sica\n",
    "\n",
    "## ðŸ“š TecnologÃ­as\n",
    "- **Apache Spark**: Motor de procesamiento distribuido\n",
    "- **PySpark**: API de Python para Spark\n",
    "- **Pandas**: Para comparaciones\n",
    "\n",
    "## â­ Complejidad: BÃ¡sico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. InstalaciÃ³n y Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar PySpark\n",
    "!pip install pyspark pandas numpy matplotlib seaborn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ… Imports completados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Crear SparkSession\n",
    "\n",
    "La SparkSession es el punto de entrada para trabajar con Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark Basico Tutorial\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Configurar nivel de log\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "print(f\"âœ… SparkSession creada\")\n",
    "print(f\"ðŸ“Š Spark version: {spark.version}\")\n",
    "print(f\"ðŸ“Š App name: {spark.sparkContext.appName}\")\n",
    "print(f\"ðŸ“Š Master: {spark.sparkContext.master}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Crear DataFrames\n",
    "\n",
    "Hay varias formas de crear DataFrames en Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Desde listas\n",
    "data = [\n",
    "    (1, \"Juan\", 28, \"MÃ©xico\", 50000),\n",
    "    (2, \"MarÃ­a\", 35, \"EspaÃ±a\", 75000),\n",
    "    (3, \"Pedro\", 42, \"Argentina\", 60000),\n",
    "    (4, \"Ana\", 29, \"MÃ©xico\", 55000),\n",
    "    (5, \"Luis\", 38, \"EspaÃ±a\", 70000)\n",
    "]\n",
    "\n",
    "columns = [\"id\", \"nombre\", \"edad\", \"pais\", \"salario\"]\n",
    "\n",
    "df1 = spark.createDataFrame(data, columns)\n",
    "\n",
    "print(\"ðŸ“Š DataFrame desde lista:\")\n",
    "df1.show()\n",
    "\n",
    "# 2. Desde Pandas\n",
    "pandas_df = pd.DataFrame(data, columns=columns)\n",
    "df2 = spark.createDataFrame(pandas_df)\n",
    "\n",
    "print(\"\\nðŸ“Š DataFrame desde Pandas:\")\n",
    "df2.show(3)\n",
    "\n",
    "# 3. Con schema explÃ­cito\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"nombre\", StringType(), True),\n",
    "    StructField(\"edad\", IntegerType(), True),\n",
    "    StructField(\"pais\", StringType(), True),\n",
    "    StructField(\"salario\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "df3 = spark.createDataFrame(data, schema)\n",
    "\n",
    "print(\"\\nðŸ“Š Schema del DataFrame:\")\n",
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Operaciones BÃ¡sicas con DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usar df1 para ejemplos\n",
    "df = df1\n",
    "\n",
    "# InformaciÃ³n bÃ¡sica\n",
    "print(\"ðŸ“Š InformaciÃ³n del DataFrame:\\n\")\n",
    "print(f\"NÃºmero de filas: {df.count()}\")\n",
    "print(f\"NÃºmero de columnas: {len(df.columns)}\")\n",
    "print(f\"Columnas: {df.columns}\")\n",
    "\n",
    "# Primeras filas\n",
    "print(\"\\nðŸ“Š Primeras 3 filas:\")\n",
    "df.show(3)\n",
    "\n",
    "# Describir datos\n",
    "print(\"\\nðŸ“Š EstadÃ­sticas descriptivas:\")\n",
    "df.describe().show()\n",
    "\n",
    "# Tipos de datos\n",
    "print(\"\\nðŸ“Š Tipos de datos:\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. SelecciÃ³n y Filtrado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select - seleccionar columnas\n",
    "print(\"ðŸ“Š Select (nombre y salario):\")\n",
    "df.select(\"nombre\", \"salario\").show()\n",
    "\n",
    "# Select con expresiones\n",
    "print(\"\\nðŸ“Š Select con expresiones:\")\n",
    "df.select(\n",
    "    \"nombre\",\n",
    "    (F.col(\"salario\") / 12).alias(\"salario_mensual\")\n",
    ").show()\n",
    "\n",
    "# Filter - filtrar filas\n",
    "print(\"\\nðŸ“Š Filter (edad > 30):\")\n",
    "df.filter(F.col(\"edad\") > 30).show()\n",
    "\n",
    "# MÃºltiples condiciones\n",
    "print(\"\\nðŸ“Š Filter (edad > 30 AND pais = MÃ©xico):\")\n",
    "df.filter(\n",
    "    (F.col(\"edad\") > 30) & (F.col(\"pais\") == \"MÃ©xico\")\n",
    ").show()\n",
    "\n",
    "# Where (sinÃ³nimo de filter)\n",
    "print(\"\\nðŸ“Š Where (salario >= 60000):\")\n",
    "df.where(F.col(\"salario\") >= 60000).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Transformaciones y Nuevas Columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# withColumn - agregar o modificar columna\n",
    "print(\"ðŸ“Š Agregar columna (salario_anual_bonus):\")\n",
    "df_with_bonus = df.withColumn(\n",
    "    \"salario_con_bonus\",\n",
    "    F.col(\"salario\") * 1.10\n",
    ")\n",
    "df_with_bonus.show()\n",
    "\n",
    "# MÃºltiples transformaciones\n",
    "print(\"\\nðŸ“Š MÃºltiples transformaciones:\")\n",
    "df_transformed = df \\\n",
    "    .withColumn(\"edad_grupo\", \n",
    "                F.when(F.col(\"edad\") < 30, \"Joven\")\n",
    "                 .when(F.col(\"edad\") < 40, \"Adulto\")\n",
    "                 .otherwise(\"Senior\")) \\\n",
    "    .withColumn(\"salario_k\", F.round(F.col(\"salario\") / 1000, 1)) \\\n",
    "    .withColumn(\"nombre_upper\", F.upper(F.col(\"nombre\")))\n",
    "\n",
    "df_transformed.show()\n",
    "\n",
    "# Renombrar columnas\n",
    "print(\"\\nðŸ“Š Renombrar columna:\")\n",
    "df.withColumnRenamed(\"pais\", \"country\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Agregaciones\n",
    "\n",
    "Operaciones de agregaciÃ³n como sum, avg, max, min, count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregaciones simples\n",
    "print(\"ðŸ“Š Agregaciones bÃ¡sicas:\")\n",
    "df.agg(\n",
    "    F.count(\"*\").alias(\"total\"),\n",
    "    F.avg(\"edad\").alias(\"edad_promedio\"),\n",
    "    F.max(\"salario\").alias(\"salario_max\"),\n",
    "    F.min(\"salario\").alias(\"salario_min\"),\n",
    "    F.sum(\"salario\").alias(\"salario_total\")\n",
    ").show()\n",
    "\n",
    "# GroupBy\n",
    "print(\"\\nðŸ“Š GroupBy por paÃ­s:\")\n",
    "df.groupBy(\"pais\").agg(\n",
    "    F.count(\"*\").alias(\"empleados\"),\n",
    "    F.avg(\"edad\").alias(\"edad_promedio\"),\n",
    "    F.avg(\"salario\").alias(\"salario_promedio\")\n",
    ").show()\n",
    "\n",
    "# MÃºltiples agrupaciones\n",
    "print(\"\\nðŸ“Š GroupBy con mÃºltiples agregaciones:\")\n",
    "df_transformed.groupBy(\"pais\", \"edad_grupo\").agg(\n",
    "    F.count(\"*\").alias(\"count\"),\n",
    "    F.round(F.avg(\"salario\"), 2).alias(\"salario_avg\")\n",
    ").orderBy(\"pais\", \"edad_grupo\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Ordenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OrderBy ascendente\n",
    "print(\"ðŸ“Š OrderBy (edad ascendente):\")\n",
    "df.orderBy(\"edad\").show()\n",
    "\n",
    "# OrderBy descendente\n",
    "print(\"\\nðŸ“Š OrderBy (salario descendente):\")\n",
    "df.orderBy(F.col(\"salario\").desc()).show()\n",
    "\n",
    "# Ordenar por mÃºltiples columnas\n",
    "print(\"\\nðŸ“Š OrderBy mÃºltiple (pais asc, salario desc):\")\n",
    "df.orderBy(\n",
    "    F.col(\"pais\").asc(),\n",
    "    F.col(\"salario\").desc()\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. SQL en Spark\n",
    "\n",
    "Podemos usar SQL estÃ¡ndar en Spark DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Registrar DataFrame como tabla temporal\n",
    "df.createOrReplaceTempView(\"empleados\")\n",
    "\n",
    "# Query SQL bÃ¡sico\n",
    "print(\"ðŸ“Š SQL Query bÃ¡sico:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT nombre, pais, salario\n",
    "    FROM empleados\n",
    "    WHERE salario > 60000\n",
    "\"\"\").show()\n",
    "\n",
    "# Query con agregaciÃ³n\n",
    "print(\"\\nðŸ“Š SQL con agregaciÃ³n:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        pais,\n",
    "        COUNT(*) as empleados,\n",
    "        ROUND(AVG(edad), 2) as edad_promedio,\n",
    "        ROUND(AVG(salario), 2) as salario_promedio\n",
    "    FROM empleados\n",
    "    GROUP BY pais\n",
    "    ORDER BY salario_promedio DESC\n",
    "\"\"\").show()\n",
    "\n",
    "# Query complejo con subconsultas\n",
    "print(\"\\nðŸ“Š SQL con subconsulta:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM empleados\n",
    "    WHERE salario > (\n",
    "        SELECT AVG(salario) FROM empleados\n",
    "    )\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Joins\n",
    "\n",
    "Combinar DataFrames con diferentes tipos de joins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear segundo DataFrame para joins\n",
    "departamentos_data = [\n",
    "    (1, \"TecnologÃ­a\", \"Edificio A\"),\n",
    "    (2, \"Ventas\", \"Edificio B\"),\n",
    "    (3, \"Marketing\", \"Edificio C\"),\n",
    "    (4, \"Recursos Humanos\", \"Edificio A\"),\n",
    "    (6, \"Finanzas\", \"Edificio B\")  # ID 6 no existe en empleados\n",
    "]\n",
    "\n",
    "df_dept = spark.createDataFrame(\n",
    "    departamentos_data, \n",
    "    [\"id\", \"departamento\", \"ubicacion\"]\n",
    ")\n",
    "\n",
    "print(\"ðŸ“Š DataFrame de departamentos:\")\n",
    "df_dept.show()\n",
    "\n",
    "# Inner Join\n",
    "print(\"\\nðŸ“Š Inner Join:\")\n",
    "df.join(df_dept, \"id\", \"inner\").show()\n",
    "\n",
    "# Left Join\n",
    "print(\"\\nðŸ“Š Left Join:\")\n",
    "df.join(df_dept, \"id\", \"left\").show()\n",
    "\n",
    "# Right Join\n",
    "print(\"\\nðŸ“Š Right Join:\")\n",
    "df.join(df_dept, \"id\", \"right\").show()\n",
    "\n",
    "# Full Outer Join\n",
    "print(\"\\nðŸ“Š Full Outer Join:\")\n",
    "df.join(df_dept, \"id\", \"outer\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Window Functions\n",
    "\n",
    "Funciones de ventana para anÃ¡lisis avanzados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir ventana por paÃ­s\n",
    "window_pais = Window.partitionBy(\"pais\").orderBy(F.col(\"salario\").desc())\n",
    "\n",
    "# Ranking dentro de cada paÃ­s\n",
    "print(\"ðŸ“Š Ranking por paÃ­s:\")\n",
    "df.withColumn(\n",
    "    \"rank\",\n",
    "    F.row_number().over(window_pais)\n",
    ").show()\n",
    "\n",
    "# Dense rank\n",
    "print(\"\\nðŸ“Š Dense Rank:\")\n",
    "df.withColumn(\n",
    "    \"dense_rank\",\n",
    "    F.dense_rank().over(window_pais)\n",
    ").show()\n",
    "\n",
    "# Lag y Lead (valores anteriores y siguientes)\n",
    "print(\"\\nðŸ“Š Lag y Lead:\")\n",
    "df.withColumn(\n",
    "    \"salario_anterior\",\n",
    "    F.lag(\"salario\", 1).over(window_pais)\n",
    ").withColumn(\n",
    "    \"salario_siguiente\",\n",
    "    F.lead(\"salario\", 1).over(window_pais)\n",
    ").show()\n",
    "\n",
    "# Agregaciones con ventana\n",
    "print(\"\\nðŸ“Š Agregaciones con ventana:\")\n",
    "df.withColumn(\n",
    "    \"salario_avg_pais\",\n",
    "    F.round(F.avg(\"salario\").over(Window.partitionBy(\"pais\")), 2)\n",
    ").withColumn(\n",
    "    \"diff_vs_avg\",\n",
    "    F.round(F.col(\"salario\") - F.avg(\"salario\").over(Window.partitionBy(\"pais\")), 2)\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Lectura y Escritura de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear datos mÃ¡s grandes para demo\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Generar datos de ventas sintÃ©ticos\n",
    "ventas_data = []\n",
    "productos = [\"Laptop\", \"Mouse\", \"Teclado\", \"Monitor\", \"AudÃ­fonos\"]\n",
    "regiones = [\"Norte\", \"Sur\", \"Este\", \"Oeste\"]\n",
    "\n",
    "start_date = datetime(2024, 1, 1)\n",
    "\n",
    "for i in range(1000):\n",
    "    fecha = start_date + timedelta(days=random.randint(0, 365))\n",
    "    ventas_data.append((\n",
    "        i + 1,\n",
    "        fecha.strftime(\"%Y-%m-%d\"),\n",
    "        random.choice(productos),\n",
    "        random.choice(regiones),\n",
    "        random.randint(1, 10),  # cantidad\n",
    "        round(random.uniform(100, 2000), 2)  # precio\n",
    "    ))\n",
    "\n",
    "df_ventas = spark.createDataFrame(\n",
    "    ventas_data,\n",
    "    [\"id\", \"fecha\", \"producto\", \"region\", \"cantidad\", \"precio\"]\n",
    ")\n",
    "\n",
    "print(\"ðŸ“Š DataFrame de ventas (muestra):\")\n",
    "df_ventas.show(10)\n",
    "\n",
    "# Escribir CSV\n",
    "df_ventas.write.mode(\"overwrite\").csv(\"ventas_csv\", header=True)\n",
    "print(\"\\nâœ… Guardado en CSV: ventas_csv/\")\n",
    "\n",
    "# Escribir Parquet (formato columnar eficiente)\n",
    "df_ventas.write.mode(\"overwrite\").parquet(\"ventas_parquet\")\n",
    "print(\"âœ… Guardado en Parquet: ventas_parquet/\")\n",
    "\n",
    "# Escribir JSON\n",
    "df_ventas.limit(100).write.mode(\"overwrite\").json(\"ventas_json\")\n",
    "print(\"âœ… Guardado en JSON: ventas_json/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer archivos\n",
    "print(\"ðŸ“Š Leer CSV:\")\n",
    "df_from_csv = spark.read.csv(\"ventas_csv\", header=True, inferSchema=True)\n",
    "df_from_csv.printSchema()\n",
    "df_from_csv.show(5)\n",
    "\n",
    "print(\"\\nðŸ“Š Leer Parquet:\")\n",
    "df_from_parquet = spark.read.parquet(\"ventas_parquet\")\n",
    "df_from_parquet.show(5)\n",
    "\n",
    "print(\"\\nðŸ“Š Leer JSON:\")\n",
    "df_from_json = spark.read.json(\"ventas_json\")\n",
    "df_from_json.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. AnÃ¡lisis ETL Completo\n",
    "\n",
    "Ejemplo de pipeline ETL completo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRACT - Leer datos\n",
    "df_raw = spark.read.parquet(\"ventas_parquet\")\n",
    "\n",
    "print(\"ðŸ“Š EXTRACT - Datos crudos:\")\n",
    "print(f\"Total registros: {df_raw.count()}\")\n",
    "df_raw.show(5)\n",
    "\n",
    "# TRANSFORM - Transformaciones\n",
    "df_clean = df_raw \\\n",
    "    .withColumn(\"fecha\", F.to_date(\"fecha\")) \\\n",
    "    .withColumn(\"total\", F.col(\"cantidad\") * F.col(\"precio\")) \\\n",
    "    .withColumn(\"aÃ±o\", F.year(\"fecha\")) \\\n",
    "    .withColumn(\"mes\", F.month(\"fecha\")) \\\n",
    "    .withColumn(\"trimestre\", F.quarter(\"fecha\")) \\\n",
    "    .filter(F.col(\"cantidad\") > 0) \\\n",
    "    .filter(F.col(\"precio\") > 0)\n",
    "\n",
    "print(\"\\nðŸ“Š TRANSFORM - Datos limpios:\")\n",
    "df_clean.show(5)\n",
    "\n",
    "# AnÃ¡lisis por producto\n",
    "df_producto = df_clean.groupBy(\"producto\").agg(\n",
    "    F.count(\"*\").alias(\"ventas_count\"),\n",
    "    F.sum(\"cantidad\").alias(\"cantidad_total\"),\n",
    "    F.round(F.sum(\"total\"), 2).alias(\"ingresos_total\"),\n",
    "    F.round(F.avg(\"precio\"), 2).alias(\"precio_promedio\")\n",
    ").orderBy(F.col(\"ingresos_total\").desc())\n",
    "\n",
    "print(\"\\nðŸ“Š AnÃ¡lisis por producto:\")\n",
    "df_producto.show()\n",
    "\n",
    "# AnÃ¡lisis temporal\n",
    "df_temporal = df_clean.groupBy(\"aÃ±o\", \"mes\").agg(\n",
    "    F.count(\"*\").alias(\"ventas\"),\n",
    "    F.round(F.sum(\"total\"), 2).alias(\"ingresos\")\n",
    ").orderBy(\"aÃ±o\", \"mes\")\n",
    "\n",
    "print(\"\\nðŸ“Š AnÃ¡lisis temporal:\")\n",
    "df_temporal.show()\n",
    "\n",
    "# AnÃ¡lisis por regiÃ³n\n",
    "df_region = df_clean.groupBy(\"region\", \"producto\").agg(\n",
    "    F.sum(\"cantidad\").alias(\"cantidad_total\"),\n",
    "    F.round(F.sum(\"total\"), 2).alias(\"ingresos_total\")\n",
    ").orderBy(\"region\", F.col(\"ingresos_total\").desc())\n",
    "\n",
    "print(\"\\nðŸ“Š AnÃ¡lisis por regiÃ³n y producto:\")\n",
    "df_region.show()\n",
    "\n",
    "# LOAD - Guardar resultados\n",
    "df_producto.write.mode(\"overwrite\").parquet(\"analytics/producto\")\n",
    "df_temporal.write.mode(\"overwrite\").parquet(\"analytics/temporal\")\n",
    "df_region.write.mode(\"overwrite\").parquet(\"analytics/region\")\n",
    "\n",
    "print(\"\\nâœ… LOAD - Datos guardados en analytics/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. OptimizaciÃ³n y Mejores PrÃ¡cticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Cache - para DataFrames usados mÃºltiples veces\n",
    "df_cached = df_clean.cache()\n",
    "\n",
    "print(\"ðŸ“Š DataFrame cacheado\")\n",
    "print(f\"Registros: {df_cached.count()}\")\n",
    "\n",
    "# 2. Repartition - redistribuir datos\n",
    "df_repartitioned = df_clean.repartition(4, \"region\")\n",
    "print(f\"\\nðŸ“Š Particiones: {df_repartitioned.rdd.getNumPartitions()}\")\n",
    "\n",
    "# 3. Coalesce - reducir particiones (mÃ¡s eficiente que repartition)\n",
    "df_coalesced = df_clean.coalesce(2)\n",
    "print(f\"ðŸ“Š Particiones despuÃ©s de coalesce: {df_coalesced.rdd.getNumPartitions()}\")\n",
    "\n",
    "# 4. Explain - ver plan de ejecuciÃ³n\n",
    "print(\"\\nðŸ“Š Plan de ejecuciÃ³n:\")\n",
    "df_clean.groupBy(\"producto\").agg(F.sum(\"total\")).explain()\n",
    "\n",
    "# 5. Broadcast join - para tablas pequeÃ±as\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# Simular tabla pequeÃ±a de categorÃ­as\n",
    "categorias_data = [\n",
    "    (\"Laptop\", \"ElectrÃ³nica\"),\n",
    "    (\"Mouse\", \"Accesorios\"),\n",
    "    (\"Teclado\", \"Accesorios\"),\n",
    "    (\"Monitor\", \"ElectrÃ³nica\"),\n",
    "    (\"AudÃ­fonos\", \"Accesorios\")\n",
    "]\n",
    "df_categorias = spark.createDataFrame(categorias_data, [\"producto\", \"categoria\"])\n",
    "\n",
    "# Join con broadcast\n",
    "df_with_cat = df_clean.join(\n",
    "    broadcast(df_categorias),\n",
    "    \"producto\",\n",
    "    \"left\"\n",
    ")\n",
    "\n",
    "print(\"\\nðŸ“Š Join con broadcast:\")\n",
    "df_with_cat.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. ConversiÃ³n Spark â†” Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark a Pandas\n",
    "pandas_df = df_producto.toPandas()\n",
    "\n",
    "print(\"ðŸ“Š DataFrame de Pandas:\")\n",
    "print(type(pandas_df))\n",
    "print(pandas_df.head())\n",
    "\n",
    "# Visualizar con pandas/matplotlib\n",
    "plt.figure(figsize=(10, 6))\n",
    "pandas_df.plot(x='producto', y='ingresos_total', kind='bar', legend=False)\n",
    "plt.title('Ingresos por Producto')\n",
    "plt.xlabel('Producto')\n",
    "plt.ylabel('Ingresos Totales')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Pandas a Spark\n",
    "spark_df = spark.createDataFrame(pandas_df)\n",
    "print(\"\\nðŸ“Š De vuelta a Spark:\")\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. UDFs (User Defined Functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, FloatType\n",
    "\n",
    "# UDF simple\n",
    "@udf(returnType=StringType())\n",
    "def categorizar_precio(precio):\n",
    "    if precio < 500:\n",
    "        return \"Bajo\"\n",
    "    elif precio < 1000:\n",
    "        return \"Medio\"\n",
    "    else:\n",
    "        return \"Alto\"\n",
    "\n",
    "# Aplicar UDF\n",
    "df_with_category = df_clean.withColumn(\n",
    "    \"precio_categoria\",\n",
    "    categorizar_precio(F.col(\"precio\"))\n",
    ")\n",
    "\n",
    "print(\"ðŸ“Š DataFrame con UDF:\")\n",
    "df_with_category.select(\"producto\", \"precio\", \"precio_categoria\").show(10)\n",
    "\n",
    "# UDF con decorador completo\n",
    "def calcular_descuento(precio, cantidad):\n",
    "    if cantidad >= 5:\n",
    "        return precio * 0.9  # 10% descuento\n",
    "    return precio\n",
    "\n",
    "udf_descuento = udf(calcular_descuento, FloatType())\n",
    "\n",
    "df_with_discount = df_clean.withColumn(\n",
    "    \"precio_con_descuento\",\n",
    "    F.round(udf_descuento(F.col(\"precio\"), F.col(\"cantidad\")), 2)\n",
    ")\n",
    "\n",
    "print(\"\\nðŸ“Š DataFrame con descuento:\")\n",
    "df_with_discount.select(\"producto\", \"cantidad\", \"precio\", \"precio_con_descuento\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Limpieza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpersist cached data\n",
    "df_cached.unpersist()\n",
    "\n",
    "print(\"âœ… Cache liberado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Resumen y Mejores PrÃ¡cticas\n",
    "\n",
    "### âœ… Conceptos Clave:\n",
    "1. **SparkSession**: Punto de entrada para Spark\n",
    "2. **DataFrames**: Estructura de datos distribuida\n",
    "3. **Transformaciones**: Lazy evaluation (map, filter, select)\n",
    "4. **Acciones**: Trigger execution (count, show, collect)\n",
    "5. **SQL**: Queries estÃ¡ndar SQL\n",
    "6. **Partitioning**: DistribuciÃ³n de datos\n",
    "7. **Caching**: OptimizaciÃ³n de performance\n",
    "\n",
    "### ðŸ’¡ Mejores PrÃ¡cticas:\n",
    "- âœ… Usa transformaciones en lugar de UDFs cuando sea posible (mÃ¡s rÃ¡pido)\n",
    "- âœ… Cache DataFrames que uses mÃºltiples veces\n",
    "- âœ… Usa broadcast joins para tablas pequeÃ±as\n",
    "- âœ… Particiona datos por columnas frecuentemente filtradas\n",
    "- âœ… Prefiere Parquet sobre CSV para almacenamiento\n",
    "- âœ… Usa `explain()` para entender el plan de ejecuciÃ³n\n",
    "- âœ… Evita `collect()` en datasets grandes (trae todo a memoria)\n",
    "- âœ… Usa SQL cuando sea mÃ¡s legible\n",
    "- âœ… Repartition antes de escribir grandes volÃºmenes\n",
    "\n",
    "### ðŸš€ PrÃ³ximos Pasos:\n",
    "- Spark Streaming para datos en tiempo real\n",
    "- MLlib para machine learning distribuido\n",
    "- GraphX para procesamiento de grafos\n",
    "- IntegraciÃ³n con S3, HDFS, Kafka\n",
    "- OptimizaciÃ³n avanzada con Catalyst Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸŽ‰ Tutorial de Spark BÃ¡sico completado!\")\n",
    "print(f\"\\nðŸ“Š Resumen:\")\n",
    "print(f\"   - DataFrames creados: 5+\")\n",
    "print(f\"   - Operaciones aprendidas: 20+\")\n",
    "print(f\"   - Archivos generados: ventas_csv/, ventas_parquet/, analytics/\")\n",
    "print(f\"\\nðŸ’¡ Spark es ideal para:\")\n",
    "print(f\"   - Procesamiento de grandes volÃºmenes de datos\")\n",
    "print(f\"   - ETL distribuido\")\n",
    "print(f\"   - Analytics en tiempo real\")\n",
    "print(f\"   - Machine Learning a escala\")\n",
    "\n",
    "# Detener SparkSession\n",
    "spark.stop()\n",
    "print(\"\\nâœ… SparkSession detenida\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
