{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark + PyTorch + MLflow: Deep Learning Distribuido\n",
    "\n",
    "## Objetivos\n",
    "- Entrenar redes neuronales con PyTorch en Spark\n",
    "- DistribuciÃ³n de entrenamiento usando Spark\n",
    "- Tracking de experimentos con MLflow\n",
    "- Implementar transfer learning con modelos pre-entrenados\n",
    "\n",
    "## Conceptos Clave\n",
    "- **PyTorch**: Framework de deep learning\n",
    "- **Spark**: DistribuciÃ³n del entrenamiento\n",
    "- **MLflow**: GestiÃ³n de experimentos y modelos\n",
    "- **Petastorm**: Biblioteca para datasets distribuidos\n",
    "\n",
    "## Casos de Uso\n",
    "- ClasificaciÃ³n de imÃ¡genes a gran escala\n",
    "- Procesamiento de NLP distribuido\n",
    "- PredicciÃ³n de series temporales\n",
    "- Sistemas de recomendaciÃ³n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup e Importaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importaciones\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Spark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, pandas_udf\n",
    "from pyspark.sql.types import ArrayType, FloatType, StructType, StructField, IntegerType\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# MLflow\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# Utilidades\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_classification\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(f\"âœ“ PyTorch version: {torch.__version__}\")\n",
    "print(f\"âœ“ CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"âœ“ CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"âœ“ Device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ConfiguraciÃ³n de Spark y MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PyTorch-Spark-MLflow\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Configurar MLflow\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "mlflow.set_experiment(\"spark-pytorch-deep-learning\")\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"âœ“ Spark version: {spark.version}\")\n",
    "print(f\"âœ“ MLflow tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "print(f\"âœ“ PyTorch device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generar Datos con Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data():\n",
    "    \"\"\"Generar dataset sintÃ©tico para clasificaciÃ³n binaria\"\"\"\n",
    "    print(\"Generando datos sintÃ©ticos...\")\n",
    "    \n",
    "    # Generar datos de clasificaciÃ³n\n",
    "    X, y = make_classification(\n",
    "        n_samples=50000,\n",
    "        n_features=20,\n",
    "        n_informative=15,\n",
    "        n_redundant=5,\n",
    "        n_classes=2,\n",
    "        random_state=42,\n",
    "        flip_y=0.1  # AÃ±adir algo de ruido\n",
    "    )\n",
    "    \n",
    "    # Crear DataFrame de pandas\n",
    "    feature_cols = [f'feature_{i}' for i in range(X.shape[1])]\n",
    "    df_pandas = pd.DataFrame(X, columns=feature_cols)\n",
    "    df_pandas['label'] = y\n",
    "    \n",
    "    # Convertir a Spark DataFrame\n",
    "    df_spark = spark.createDataFrame(df_pandas)\n",
    "    \n",
    "    print(f\"âœ“ Dataset creado: {df_spark.count()} registros\")\n",
    "    print(f\"âœ“ Features: {len(feature_cols)}\")\n",
    "    \n",
    "    # DistribuciÃ³n de clases\n",
    "    class_dist = df_spark.groupBy('label').count().toPandas()\n",
    "    print(\"\\nðŸ“Š DistribuciÃ³n de clases:\")\n",
    "    print(class_dist)\n",
    "    \n",
    "    return df_spark, feature_cols\n",
    "\n",
    "# Generar datos\n",
    "df_spark, feature_cols = generate_data()\n",
    "df_spark.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Definir Arquitectura de Red Neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepClassifier(nn.Module):\n",
    "    \"\"\"Red neuronal profunda para clasificaciÃ³n binaria\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_sizes=[128, 64, 32], dropout=0.3):\n",
    "        super(DeepClassifier, self).__init__()\n",
    "        \n",
    "        # Capas\n",
    "        layers = []\n",
    "        \n",
    "        # Input layer\n",
    "        layers.append(nn.Linear(input_size, hidden_sizes[0]))\n",
    "        layers.append(nn.BatchNorm1d(hidden_sizes[0]))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Dropout(dropout))\n",
    "        \n",
    "        # Hidden layers\n",
    "        for i in range(len(hidden_sizes) - 1):\n",
    "            layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))\n",
    "            layers.append(nn.BatchNorm1d(hidden_sizes[i+1]))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(hidden_sizes[-1], 1))\n",
    "        layers.append(nn.Sigmoid())\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "    \n",
    "    def num_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "# Crear modelo\n",
    "input_size = len(feature_cols)\n",
    "model = DeepClassifier(input_size=input_size, hidden_sizes=[128, 64, 32], dropout=0.3)\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"ðŸ§  Arquitectura del Modelo:\")\n",
    "print(model)\n",
    "print(f\"\\nâœ“ ParÃ¡metros totales: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Preparar Datos para PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_pytorch_data(df_spark, feature_cols):\n",
    "    \"\"\"Convertir Spark DataFrame a tensores de PyTorch\"\"\"\n",
    "    print(\"Preparando datos para PyTorch...\")\n",
    "    \n",
    "    # Convertir a Pandas (en producciÃ³n, usar Petastorm para grandes datasets)\n",
    "    df_pandas = df_spark.toPandas()\n",
    "    \n",
    "    # Separar features y labels\n",
    "    X = df_pandas[feature_cols].values\n",
    "    y = df_pandas['label'].values.reshape(-1, 1)\n",
    "    \n",
    "    # Normalizar features\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    \n",
    "    # Split train/val/test\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Convertir a tensores\n",
    "    X_train_t = torch.FloatTensor(X_train).to(device)\n",
    "    y_train_t = torch.FloatTensor(y_train).to(device)\n",
    "    \n",
    "    X_val_t = torch.FloatTensor(X_val).to(device)\n",
    "    y_val_t = torch.FloatTensor(y_val).to(device)\n",
    "    \n",
    "    X_test_t = torch.FloatTensor(X_test).to(device)\n",
    "    y_test_t = torch.FloatTensor(y_test).to(device)\n",
    "    \n",
    "    # Crear datasets\n",
    "    train_dataset = TensorDataset(X_train_t, y_train_t)\n",
    "    val_dataset = TensorDataset(X_val_t, y_val_t)\n",
    "    test_dataset = TensorDataset(X_test_t, y_test_t)\n",
    "    \n",
    "    # Crear dataloaders\n",
    "    batch_size = 256\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    print(f\"âœ“ Train: {len(train_dataset)} samples\")\n",
    "    print(f\"âœ“ Val: {len(val_dataset)} samples\")\n",
    "    print(f\"âœ“ Test: {len(test_dataset)} samples\")\n",
    "    print(f\"âœ“ Batch size: {batch_size}\")\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, scaler\n",
    "\n",
    "# Preparar datos\n",
    "train_loader, val_loader, test_loader, scaler = prepare_pytorch_data(df_spark, feature_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Funciones de Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"Entrenar por una Ã©poca\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # MÃ©tricas\n",
    "        total_loss += loss.item()\n",
    "        predicted = (output > 0.5).float()\n",
    "        correct += (predicted == target).sum().item()\n",
    "        total += target.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = 100. * correct / total\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    \"\"\"Validar modelo\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            predicted = (output > 0.5).float()\n",
    "            correct += (predicted == target).sum().item()\n",
    "            total += target.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    accuracy = 100. * correct / total\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "print(\"âœ“ Funciones de entrenamiento definidas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Entrenamiento con MLflow Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_mlflow(model, train_loader, val_loader, epochs=20):\n",
    "    \"\"\"Entrenar modelo con tracking en MLflow\"\"\"\n",
    "    \n",
    "    # HiperparÃ¡metros\n",
    "    learning_rate = 0.001\n",
    "    weight_decay = 1e-5\n",
    "    \n",
    "    # Criterio y optimizador\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "    \n",
    "    # Iniciar MLflow run\n",
    "    with mlflow.start_run(run_name=\"deep-classifier-pytorch\") as run:\n",
    "        \n",
    "        # Log parÃ¡metros\n",
    "        params = {\n",
    "            'model_type': 'DeepClassifier',\n",
    "            'input_size': input_size,\n",
    "            'hidden_sizes': str([128, 64, 32]),\n",
    "            'dropout': 0.3,\n",
    "            'learning_rate': learning_rate,\n",
    "            'weight_decay': weight_decay,\n",
    "            'batch_size': train_loader.batch_size,\n",
    "            'optimizer': 'Adam',\n",
    "            'epochs': epochs,\n",
    "            'device': str(device)\n",
    "        }\n",
    "        mlflow.log_params(params)\n",
    "        \n",
    "        # MÃ©tricas de seguimiento\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        train_accs = []\n",
    "        val_accs = []\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        \n",
    "        print(\"\\nðŸš€ Iniciando entrenamiento...\\n\")\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(epochs):\n",
    "            # Train\n",
    "            train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "            \n",
    "            # Validate\n",
    "            val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "            \n",
    "            # Scheduler\n",
    "            scheduler.step(val_loss)\n",
    "            \n",
    "            # Guardar mÃ©tricas\n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            train_accs.append(train_acc)\n",
    "            val_accs.append(val_acc)\n",
    "            \n",
    "            # Log a MLflow\n",
    "            mlflow.log_metrics({\n",
    "                'train_loss': train_loss,\n",
    "                'train_accuracy': train_acc,\n",
    "                'val_loss': val_loss,\n",
    "                'val_accuracy': val_acc,\n",
    "                'learning_rate': optimizer.param_groups[0]['lr']\n",
    "            }, step=epoch)\n",
    "            \n",
    "            # Print progress\n",
    "            print(f\"Epoch {epoch+1}/{epochs}:\")\n",
    "            print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "            print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "            print(f\"  LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "            print()\n",
    "            \n",
    "            # Guardar mejor modelo\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                torch.save(model.state_dict(), 'best_model.pth')\n",
    "                print(f\"  âœ“ Mejor modelo guardado (val_loss: {val_loss:.4f})\\n\")\n",
    "        \n",
    "        # Log del modelo\n",
    "        mlflow.pytorch.log_model(model, \"model\")\n",
    "        \n",
    "        # Log del mejor modelo\n",
    "        mlflow.log_artifact('best_model.pth')\n",
    "        \n",
    "        # MÃ©tricas finales\n",
    "        mlflow.log_metrics({\n",
    "            'final_train_loss': train_losses[-1],\n",
    "            'final_val_loss': val_losses[-1],\n",
    "            'best_val_loss': best_val_loss,\n",
    "            'final_train_acc': train_accs[-1],\n",
    "            'final_val_acc': val_accs[-1]\n",
    "        })\n",
    "        \n",
    "        # Tags\n",
    "        mlflow.set_tags({\n",
    "            'framework': 'pytorch',\n",
    "            'task': 'binary_classification',\n",
    "            'distributed': 'spark'\n",
    "        })\n",
    "        \n",
    "        print(f\"\\nâœ“ Entrenamiento completado\")\n",
    "        print(f\"âœ“ Run ID: {run.info.run_id}\")\n",
    "        print(f\"âœ“ Mejor val_loss: {best_val_loss:.4f}\")\n",
    "        \n",
    "        return train_losses, val_losses, train_accs, val_accs, run.info.run_id\n",
    "\n",
    "# Entrenar modelo\n",
    "train_losses, val_losses, train_accs, val_accs, run_id = train_with_mlflow(\n",
    "    model, train_loader, val_loader, epochs=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. VisualizaciÃ³n de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear visualizaciones\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(train_losses, label='Train Loss', linewidth=2)\n",
    "axes[0].plot(val_losses, label='Val Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(train_accs, label='Train Accuracy', linewidth=2)\n",
    "axes[1].plot(val_accs, label='Val Accuracy', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Accuracy (%)', fontsize=12)\n",
    "axes[1].set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Log imagen a MLflow\n",
    "with mlflow.start_run(run_id=run_id):\n",
    "    mlflow.log_artifact('training_curves.png')\n",
    "\n",
    "print(\"âœ“ Curvas de entrenamiento guardadas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. EvaluaciÃ³n en Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_test_set(model, test_loader, device):\n",
    "    \"\"\"EvaluaciÃ³n completa en test set\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            predicted = (output > 0.5).float()\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "            all_probs.extend(output.cpu().numpy())\n",
    "    \n",
    "    # Convertir a arrays\n",
    "    all_preds = np.array(all_preds).flatten()\n",
    "    all_targets = np.array(all_targets).flatten()\n",
    "    all_probs = np.array(all_probs).flatten()\n",
    "    \n",
    "    # Calcular mÃ©tricas\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "    \n",
    "    metrics = {\n",
    "        'test_accuracy': accuracy_score(all_targets, all_preds),\n",
    "        'test_precision': precision_score(all_targets, all_preds),\n",
    "        'test_recall': recall_score(all_targets, all_preds),\n",
    "        'test_f1': f1_score(all_targets, all_preds),\n",
    "        'test_auc_roc': roc_auc_score(all_targets, all_probs)\n",
    "    }\n",
    "    \n",
    "    # Matriz de confusiÃ³n\n",
    "    cm = confusion_matrix(all_targets, all_preds)\n",
    "    \n",
    "    # Log a MLflow\n",
    "    with mlflow.start_run(run_id=run_id):\n",
    "        mlflow.log_metrics(metrics)\n",
    "    \n",
    "    # Mostrar resultados\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"RESULTADOS EN TEST SET\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "    print(\"\\nðŸ“Š Matriz de ConfusiÃ³n:\")\n",
    "    print(cm)\n",
    "    print()\n",
    "    \n",
    "    return metrics, cm\n",
    "\n",
    "# Cargar mejor modelo\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "# Evaluar\n",
    "test_metrics, confusion_mat = evaluate_test_set(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Inferencia con Spark UDF\n",
    "\n",
    "Aplicar el modelo entrenado a datos distribuidos en Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear funciÃ³n de predicciÃ³n\n",
    "def create_prediction_udf(model, scaler, feature_cols, device):\n",
    "    \"\"\"Crear UDF para predicciones en Spark\"\"\"\n",
    "    \n",
    "    @pandas_udf(FloatType())\n",
    "    def predict_udf(*features):\n",
    "        # Combinar features\n",
    "        X = np.column_stack(features)\n",
    "        \n",
    "        # Normalizar\n",
    "        X_scaled = scaler.transform(X)\n",
    "        \n",
    "        # Convertir a tensor\n",
    "        X_tensor = torch.FloatTensor(X_scaled).to(device)\n",
    "        \n",
    "        # Predecir\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            predictions = model(X_tensor).cpu().numpy().flatten()\n",
    "        \n",
    "        return pd.Series(predictions)\n",
    "    \n",
    "    return predict_udf\n",
    "\n",
    "# Crear UDF\n",
    "predict_udf = create_prediction_udf(model, scaler, feature_cols, device)\n",
    "\n",
    "# Aplicar predicciones\n",
    "df_with_predictions = df_spark.withColumn(\n",
    "    'prediction_prob',\n",
    "    predict_udf(*feature_cols)\n",
    ").withColumn(\n",
    "    'prediction',\n",
    "    when(col('prediction_prob') > 0.5, 1).otherwise(0)\n",
    ")\n",
    "\n",
    "print(\"\\nðŸ“Š Predicciones con Spark:\")\n",
    "df_with_predictions.select('label', 'prediction', 'prediction_prob').show(10)\n",
    "\n",
    "# Calcular accuracy en todo el dataset\n",
    "accuracy = df_with_predictions.filter(col('label') == col('prediction')).count() / df_with_predictions.count()\n",
    "print(f\"\\nâœ“ Accuracy en todo el dataset: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Guardar Modelo para ProducciÃ³n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Registrar modelo en MLflow Model Registry\n",
    "model_name = \"pytorch-deep-classifier\"\n",
    "\n",
    "with mlflow.start_run(run_id=run_id):\n",
    "    # Crear signature\n",
    "    from mlflow.models.signature import infer_signature\n",
    "    \n",
    "    sample_input = next(iter(test_loader))[0][:5].cpu().numpy()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        sample_output = model(torch.FloatTensor(sample_input).to(device)).cpu().numpy()\n",
    "    \n",
    "    signature = infer_signature(sample_input, sample_output)\n",
    "    \n",
    "    # Log modelo con signature\n",
    "    mlflow.pytorch.log_model(\n",
    "        model,\n",
    "        \"production_model\",\n",
    "        signature=signature,\n",
    "        registered_model_name=model_name\n",
    "    )\n",
    "\n",
    "print(f\"\\nâœ“ Modelo registrado: {model_name}\")\n",
    "print(f\"âœ“ Run ID: {run_id}\")\n",
    "print(f\"âœ“ Para cargar el modelo: mlflow.pytorch.load_model('runs:/{run_id}/production_model')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Resumen y Conclusiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"RESUMEN: SPARK + PYTORCH + MLFLOW\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nðŸŽ¯ Logros:\")\n",
    "print(\"  âœ“ Red neuronal profunda con PyTorch\")\n",
    "print(f\"  âœ“ Modelo con {model.num_parameters():,} parÃ¡metros\")\n",
    "print(\"  âœ“ Entrenamiento con early stopping y learning rate scheduling\")\n",
    "print(\"  âœ“ Tracking completo con MLflow\")\n",
    "print(\"  âœ“ Inferencia distribuida con Spark UDFs\")\n",
    "\n",
    "print(\"\\nðŸ“Š MÃ©tricas Finales:\")\n",
    "for metric, value in test_metrics.items():\n",
    "    print(f\"  â€¢ {metric}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nðŸ”— MLflow:\")\n",
    "print(f\"  â€¢ Experiment: spark-pytorch-deep-learning\")\n",
    "print(f\"  â€¢ Run ID: {run_id}\")\n",
    "print(f\"  â€¢ Model: {model_name}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Casos de Uso:\")\n",
    "print(\"  â€¢ ClasificaciÃ³n de imÃ¡genes a gran escala\")\n",
    "print(\"  â€¢ Procesamiento de NLP con transformers\")\n",
    "print(\"  â€¢ DetecciÃ³n de anomalÃ­as en tiempo real\")\n",
    "print(\"  â€¢ Sistemas de recomendaciÃ³n\")\n",
    "\n",
    "print(\"\\nðŸš€ PrÃ³ximos Pasos:\")\n",
    "print(\"  1. Implementar arquitecturas mÃ¡s complejas (ResNet, Transformers)\")\n",
    "print(\"  2. Usar Petastorm para datasets muy grandes\")\n",
    "print(\"  3. Implementar distributed training con Horovod\")\n",
    "print(\"  4. Deploy con MLflow Model Serving\")\n",
    "print(\"  5. A/B testing de modelos\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpiar archivos temporales\n",
    "import os\n",
    "if os.path.exists('best_model.pth'):\n",
    "    os.remove('best_model.pth')\n",
    "if os.path.exists('training_curves.png'):\n",
    "    os.remove('training_curves.png')\n",
    "\n",
    "# spark.stop()\n",
    "print(\"âœ“ Cleanup completado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursos Adicionales\n",
    "\n",
    "### DocumentaciÃ³n\n",
    "- [PyTorch Documentation](https://pytorch.org/docs/stable/index.html)\n",
    "- [MLflow PyTorch](https://mlflow.org/docs/latest/python_api/mlflow.pytorch.html)\n",
    "- [Petastorm](https://github.com/uber/petastorm)\n",
    "- [Horovod](https://github.com/horovod/horovod)\n",
    "\n",
    "### Ejercicios\n",
    "1. Implementar una arquitectura CNN para imÃ¡genes\n",
    "2. Usar transfer learning con modelos pre-entrenados\n",
    "3. Implementar attention mechanisms\n",
    "4. Crear un modelo de NLP con embeddings\n",
    "5. Implementar hyperparameter tuning con Optuna + MLflow"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
