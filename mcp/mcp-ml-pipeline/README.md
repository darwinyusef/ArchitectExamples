# MCP ML Pipeline con Spark, MLflow y Airflow

Servidor MCP para orquestar pipelines de Machine Learning usando Spark para procesamiento, MLflow para tracking de experimentos y Airflow para orquestaciÃ³n. Incluye monitoreo en tiempo real mediante WebSockets.

## Arquitectura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Airflow   â”‚ â† OrquestaciÃ³n de pipelines
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â†’ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚   â”‚  Spark  â”‚ â† Procesamiento de datos y entrenamiento
       â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â””â”€â†’ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚ MLflow  â”‚ â† Tracking y registro de modelos
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## CaracterÃ­sticas

- âœ… Entrenamiento de modelos con Spark MLlib
- âœ… Tracking de experimentos con MLflow
- âœ… ComparaciÃ³n y versionado de modelos
- âœ… OrquestaciÃ³n de pipelines con Airflow
- âœ… Monitoreo en tiempo real (WebSockets)
- âœ… Procesamiento distribuido de datos
- âœ… API REST completa

## InstalaciÃ³n

### 1. Instalar dependencias Python

```bash
pip install -r requirements.txt
```

### 2. Iniciar servicios con Docker

```bash
# Crear directorios necesarios
mkdir -p airflow/dags airflow/logs airflow/plugins

# Dar permisos al script de inicializaciÃ³n
chmod +x init-db.sh

# Iniciar todos los servicios
docker-compose up -d

# Ver logs
docker-compose logs -f
```

### 3. Verificar servicios

```bash
# Airflow: http://localhost:8080
# Usuario: airflow / ContraseÃ±a: airflow

# MLflow: http://localhost:5000

# Spark Master: http://localhost:8081
```

### 4. Configurar variables de entorno

```bash
cp .env.example .env
```

## Uso

### Iniciar el servidor MCP

```bash
python server.py
```

### ğŸ““ Tutorial Interactivo con Jupyter Notebook

Incluimos un **notebook completo** que te guÃ­a paso a paso:

```bash
# Instalar Jupyter
pip install jupyter notebook

# Iniciar Jupyter
jupyter notebook

# Abrir: tutorial_spark_airflow.ipynb
```

El notebook cubre:
- âœ… Spark fundamentals (DataFrames, operaciones, Window Functions)
- âœ… Procesamiento de 10K registros de ventas simulados
- âœ… Entrenamiento de modelos ML (Linear Regression, Random Forest)
- âœ… Tracking con MLflow
- âœ… OrquestaciÃ³n con Airflow
- âœ… Pipeline completo end-to-end
- âœ… Monitoreo y debugging

**Ver**: [TUTORIAL_NOTEBOOK.md](./TUTORIAL_NOTEBOOK.md) para mÃ¡s detalles.

### Herramientas disponibles

#### 1. `train_model`
Entrena un modelo con Spark y lo registra en MLflow:
```python
train_model(
    data_path="/path/to/data.csv",
    features=["feature1", "feature2", "feature3"],
    target="target_column",
    experiment_name="my_experiment",
    model_name="linear_regression"
)
```

#### 2. `process_spark_data`
Procesa datos con Spark:
```python
process_spark_data(
    input_path="/data/raw.csv",
    output_path="/data/processed.parquet",
    transformation="filter"
)
```

#### 3. `list_experiments`
Lista todos los experimentos de MLflow:
```python
list_experiments()
```

#### 4. `get_experiment_runs`
Obtiene los runs de un experimento:
```python
get_experiment_runs(
    experiment_name="my_experiment",
    max_results=10
)
```

#### 5. `compare_models`
Compara modelos basÃ¡ndose en mÃ©tricas:
```python
compare_models(
    experiment_name="my_experiment",
    metric="rmse"
)
```

#### 6. `trigger_airflow_dag`
Dispara un DAG de Airflow:
```python
trigger_airflow_dag(
    dag_id="ml_pipeline",
    conf={"param1": "value1"}
)
```

#### 7. `get_dag_status`
Obtiene el estado de un DAG:
```python
get_dag_status(
    dag_id="ml_pipeline",
    dag_run_id="manual__2024-01-01T00:00:00+00:00"
)
```

#### 8. `list_airflow_dags`
Lista todos los DAGs disponibles:
```python
list_airflow_dags()
```

#### 9. `get_pipeline_status`
Obtiene el estado en tiempo real del pipeline:
```python
get_pipeline_status()
```

## Ejemplo de uso con LLM

El LLM puede interactuar naturalmente con el pipeline:

- "Entrena un modelo de regresiÃ³n lineal con los datos en /data/sales.csv usando las columnas price, quantity y discount para predecir revenue"
- "Compara todos los modelos del experimento 'sales_prediction' usando RMSE"
- "Dispara el pipeline de ML y monitorea su progreso"
- "Â¿CuÃ¡l es el mejor modelo basado en RÂ²?"
- "Lista todos los experimentos y muÃ©strame el mÃ¡s reciente"

## DAG de ejemplo

El archivo `airflow/dags/ml_pipeline_dag.py` contiene un DAG de ejemplo que:

1. Verifica la calidad de los datos
2. Preprocesa los datos con Spark
3. Entrena el modelo
4. Registra el modelo en MLflow
5. Valida el modelo
6. Notifica la finalizaciÃ³n

## Monitoreo en tiempo real

El servidor mantiene un estado global del pipeline que se puede consultar:

```python
status = get_pipeline_status()
# Retorna:
# {
#   "current_step": "training",
#   "progress": 75,
#   "logs": [...],
#   "last_update": "2024-01-01T12:00:00"
# }
```

## Estructura de datos para entrenamiento

Ejemplo de CSV para entrenamiento:

```csv
feature1,feature2,feature3,target
10,20,30,100
15,25,35,125
20,30,40,150
```

## Comandos Ãºtiles

### Docker

```bash
# Ver estado de servicios
docker-compose ps

# Reiniciar un servicio
docker-compose restart mlflow

# Ver logs de un servicio especÃ­fico
docker-compose logs -f spark-master

# Detener todos los servicios
docker-compose down

# Detener y eliminar volÃºmenes
docker-compose down -v
```

### Airflow CLI

```bash
# Ejecutar comando en el contenedor de Airflow
docker exec -it ml-airflow-webserver bash

# Listar DAGs
docker exec ml-airflow-webserver airflow dags list

# Disparar un DAG manualmente
docker exec ml-airflow-webserver airflow dags trigger ml_pipeline

# Ver tareas de un DAG
docker exec ml-airflow-webserver airflow tasks list ml_pipeline
```

### Spark

```bash
# Acceder al Spark Master
docker exec -it ml-spark-master bash

# Ejecutar spark-submit
docker exec ml-spark-master spark-submit --master spark://spark-master:7077 /path/to/script.py
```

## Troubleshooting

### Airflow no inicia
```bash
# Verificar base de datos
docker-compose logs postgres

# Reinicializar base de datos de Airflow
docker exec ml-airflow-webserver airflow db reset
```

### MLflow no registra modelos
```bash
# Verificar conexiÃ³n a PostgreSQL
docker exec ml-mlflow curl -f http://localhost:5000/health

# Revisar logs
docker-compose logs mlflow
```

### Spark no procesa datos
```bash
# Verificar conectividad master-worker
docker-compose logs spark-worker

# Reiniciar cluster de Spark
docker-compose restart spark-master spark-worker
```

## Seguridad

âš ï¸ **IMPORTANTE**: Esta configuraciÃ³n es para desarrollo. Para producciÃ³n:

- Cambiar credenciales por defecto
- Configurar autenticaciÃ³n robusta
- Usar HTTPS/TLS
- Implementar control de acceso basado en roles
- Aislar redes de Docker
- Configurar lÃ­mites de recursos

## Extensiones

### Agregar mÃ¡s transformaciones de Spark

Edita la funciÃ³n `process_spark_data` en `server.py` para agregar transformaciones personalizadas.

### Crear nuevos DAGs

Agrega archivos Python en `airflow/dags/` siguiendo el patrÃ³n de `ml_pipeline_dag.py`.

### Integrar otros frameworks ML

Puedes agregar soporte para TensorFlow, PyTorch, XGBoost, etc. modificando la funciÃ³n `train_model`.

## Referencias

- [Apache Spark](https://spark.apache.org/)
- [MLflow](https://mlflow.org/)
- [Apache Airflow](https://airflow.apache.org/)
- [FastMCP](https://github.com/jlowin/fastmcp)
