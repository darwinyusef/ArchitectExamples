{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15 - MLflow + LangGraph: Tracking de Workflows de AI\n",
    "\n",
    "## üéØ Objetivos\n",
    "- Integrar MLflow con LangGraph para tracking de flujos LLM\n",
    "- Crear workflows de AI complejos con grafos de estados\n",
    "- Trackear prompts, respuestas y m√©tricas\n",
    "- Comparar diferentes configuraciones de LLM\n",
    "- Versionado de prompts y chains\n",
    "- Evaluaci√≥n de calidad de respuestas\n",
    "\n",
    "## üìö Tecnolog√≠as\n",
    "- **MLflow**: Experiment tracking y model registry\n",
    "- **LangGraph**: Framework para workflows de LLMs\n",
    "- **LangChain**: Herramientas para LLMs\n",
    "- **OpenAI/Anthropic**: APIs de LLMs (opcional)\n",
    "\n",
    "## ‚≠ê Complejidad: Avanzado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Instalaci√≥n y Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar dependencias\n",
    "!pip install mlflow langgraph langchain langchain-core langchain-community langchain-openai pandas numpy matplotlib -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.langchain\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# LangGraph y LangChain\n",
    "from langgraph.graph import Graph, StateGraph, END\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from typing import TypedDict, Annotated, Sequence\n",
    "import operator\n",
    "\n",
    "print(f\"‚úÖ MLflow version: {mlflow.__version__}\")\n",
    "print(f\"‚úÖ Imports completados\")\n",
    "print(f\"\\nüí° Nota: Este notebook usa LLMs simulados para demo\")\n",
    "print(f\"   Para usar LLMs reales, configura API keys de OpenAI o Anthropic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configurar MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar MLflow\n",
    "mlflow.set_tracking_uri(\"./mlruns\")\n",
    "experiment_name = \"langgraph_workflows\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "print(f\"‚úÖ MLflow configurado\")\n",
    "print(f\"üìä Experimento: {experiment_name}\")\n",
    "print(f\"üìÅ Tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "print(f\"\\nüí° Para ver la UI ejecuta: mlflow ui --port 5000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LLM Simulado para Demo\n",
    "\n",
    "Usaremos un LLM simulado para demostraci√≥n. En producci√≥n, usa OpenAI, Anthropic, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MockLLM:\n",
    "    \"\"\"\n",
    "    LLM simulado para prop√≥sitos de demostraci√≥n.\n",
    "    En producci√≥n, reemplaza con:\n",
    "    - ChatOpenAI de langchain_openai\n",
    "    - ChatAnthropic de langchain_anthropic\n",
    "    - Otros providers\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"mock-gpt-4\", temperature=0.7):\n",
    "        self.model_name = model_name\n",
    "        self.temperature = temperature\n",
    "        self.call_count = 0\n",
    "        \n",
    "    def invoke(self, messages):\n",
    "        \"\"\"Simula una llamada a LLM\"\"\"\n",
    "        self.call_count += 1\n",
    "        \n",
    "        # Simular diferentes respuestas seg√∫n el contexto\n",
    "        last_message = messages[-1] if isinstance(messages, list) else str(messages)\n",
    "        \n",
    "        if isinstance(last_message, HumanMessage):\n",
    "            content = last_message.content\n",
    "        else:\n",
    "            content = str(last_message)\n",
    "        \n",
    "        # Respuestas simuladas\n",
    "        if \"resume\" in content.lower() or \"summarize\" in content.lower():\n",
    "            response = \"Resumen: El texto habla sobre machine learning y sus aplicaciones en la industria moderna.\"\n",
    "        elif \"translate\" in content.lower() or \"traduce\" in content.lower():\n",
    "            response = \"Translation: This is a simulated translation of the input text.\"\n",
    "        elif \"analiza\" in content.lower() or \"analyze\" in content.lower():\n",
    "            response = \"An√°lisis: Los datos muestran una tendencia positiva con crecimiento del 15% anual.\"\n",
    "        elif \"classify\" in content.lower() or \"clasifica\" in content.lower():\n",
    "            response = '{\"category\": \"Technology\", \"sentiment\": \"positive\", \"confidence\": 0.85}'\n",
    "        else:\n",
    "            response = f\"Respuesta simulada para: {content[:50]}... (llamada #{self.call_count})\"\n",
    "        \n",
    "        return AIMessage(content=response)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"MockLLM(model={self.model_name}, temp={self.temperature})\"\n",
    "\n",
    "# Crear instancia\n",
    "llm = MockLLM(model_name=\"mock-gpt-4\", temperature=0.7)\n",
    "print(f\"‚úÖ LLM simulado creado: {llm}\")\n",
    "print(f\"\\nüí° Para usar LLM real:\")\n",
    "print(f\"   from langchain_openai import ChatOpenAI\")\n",
    "print(f\"   llm = ChatOpenAI(model='gpt-4', temperature=0.7)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Workflow Simple con LangGraph y MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir estado del grafo\n",
    "class SimpleState(TypedDict):\n",
    "    messages: Sequence[HumanMessage | AIMessage]\n",
    "    current_step: str\n",
    "    metadata: dict\n",
    "\n",
    "def create_simple_workflow():\n",
    "    \"\"\"\n",
    "    Crea un workflow simple de procesamiento de texto\n",
    "    \"\"\"\n",
    "    \n",
    "    # Nodos del grafo\n",
    "    def analyze_input(state: SimpleState):\n",
    "        \"\"\"Analiza el input del usuario\"\"\"\n",
    "        print(\"üîç Analizando input...\")\n",
    "        messages = state[\"messages\"]\n",
    "        last_message = messages[-1]\n",
    "        \n",
    "        # Simular an√°lisis\n",
    "        word_count = len(last_message.content.split())\n",
    "        \n",
    "        return {\n",
    "            \"messages\": messages,\n",
    "            \"current_step\": \"analyzed\",\n",
    "            \"metadata\": {\"word_count\": word_count, \"language\": \"es\"}\n",
    "        }\n",
    "    \n",
    "    def process_with_llm(state: SimpleState):\n",
    "        \"\"\"Procesa con LLM\"\"\"\n",
    "        print(\"ü§ñ Procesando con LLM...\")\n",
    "        messages = state[\"messages\"]\n",
    "        \n",
    "        # Llamar al LLM\n",
    "        response = llm.invoke(messages)\n",
    "        messages.append(response)\n",
    "        \n",
    "        return {\n",
    "            \"messages\": messages,\n",
    "            \"current_step\": \"processed\",\n",
    "            \"metadata\": state[\"metadata\"]\n",
    "        }\n",
    "    \n",
    "    def format_output(state: SimpleState):\n",
    "        \"\"\"Formatea la salida\"\"\"\n",
    "        print(\"üìù Formateando output...\")\n",
    "        messages = state[\"messages\"]\n",
    "        last_response = messages[-1].content\n",
    "        \n",
    "        formatted = f\"**Respuesta del Asistente:**\\n{last_response}\"\n",
    "        messages.append(AIMessage(content=formatted))\n",
    "        \n",
    "        return {\n",
    "            \"messages\": messages,\n",
    "            \"current_step\": \"completed\",\n",
    "            \"metadata\": state[\"metadata\"]\n",
    "        }\n",
    "    \n",
    "    # Crear grafo\n",
    "    workflow = StateGraph(SimpleState)\n",
    "    \n",
    "    # Agregar nodos\n",
    "    workflow.add_node(\"analyze\", analyze_input)\n",
    "    workflow.add_node(\"process\", process_with_llm)\n",
    "    workflow.add_node(\"format\", format_output)\n",
    "    \n",
    "    # Definir flujo\n",
    "    workflow.set_entry_point(\"analyze\")\n",
    "    workflow.add_edge(\"analyze\", \"process\")\n",
    "    workflow.add_edge(\"process\", \"format\")\n",
    "    workflow.add_edge(\"format\", END)\n",
    "    \n",
    "    return workflow.compile()\n",
    "\n",
    "# Crear workflow\n",
    "simple_workflow = create_simple_workflow()\n",
    "print(\"‚úÖ Workflow simple creado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Ejecutar y Trackear Workflow con MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_and_track_workflow(workflow, user_input, run_name):\n",
    "    \"\"\"\n",
    "    Ejecuta workflow y trackea con MLflow\n",
    "    \"\"\"\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        \n",
    "        # Log par√°metros\n",
    "        mlflow.log_param(\"workflow_type\", \"simple\")\n",
    "        mlflow.log_param(\"llm_model\", llm.model_name)\n",
    "        mlflow.log_param(\"llm_temperature\", llm.temperature)\n",
    "        mlflow.log_param(\"user_input\", user_input)\n",
    "        \n",
    "        # Estado inicial\n",
    "        initial_state = {\n",
    "            \"messages\": [HumanMessage(content=user_input)],\n",
    "            \"current_step\": \"start\",\n",
    "            \"metadata\": {}\n",
    "        }\n",
    "        \n",
    "        # Ejecutar workflow\n",
    "        start_time = datetime.now()\n",
    "        result = workflow.invoke(initial_state)\n",
    "        end_time = datetime.now()\n",
    "        \n",
    "        execution_time = (end_time - start_time).total_seconds()\n",
    "        \n",
    "        # Log m√©tricas\n",
    "        mlflow.log_metric(\"execution_time_seconds\", execution_time)\n",
    "        mlflow.log_metric(\"total_messages\", len(result[\"messages\"]))\n",
    "        mlflow.log_metric(\"word_count\", result[\"metadata\"].get(\"word_count\", 0))\n",
    "        mlflow.log_metric(\"llm_calls\", llm.call_count)\n",
    "        \n",
    "        # Guardar conversaci√≥n\n",
    "        conversation = []\n",
    "        for msg in result[\"messages\"]:\n",
    "            conversation.append({\n",
    "                \"type\": type(msg).__name__,\n",
    "                \"content\": msg.content\n",
    "            })\n",
    "        \n",
    "        with open('conversation.json', 'w') as f:\n",
    "            json.dump(conversation, f, indent=2)\n",
    "        mlflow.log_artifact('conversation.json')\n",
    "        \n",
    "        # Log metadata\n",
    "        mlflow.log_dict(result[\"metadata\"], \"metadata.json\")\n",
    "        \n",
    "        # Tags\n",
    "        mlflow.set_tag(\"workflow_status\", result[\"current_step\"])\n",
    "        mlflow.set_tag(\"language\", result[\"metadata\"].get(\"language\", \"unknown\"))\n",
    "        \n",
    "        print(f\"\\n‚úÖ Workflow completado en {execution_time:.2f}s\")\n",
    "        print(f\"üìä Total mensajes: {len(result['messages'])}\")\n",
    "        print(f\"üìä Llamadas LLM: {llm.call_count}\")\n",
    "        print(f\"\\nüìù Respuesta final:\")\n",
    "        print(result[\"messages\"][-1].content)\n",
    "        \n",
    "        return result\n",
    "\n",
    "# Ejecutar m√∫ltiples variaciones\n",
    "test_inputs = [\n",
    "    \"Resume este documento sobre machine learning y sus aplicaciones.\",\n",
    "    \"Analiza las tendencias de ventas del √∫ltimo trimestre.\",\n",
    "    \"Clasifica el siguiente texto en categor√≠as: Tecnolog√≠a innovadora para el futuro.\"\n",
    "]\n",
    "\n",
    "results = []\n",
    "for i, user_input in enumerate(test_inputs, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Ejecutando workflow {i}/{len(test_inputs)}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Reset LLM call count\n",
    "    llm.call_count = 0\n",
    "    \n",
    "    result = run_and_track_workflow(\n",
    "        simple_workflow,\n",
    "        user_input,\n",
    "        f\"simple_workflow_test_{i}\"\n",
    "    )\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Workflow Avanzado: Multi-Agente con Decisiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estado para workflow multi-agente\n",
    "class AgentState(TypedDict):\n",
    "    messages: Sequence[HumanMessage | AIMessage]\n",
    "    next_agent: str\n",
    "    task_type: str\n",
    "    results: dict\n",
    "\n",
    "def create_multi_agent_workflow():\n",
    "    \"\"\"\n",
    "    Workflow con m√∫ltiples agentes especializados\n",
    "    \"\"\"\n",
    "    \n",
    "    def router(state: AgentState):\n",
    "        \"\"\"Decide qu√© agente debe procesar la tarea\"\"\"\n",
    "        print(\"üîÄ Router: Analizando tipo de tarea...\")\n",
    "        \n",
    "        last_message = state[\"messages\"][-1].content.lower()\n",
    "        \n",
    "        if \"resume\" in last_message or \"summarize\" in last_message:\n",
    "            task_type = \"summarization\"\n",
    "            next_agent = \"summarizer\"\n",
    "        elif \"translate\" in last_message or \"traduce\" in last_message:\n",
    "            task_type = \"translation\"\n",
    "            next_agent = \"translator\"\n",
    "        elif \"analiza\" in last_message or \"analyze\" in last_message:\n",
    "            task_type = \"analysis\"\n",
    "            next_agent = \"analyzer\"\n",
    "        else:\n",
    "            task_type = \"general\"\n",
    "            next_agent = \"general_agent\"\n",
    "        \n",
    "        print(f\"   ‚Üí Tipo de tarea: {task_type}\")\n",
    "        print(f\"   ‚Üí Agente asignado: {next_agent}\")\n",
    "        \n",
    "        return {\n",
    "            \"messages\": state[\"messages\"],\n",
    "            \"next_agent\": next_agent,\n",
    "            \"task_type\": task_type,\n",
    "            \"results\": {}\n",
    "        }\n",
    "    \n",
    "    def summarizer_agent(state: AgentState):\n",
    "        \"\"\"Agente especializado en res√∫menes\"\"\"\n",
    "        print(\"üìù Summarizer Agent trabajando...\")\n",
    "        \n",
    "        prompt = f\"Resume el siguiente texto de manera concisa: {state['messages'][-1].content}\"\n",
    "        response = llm.invoke([HumanMessage(content=prompt)])\n",
    "        \n",
    "        state[\"messages\"].append(response)\n",
    "        state[\"results\"][\"summary\"] = response.content\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def analyzer_agent(state: AgentState):\n",
    "        \"\"\"Agente especializado en an√°lisis\"\"\"\n",
    "        print(\"üìä Analyzer Agent trabajando...\")\n",
    "        \n",
    "        prompt = f\"Analiza el siguiente contenido: {state['messages'][-1].content}\"\n",
    "        response = llm.invoke([HumanMessage(content=prompt)])\n",
    "        \n",
    "        state[\"messages\"].append(response)\n",
    "        state[\"results\"][\"analysis\"] = response.content\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def translator_agent(state: AgentState):\n",
    "        \"\"\"Agente especializado en traducci√≥n\"\"\"\n",
    "        print(\"üåç Translator Agent trabajando...\")\n",
    "        \n",
    "        prompt = f\"Traduce el siguiente texto: {state['messages'][-1].content}\"\n",
    "        response = llm.invoke([HumanMessage(content=prompt)])\n",
    "        \n",
    "        state[\"messages\"].append(response)\n",
    "        state[\"results\"][\"translation\"] = response.content\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def general_agent(state: AgentState):\n",
    "        \"\"\"Agente general\"\"\"\n",
    "        print(\"ü§ñ General Agent trabajando...\")\n",
    "        \n",
    "        response = llm.invoke(state[\"messages\"])\n",
    "        state[\"messages\"].append(response)\n",
    "        state[\"results\"][\"response\"] = response.content\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def should_continue(state: AgentState) -> str:\n",
    "        \"\"\"Decide si continuar o terminar\"\"\"\n",
    "        return state[\"next_agent\"]\n",
    "    \n",
    "    # Crear grafo\n",
    "    workflow = StateGraph(AgentState)\n",
    "    \n",
    "    # Agregar nodos\n",
    "    workflow.add_node(\"router\", router)\n",
    "    workflow.add_node(\"summarizer\", summarizer_agent)\n",
    "    workflow.add_node(\"analyzer\", analyzer_agent)\n",
    "    workflow.add_node(\"translator\", translator_agent)\n",
    "    workflow.add_node(\"general_agent\", general_agent)\n",
    "    \n",
    "    # Definir flujo con decisiones\n",
    "    workflow.set_entry_point(\"router\")\n",
    "    \n",
    "    workflow.add_conditional_edges(\n",
    "        \"router\",\n",
    "        should_continue,\n",
    "        {\n",
    "            \"summarizer\": \"summarizer\",\n",
    "            \"analyzer\": \"analyzer\",\n",
    "            \"translator\": \"translator\",\n",
    "            \"general_agent\": \"general_agent\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Todos los agentes terminan\n",
    "    workflow.add_edge(\"summarizer\", END)\n",
    "    workflow.add_edge(\"analyzer\", END)\n",
    "    workflow.add_edge(\"translator\", END)\n",
    "    workflow.add_edge(\"general_agent\", END)\n",
    "    \n",
    "    return workflow.compile()\n",
    "\n",
    "# Crear workflow multi-agente\n",
    "multi_agent_workflow = create_multi_agent_workflow()\n",
    "print(\"‚úÖ Workflow multi-agente creado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Ejecutar Workflow Multi-Agente con MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_multi_agent_workflow(workflow, user_input, run_name):\n",
    "    \"\"\"\n",
    "    Ejecuta workflow multi-agente y trackea con MLflow\n",
    "    \"\"\"\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        \n",
    "        # Log par√°metros\n",
    "        mlflow.log_param(\"workflow_type\", \"multi_agent\")\n",
    "        mlflow.log_param(\"llm_model\", llm.model_name)\n",
    "        mlflow.log_param(\"user_input\", user_input)\n",
    "        \n",
    "        # Estado inicial\n",
    "        initial_state = {\n",
    "            \"messages\": [HumanMessage(content=user_input)],\n",
    "            \"next_agent\": \"\",\n",
    "            \"task_type\": \"\",\n",
    "            \"results\": {}\n",
    "        }\n",
    "        \n",
    "        # Ejecutar\n",
    "        llm.call_count = 0\n",
    "        start_time = datetime.now()\n",
    "        result = workflow.invoke(initial_state)\n",
    "        end_time = datetime.now()\n",
    "        \n",
    "        execution_time = (end_time - start_time).total_seconds()\n",
    "        \n",
    "        # Log m√©tricas\n",
    "        mlflow.log_metric(\"execution_time_seconds\", execution_time)\n",
    "        mlflow.log_metric(\"total_messages\", len(result[\"messages\"]))\n",
    "        mlflow.log_metric(\"llm_calls\", llm.call_count)\n",
    "        \n",
    "        # Log par√°metros de routing\n",
    "        mlflow.log_param(\"task_type\", result[\"task_type\"])\n",
    "        mlflow.log_param(\"selected_agent\", result[\"next_agent\"])\n",
    "        \n",
    "        # Guardar resultados\n",
    "        with open('agent_results.json', 'w') as f:\n",
    "            json.dump(result[\"results\"], f, indent=2)\n",
    "        mlflow.log_artifact('agent_results.json')\n",
    "        \n",
    "        # Guardar conversaci√≥n completa\n",
    "        conversation = []\n",
    "        for msg in result[\"messages\"]:\n",
    "            conversation.append({\n",
    "                \"type\": type(msg).__name__,\n",
    "                \"content\": msg.content\n",
    "            })\n",
    "        \n",
    "        with open('multi_agent_conversation.json', 'w') as f:\n",
    "            json.dump(conversation, f, indent=2)\n",
    "        mlflow.log_artifact('multi_agent_conversation.json')\n",
    "        \n",
    "        # Tags\n",
    "        mlflow.set_tag(\"agent_used\", result[\"next_agent\"])\n",
    "        mlflow.set_tag(\"task_type\", result[\"task_type\"])\n",
    "        \n",
    "        print(f\"\\n‚úÖ Workflow multi-agente completado en {execution_time:.2f}s\")\n",
    "        print(f\"üìä Agente usado: {result['next_agent']}\")\n",
    "        print(f\"üìä Tipo de tarea: {result['task_type']}\")\n",
    "        print(f\"üìä Llamadas LLM: {llm.call_count}\")\n",
    "        print(f\"\\nüìù Resultado:\")\n",
    "        print(result[\"messages\"][-1].content)\n",
    "        \n",
    "        return result\n",
    "\n",
    "# Ejecutar con diferentes tipos de tareas\n",
    "test_tasks = [\n",
    "    \"Resume este art√≠culo sobre inteligencia artificial en medicina.\",\n",
    "    \"Analiza las m√©tricas de rendimiento del √∫ltimo sprint.\",\n",
    "    \"Traduce este texto al ingl√©s: Hola, ¬øc√≥mo est√°s?\",\n",
    "    \"¬øCu√°l es la capital de Francia?\"  # General\n",
    "]\n",
    "\n",
    "multi_agent_results = []\n",
    "for i, task in enumerate(test_tasks, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Ejecutando tarea {i}/{len(test_tasks)}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    result = run_multi_agent_workflow(\n",
    "        multi_agent_workflow,\n",
    "        task,\n",
    "        f\"multi_agent_task_{i}\"\n",
    "    )\n",
    "    multi_agent_results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. An√°lisis de Experimentos con MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "experiment = client.get_experiment_by_name(experiment_name)\n",
    "\n",
    "# Obtener todos los runs\n",
    "runs = client.search_runs(\n",
    "    experiment_ids=[experiment.experiment_id],\n",
    "    order_by=[\"start_time DESC\"]\n",
    ")\n",
    "\n",
    "print(f\"üìä Total de runs: {len(runs)}\\n\")\n",
    "\n",
    "# Analizar m√©tricas\n",
    "runs_data = []\n",
    "for run in runs:\n",
    "    runs_data.append({\n",
    "        'run_name': run.info.run_name,\n",
    "        'workflow_type': run.data.params.get('workflow_type', 'unknown'),\n",
    "        'task_type': run.data.params.get('task_type', 'N/A'),\n",
    "        'selected_agent': run.data.params.get('selected_agent', 'N/A'),\n",
    "        'execution_time': run.data.metrics.get('execution_time_seconds', 0),\n",
    "        'llm_calls': run.data.metrics.get('llm_calls', 0),\n",
    "        'total_messages': run.data.metrics.get('total_messages', 0)\n",
    "    })\n",
    "\n",
    "runs_df = pd.DataFrame(runs_data)\n",
    "print(\"üìä Resumen de Experimentos:\")\n",
    "print(runs_df.to_string(index=False))\n",
    "\n",
    "# Estad√≠sticas por workflow type\n",
    "print(\"\\nüìä Estad√≠sticas por Tipo de Workflow:\")\n",
    "stats = runs_df.groupby('workflow_type').agg({\n",
    "    'execution_time': ['mean', 'min', 'max'],\n",
    "    'llm_calls': ['mean', 'sum'],\n",
    "    'run_name': 'count'\n",
    "}).round(3)\n",
    "print(stats)\n",
    "\n",
    "# An√°lisis de agentes\n",
    "multi_agent_runs = runs_df[runs_df['workflow_type'] == 'multi_agent']\n",
    "if len(multi_agent_runs) > 0:\n",
    "    print(\"\\nüìä Uso de Agentes:\")\n",
    "    print(multi_agent_runs['selected_agent'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualizaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configurar estilo\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Gr√°fico 1: Tiempo de ejecuci√≥n por workflow type\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Execution time\n",
    "runs_df.boxplot(column='execution_time', by='workflow_type', ax=axes[0])\n",
    "axes[0].set_title('Tiempo de Ejecuci√≥n por Tipo de Workflow')\n",
    "axes[0].set_xlabel('Tipo de Workflow')\n",
    "axes[0].set_ylabel('Tiempo (segundos)')\n",
    "plt.sca(axes[0])\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# LLM calls\n",
    "runs_df.boxplot(column='llm_calls', by='workflow_type', ax=axes[1])\n",
    "axes[1].set_title('Llamadas LLM por Tipo de Workflow')\n",
    "axes[1].set_xlabel('Tipo de Workflow')\n",
    "axes[1].set_ylabel('N√∫mero de Llamadas')\n",
    "plt.sca(axes[1])\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('workflow_metrics.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Gr√°fico 2: Distribuci√≥n de agentes (si hay runs multi-agent)\n",
    "if len(multi_agent_runs) > 0:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    agent_counts = multi_agent_runs['selected_agent'].value_counts()\n",
    "    colors = sns.color_palette('viridis', len(agent_counts))\n",
    "    agent_counts.plot(kind='bar', color=colors)\n",
    "    plt.title('Distribuci√≥n de Uso de Agentes', fontsize=14)\n",
    "    plt.xlabel('Agente')\n",
    "    plt.ylabel('N√∫mero de Veces Usado')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('agent_distribution.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "print(\"‚úÖ Visualizaciones guardadas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Comparaci√≥n de Prompts (A/B Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_prompt_variations():\n",
    "    \"\"\"\n",
    "    Compara diferentes variaciones de prompts\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt_versions = {\n",
    "        \"v1_simple\": \"Resume el siguiente texto: {text}\",\n",
    "        \"v2_detailed\": \"Por favor, proporciona un resumen conciso y detallado del siguiente texto, destacando los puntos clave: {text}\",\n",
    "        \"v3_bullet\": \"Resume el siguiente texto en formato de bullet points: {text}\"\n",
    "    }\n",
    "    \n",
    "    test_text = \"Machine Learning es una rama de la inteligencia artificial que permite a las computadoras aprender sin ser expl√≠citamente programadas. Utiliza algoritmos para encontrar patrones en datos.\"\n",
    "    \n",
    "    for version_name, prompt_template in prompt_versions.items():\n",
    "        with mlflow.start_run(run_name=f\"prompt_test_{version_name}\"):\n",
    "            \n",
    "            # Log prompt version\n",
    "            mlflow.log_param(\"prompt_version\", version_name)\n",
    "            mlflow.log_param(\"prompt_template\", prompt_template)\n",
    "            mlflow.log_param(\"test_text\", test_text)\n",
    "            \n",
    "            # Ejecutar\n",
    "            prompt = prompt_template.format(text=test_text)\n",
    "            llm.call_count = 0\n",
    "            \n",
    "            start_time = datetime.now()\n",
    "            response = llm.invoke([HumanMessage(content=prompt)])\n",
    "            end_time = datetime.now()\n",
    "            \n",
    "            execution_time = (end_time - start_time).total_seconds()\n",
    "            \n",
    "            # Log m√©tricas\n",
    "            mlflow.log_metric(\"execution_time_seconds\", execution_time)\n",
    "            mlflow.log_metric(\"response_length\", len(response.content))\n",
    "            mlflow.log_metric(\"llm_calls\", llm.call_count)\n",
    "            \n",
    "            # Guardar respuesta\n",
    "            with open(f'response_{version_name}.txt', 'w') as f:\n",
    "                f.write(response.content)\n",
    "            mlflow.log_artifact(f'response_{version_name}.txt')\n",
    "            \n",
    "            # Tags\n",
    "            mlflow.set_tag(\"test_type\", \"prompt_comparison\")\n",
    "            mlflow.set_tag(\"prompt_version\", version_name)\n",
    "            \n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Prompt Version: {version_name}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            print(f\"Prompt: {prompt}\")\n",
    "            print(f\"\\nRespuesta: {response.content}\")\n",
    "            print(f\"Tiempo: {execution_time:.3f}s\")\n",
    "\n",
    "test_prompt_variations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Best Practices y Recomendaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üí° MEJORES PR√ÅCTICAS: MLFLOW + LANGGRAPH\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n1Ô∏è‚É£ TRACKING:\")\n",
    "print(\"   ‚úÖ Loggea todos los par√°metros del LLM (model, temperature, etc.)\")\n",
    "print(\"   ‚úÖ Trackea tiempo de ejecuci√≥n y n√∫mero de llamadas\")\n",
    "print(\"   ‚úÖ Guarda prompts y respuestas como artefactos\")\n",
    "print(\"   ‚úÖ Usa tags para categorizar experimentos\")\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£ VERSIONADO:\")\n",
    "print(\"   ‚úÖ Versiona tus prompts y gu√°rdalos en MLflow\")\n",
    "print(\"   ‚úÖ Usa el Model Registry para producci√≥n\")\n",
    "print(\"   ‚úÖ Mant√©n historial de cambios en workflows\")\n",
    "\n",
    "print(\"\\n3Ô∏è‚É£ EVALUACI√ìN:\")\n",
    "print(\"   ‚úÖ Define m√©tricas claras (latencia, calidad, costo)\")\n",
    "print(\"   ‚úÖ Compara diferentes modelos y configuraciones\")\n",
    "print(\"   ‚úÖ A/B testing de prompts\")\n",
    "\n",
    "print(\"\\n4Ô∏è‚É£ LANGGRAPH:\")\n",
    "print(\"   ‚úÖ Dise√±a workflows con estados claros\")\n",
    "print(\"   ‚úÖ Usa agentes especializados para tareas espec√≠ficas\")\n",
    "print(\"   ‚úÖ Implementa routing inteligente\")\n",
    "print(\"   ‚úÖ Maneja errores y reintentos\")\n",
    "\n",
    "print(\"\\n5Ô∏è‚É£ PRODUCCI√ìN:\")\n",
    "print(\"   ‚úÖ Monitorea costos de API\")\n",
    "print(\"   ‚úÖ Implementa caching para respuestas comunes\")\n",
    "print(\"   ‚úÖ Usa rate limiting\")\n",
    "print(\"   ‚úÖ Loggea errores y excepciones\")\n",
    "\n",
    "print(\"\\n6Ô∏è‚É£ SEGURIDAD:\")\n",
    "print(\"   ‚úÖ NUNCA loggees API keys\")\n",
    "print(\"   ‚úÖ Sanitiza inputs de usuario\")\n",
    "print(\"   ‚úÖ Implementa validaci√≥n de outputs\")\n",
    "print(\"   ‚úÖ Usa variables de entorno para secrets\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Resumen y Pr√≥ximos Pasos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estad√≠sticas finales\n",
    "all_runs = client.search_runs(\n",
    "    experiment_ids=[experiment.experiment_id]\n",
    ")\n",
    "\n",
    "print(\"üéâ RESUMEN DEL TUTORIAL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìä Experimento: {experiment_name}\")\n",
    "print(f\"üìä Total de runs: {len(all_runs)}\")\n",
    "\n",
    "# Contar por tipo\n",
    "workflow_types = {}\n",
    "for run in all_runs:\n",
    "    wf_type = run.data.params.get('workflow_type', 'unknown')\n",
    "    workflow_types[wf_type] = workflow_types.get(wf_type, 0) + 1\n",
    "\n",
    "print(f\"\\nüìä Runs por tipo de workflow:\")\n",
    "for wf_type, count in workflow_types.items():\n",
    "    print(f\"   {wf_type}: {count}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Conceptos aprendidos:\")\n",
    "print(f\"   - Integraci√≥n MLflow + LangGraph\")\n",
    "print(f\"   - Workflows simples y multi-agente\")\n",
    "print(f\"   - Tracking de prompts y respuestas\")\n",
    "print(f\"   - Comparaci√≥n de configuraciones\")\n",
    "print(f\"   - A/B testing de prompts\")\n",
    "print(f\"   - M√©tricas y evaluaci√≥n\")\n",
    "\n",
    "print(f\"\\nüöÄ Pr√≥ximos pasos:\")\n",
    "print(f\"   - Integrar con LLMs reales (OpenAI, Anthropic, etc.)\")\n",
    "print(f\"   - Implementar RAG (Retrieval-Augmented Generation)\")\n",
    "print(f\"   - Agregar evaluaci√≥n autom√°tica de calidad\")\n",
    "print(f\"   - Deployar workflows en producci√≥n\")\n",
    "print(f\"   - Implementar human-in-the-loop\")\n",
    "print(f\"   - Monitoreo y alertas\")\n",
    "\n",
    "print(f\"\\nüíª Ver resultados: mlflow ui --port 5000\")\n",
    "print(f\"\\nüìÅ Artefactos guardados:\")\n",
    "print(f\"   - Conversaciones JSON\")\n",
    "print(f\"   - Resultados de agentes\")\n",
    "print(f\"   - Visualizaciones\")\n",
    "print(f\"   - Respuestas de prompts\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ Tutorial completado exitosamente!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
