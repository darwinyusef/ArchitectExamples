# MCP RAG con LangGraph y OpenAI

Servidor MCP para sistema RAG (Retrieval Augmented Generation) usando **LangGraph** para orquestar el flujo de consultas con grafos de estado. Utiliza **OpenAI** para embeddings y generaci√≥n de respuestas.

## Caracter√≠sticas

- ‚úÖ RAG con arquitectura de grafos (LangGraph)
- ‚úÖ Embeddings con OpenAI (text-embedding-3-small)
- ‚úÖ LLM con GPT-4 Turbo
- ‚úÖ Vector store con ChromaDB
- ‚úÖ Razonamiento multi-paso
- ‚úÖ Soporte para historial de chat
- ‚úÖ Soporte para m√∫ltiples formatos (TXT, PDF, Markdown)
- ‚úÖ B√∫squeda sem√°ntica avanzada
- ‚úÖ Consultas con fuentes citadas

## Arquitectura con LangGraph

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  LangGraph Workflow               ‚îÇ
‚îÇ                                                   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ Retrieve ‚îÇ‚îÄ‚îÄ‚îÄ‚Üí‚îÇ  Prepare  ‚îÇ‚îÄ‚îÄ‚îÄ‚Üí‚îÇ Generate  ‚îÇ ‚îÇ
‚îÇ  ‚îÇDocuments ‚îÇ    ‚îÇ  Context  ‚îÇ    ‚îÇ  Answer   ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                                   ‚îÇ
‚îÇ  State: { question, documents, context, answer } ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ                           ‚îÇ
           ‚ñº                           ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  ChromaDB   ‚îÇ            ‚îÇ   OpenAI   ‚îÇ
    ‚îÇ(Vectorstore)‚îÇ            ‚îÇ   GPT-4    ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### ¬øPor qu√© LangGraph?

LangGraph permite crear flujos complejos mediante grafos de estado:
- **Nodos**: Funciones que transforman el estado
- **Edges**: Conexiones entre nodos
- **Estado**: Diccionario compartido entre nodos
- **Condicionales**: Rutas din√°micas basadas en el estado

## Instalaci√≥n

### 1. Instalar dependencias Python

```bash
pip install -r requirements.txt
```

### 2. Configurar OpenAI API Key

```bash
cp .env.example .env
# Editar .env y agregar tu API key de OpenAI
```

`.env`:
```bash
OPENAI_API_KEY=sk-tu-api-key-aqui
OPENAI_MODEL=gpt-4-turbo-preview
EMBEDDING_MODEL=text-embedding-3-small
```

### 3. (Opcional) Iniciar PostgreSQL

Si quieres almacenar metadata adicional:
```bash
docker-compose up -d
```

## Uso

### Iniciar el servidor MCP

```bash
python server.py
```

### Herramientas disponibles

#### 1. `load_documents` - Cargar documentos
```python
load_documents(
    directory_path="./documents",
    file_types=["txt", "pdf", "md"],
    chunk_size=1000,
    chunk_overlap=200
)
```

**Respuesta**:
```json
{
  "success": true,
  "documents_loaded": 10,
  "chunks_created": 45,
  "directory": "./documents",
  "file_types": ["txt", "pdf", "md"]
}
```

#### 2. `add_text_document` - Agregar texto directo
```python
add_text_document(
    text="LangGraph es un framework...",
    metadata={"source": "manual", "category": "documentation"},
    chunk_size=1000
)
```

#### 3. `query_rag` - Consultar con LangGraph ‚≠ê
```python
query_rag(
    question="¬øQu√© es LangGraph y c√≥mo funciona?",
    num_sources=3,
    return_sources=True,
    chat_history=[
        {"role": "user", "content": "Hola"},
        {"role": "assistant", "content": "¬°Hola! ¬øEn qu√© puedo ayudarte?"}
    ]
)
```

**Flujo interno del grafo**:
1. **Nodo Retrieve**: Busca 3 documentos similares en ChromaDB
2. **Nodo Prepare Context**: Formatea los documentos como contexto
3. **Nodo Generate**: GPT-4 genera respuesta basada en contexto

**Respuesta**:
```json
{
  "success": true,
  "question": "¬øQu√© es LangGraph?",
  "answer": "LangGraph es un framework para crear aplicaciones con grafos de estado...",
  "sources": [
    {
      "content": "LangGraph permite...",
      "metadata": {"source": "example.txt"},
      "source": "example.txt"
    }
  ],
  "num_sources": 3
}
```

#### 4. `multi_step_reasoning` - Razonamiento multi-paso ‚≠ê‚≠ê

Para preguntas complejas que requieren m√∫ltiples pasos:

```python
multi_step_reasoning(
    question="Compara las ventajas de LangGraph vs cadenas tradicionales de LangChain",
    max_steps=3
)
```

**Flujo interno**:
```
Paso 1: "Necesito buscar informaci√≥n sobre LangGraph"
        ‚Üí Busca en vectorstore ‚Üí Obtiene contexto

Paso 2: "Necesito buscar informaci√≥n sobre cadenas tradicionales"
        ‚Üí Busca en vectorstore ‚Üí Obtiene contexto

Paso 3: "Necesito comparar ambos enfoques"
        ‚Üí Busca en vectorstore ‚Üí Obtiene contexto

Final: Genera respuesta basada en todos los pasos
```

**Respuesta**:
```json
{
  "success": true,
  "question": "Compara...",
  "reasoning_steps": [
    {
      "step": 1,
      "reasoning": "Investigar caracter√≠sticas de LangGraph",
      "context": "..."
    },
    {
      "step": 2,
      "reasoning": "Investigar cadenas LangChain",
      "context": "..."
    },
    {
      "step": 3,
      "reasoning": "Comparar ambos enfoques",
      "context": "..."
    }
  ],
  "final_answer": "LangGraph ofrece... mientras que las cadenas..."
}
```

#### 5. `search_similar_documents` - B√∫squeda sem√°ntica
```python
search_similar_documents(
    query="grafos de estado",
    k=5,
    filter_metadata={"category": "documentation"}
)
```

#### 6. `get_vectorstore_stats` - Estad√≠sticas
```python
get_vectorstore_stats()
```

**Respuesta**:
```json
{
  "success": true,
  "total_documents": 45,
  "vectorstore_path": "./chroma_db",
  "embedding_model": "text-embedding-3-small",
  "llm_model": "gpt-4-turbo-preview"
}
```

## Ejemplo de uso con LLM

El LLM puede interactuar naturalmente:

**Usuario**: "Carga los documentos de la carpeta ./docs"
‚Üí Llama a `load_documents(directory_path="./docs")`

**Usuario**: "¬øQu√© es LangGraph?"
‚Üí Llama a `query_rag(question="¬øQu√© es LangGraph?")`
‚Üí **Grafo ejecuta**: Retrieve ‚Üí Prepare ‚Üí Generate

**Usuario**: "Expl√≠came paso a paso c√≥mo funciona el razonamiento multi-paso"
‚Üí Llama a `multi_step_reasoning(question="...")`
‚Üí **Grafo ejecuta**: Reasoning Step 1 ‚Üí Step 2 ‚Üí Step 3 ‚Üí Generate Final

**Usuario**: "Cu√°ntos documentos tengo?"
‚Üí Llama a `get_vectorstore_stats()`

## Ventajas de LangGraph vs Cadenas Tradicionales

### Cadenas Tradicionales (RetrievalQA)
```
Question ‚Üí Retrieve ‚Üí Format ‚Üí LLM ‚Üí Answer
(Flujo lineal fijo)
```

### LangGraph
```
          ‚îå‚îÄ‚Üí Route A ‚îÄ‚Üí Process ‚îÄ‚îê
Question ‚îÄ‚î§              ‚Üì         ‚îú‚îÄ‚Üí Answer
          ‚îî‚îÄ‚Üí Route B ‚îÄ‚Üí Enhance ‚îÄ‚îò
(Flujo din√°mico con condicionales)
```

**Ventajas de LangGraph**:
- ‚úÖ Flujos condicionales basados en estado
- ‚úÖ Ciclos y repeticiones (ej: retry, refinement)
- ‚úÖ Paralelizaci√≥n de nodos
- ‚úÖ Estado compartido entre nodos
- ‚úÖ Debugging m√°s f√°cil (cada nodo es una funci√≥n)
- ‚úÖ Composici√≥n de grafos complejos

## Estructura del Proyecto

```
mcp-rag-langchain/
‚îú‚îÄ‚îÄ server.py              # Servidor MCP con LangGraph
‚îú‚îÄ‚îÄ requirements.txt       # Dependencias (incluye langgraph)
‚îú‚îÄ‚îÄ docker-compose.yml     # PostgreSQL opcional
‚îú‚îÄ‚îÄ .env.example          # Variables de entorno
‚îú‚îÄ‚îÄ documents/            # Documentos fuente
‚îÇ   ‚îî‚îÄ‚îÄ example.txt
‚îú‚îÄ‚îÄ chroma_db/           # Vector store (generado)
‚îî‚îÄ‚îÄ README.md
```

## Modelos OpenAI Recomendados

### Para Embeddings:
```bash
# M√°s econ√≥mico y r√°pido
EMBEDDING_MODEL=text-embedding-3-small

# Mejor calidad
EMBEDDING_MODEL=text-embedding-3-large
```

### Para LLM:
```bash
# Mejor calidad/velocidad
OPENAI_MODEL=gpt-4-turbo-preview

# M√°s econ√≥mico
OPENAI_MODEL=gpt-3.5-turbo

# M√°xima calidad
OPENAI_MODEL=gpt-4
```

## Personalizaci√≥n del Grafo RAG

Puedes modificar el grafo en `server.py`:

```python
def create_rag_graph() -> StateGraph:
    """Crea el grafo de estado para RAG"""
    workflow = StateGraph(RAGState)

    # Agregar m√°s nodos
    workflow.add_node("retrieve", retrieve_documents)
    workflow.add_node("rerank", rerank_documents)  # Nuevo
    workflow.add_node("prepare_context", prepare_context)
    workflow.add_node("generate", generate_answer)

    # Modificar flujo
    workflow.set_entry_point("retrieve")
    workflow.add_edge("retrieve", "rerank")  # Nuevo
    workflow.add_edge("rerank", "prepare_context")
    workflow.add_edge("prepare_context", "generate")
    workflow.add_edge("generate", END)

    return workflow.compile()
```

## Ejemplo: Agregar Re-ranking

```python
def rerank_documents(state: RAGState) -> RAGState:
    """Nodo: Re-rankea documentos por relevancia"""
    llm = ChatOpenAI(model=OPENAI_MODEL, temperature=0)

    scored_docs = []
    for doc in state["documents"]:
        prompt = f"Del 1-10, qu√© tan relevante es este documento para: {state['question']}\n\n{doc.page_content}\n\nPuntaje:"
        score = llm.invoke(prompt)
        scored_docs.append((doc, int(score.content)))

    # Ordenar por puntaje descendente
    scored_docs.sort(key=lambda x: x[1], reverse=True)
    state["documents"] = [doc for doc, _ in scored_docs[:3]]

    return state
```

## Costos de OpenAI

### Estimaci√≥n de costos t√≠picos:

**Embeddings** (text-embedding-3-small):
- $0.02 / 1M tokens
- 1000 documentos ‚âà $0.02

**LLM** (gpt-4-turbo-preview):
- Input: $10 / 1M tokens
- Output: $30 / 1M tokens
- 100 queries ‚âà $0.50

**Total mensual** (uso moderado): ~$10-20

### Tips para reducir costos:
- Usar `gpt-3.5-turbo` en lugar de GPT-4
- Reducir `chunk_size` y `num_sources`
- Cachear embeddings
- Usar `text-embedding-3-small`

## Troubleshooting

### Error: "OPENAI_API_KEY no est√° configurada"
```bash
# Verificar que .env existe
cat .env

# Exportar manualmente
export OPENAI_API_KEY=sk-...
python server.py
```

### Error: Rate limit OpenAI
```python
# Agregar reintentos en server.py
from openai import OpenAI
client = OpenAI(
    max_retries=3,
    timeout=60.0
)
```

### ChromaDB muy lento
```bash
# Limpiar y recrear
rm -rf chroma_db/
python server.py
# Recargar documentos
```

### Respuestas de baja calidad
```python
# Aumentar n√∫mero de fuentes
query_rag(question="...", num_sources=5)

# Usar modelo m√°s potente
OPENAI_MODEL=gpt-4
```

## Seguridad

‚ö†Ô∏è **IMPORTANTE**:

- Nunca commitear `.env` con tu API key
- Usar variables de entorno en producci√≥n
- Implementar rate limiting
- Validar inputs del usuario
- Monitorear costos de OpenAI

## Ejemplos de Grafos Avanzados

### 1. Grafo con Retry
```python
def should_retry(state):
    if state["answer_quality"] < 0.7:
        return "retrieve"  # Volver a buscar
    return END

workflow.add_conditional_edges("generate", should_retry)
```

### 2. Grafo con Paralelizaci√≥n
```python
# Buscar en m√∫ltiples fuentes en paralelo
workflow.add_node("retrieve_docs", retrieve_documents)
workflow.add_node("retrieve_web", retrieve_web_results)

# Ambos nodos se ejecutan en paralelo
workflow.set_entry_point("retrieve_docs")
workflow.set_entry_point("retrieve_web")

workflow.add_edge(["retrieve_docs", "retrieve_web"], "merge")
```

## Casos de Uso

### 1. Knowledge Base Empresarial
```python
# Cargar toda la documentaci√≥n
load_documents("./company_docs", ["pdf", "md", "txt"])

# Consultar con contexto
query_rag("¬øCu√°l es la pol√≠tica de vacaciones?")
```

### 2. An√°lisis Multi-documento
```python
# Pregunta compleja que requiere m√∫ltiples pasos
multi_step_reasoning(
    "Compara las pol√≠ticas de privacidad de nuestros productos A, B y C",
    max_steps=4
)
```

### 3. Chatbot Conversacional
```python
# Mantener historial de conversaci√≥n
history = []
response1 = query_rag("¬øQu√© es LangGraph?", chat_history=history)
history.append({"role": "user", "content": "¬øQu√© es LangGraph?"})
history.append({"role": "assistant", "content": response1["answer"]})

response2 = query_rag("¬øCu√°les son sus ventajas?", chat_history=history)
```

## Referencias

- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)
- [LangChain Documentation](https://python.langchain.com/)
- [OpenAI API](https://platform.openai.com/docs)
- [ChromaDB](https://www.trychroma.com/)
- [FastMCP](https://github.com/jlowin/fastmcp)

## Pr√≥ximos Pasos

- [ ] Implementar re-ranking de documentos
- [ ] Agregar soporte para im√°genes (GPT-4 Vision)
- [ ] Implementar cache de embeddings
- [ ] Agregar m√©tricas de calidad de respuestas
- [ ] Soporte para streaming de respuestas
- [ ] Integraci√≥n con bases de datos vectoriales en la nube

---

**¬°LangGraph + OpenAI = RAG Profesional! üöÄ**
