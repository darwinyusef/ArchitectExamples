# Spark MLflow Learning Examples

Proyecto de aprendizaje de Apache Spark integrado con MLflow y diferentes frameworks de Machine Learning.

## Estructura del Proyecto

```
spark/
â”œâ”€â”€ notebooks/           # Jupyter notebooks con ejemplos prÃ¡cticos
â”‚   â”œâ”€â”€ 01_spark_airflow_mlflow.ipynb
â”‚   â”œâ”€â”€ 02_spark_pytorch_mlflow.ipynb
â”‚   â”œâ”€â”€ 03_spark_tensorflow_mlflow.ipynb
â”‚   â”œâ”€â”€ 04_spark_sklearn_mlflow.ipynb
â”‚   â”œâ”€â”€ 05_spark_scipy_mathematics.ipynb
â”‚   â”œâ”€â”€ 06_spark_clustering_postgres.ipynb
â”‚   â”œâ”€â”€ 07_mlflow_visualizations.ipynb
â”‚   â””â”€â”€ 08_proyecto_final_integracion.ipynb   â­ PROYECTO FINAL
â”œâ”€â”€ data/               # Datos para los ejemplos
â”‚   â”œâ”€â”€ raw/           # Datos sin procesar
â”‚   â””â”€â”€ processed/     # Datos procesados
â”œâ”€â”€ scripts/           # Scripts auxiliares
â”œâ”€â”€ config/            # Configuraciones
â”œâ”€â”€ logs/              # Logs de ejecuciÃ³n
â”œâ”€â”€ docker-compose.yml # Servicios (MLflow, Postgres, Airflow)
â””â”€â”€ requirements.txt   # Dependencias Python
```

## Notebooks Disponibles

### 1. Spark + Airflow + MLflow
- OrquestaciÃ³n de pipelines de datos con Airflow
- Tracking de experimentos con MLflow
- EjecuciÃ³n distribuida con Spark

### 2. Spark + PyTorch + MLflow
- Deep Learning distribuido con PyTorch y Spark
- Entrenamiento de redes neuronales
- Versionado de modelos con MLflow

### 3. Spark + TensorFlow + MLflow
- IntegraciÃ³n de TensorFlow con Spark
- Procesamiento de datos a escala
- Registro de mÃ©tricas y modelos

### 4. Spark + Scikit-learn + MLflow
- Machine Learning clÃ¡sico con Spark ML y Sklearn
- Pipelines de ML distribuidos
- ComparaciÃ³n de modelos

### 5. Spark + SciPy + MatemÃ¡ticas para ML
- Operaciones matemÃ¡ticas avanzadas
- OptimizaciÃ³n y estadÃ­stica
- Ãlgebra lineal distribuida

### 6. Spark + Clustering + PostgreSQL
- Modelos de clustering (K-Means, DBSCAN, etc.)
- Almacenamiento en PostgreSQL
- AnÃ¡lisis de grandes volÃºmenes de datos

### 7. MLflow + Visualizaciones (Matplotlib & Seaborn)
- IntegraciÃ³n de grÃ¡ficos con MLflow
- Visualizaciones estÃ¡ticas e interactivas
- Confusion matrix, ROC curves, Feature importance
- Dashboards con Plotly
- Best practices de visualizaciÃ³n en ML

### 8. â­ PROYECTO FINAL: Sistema de DetecciÃ³n de Fraude End-to-End
**IntegraciÃ³n completa de todas las tecnologÃ­as**
- âœ… Spark (procesamiento distribuido)
- âœ… MLflow (tracking y gestiÃ³n de modelos)
- âœ… Airflow (orquestaciÃ³n de pipelines)
- âœ… Scikit-learn (modelos de ML)
- âœ… SciPy (anÃ¡lisis estadÃ­stico)
- âœ… PostgreSQL (almacenamiento de datos)
- âœ… Parquet (formato de datos eficiente)
- âœ… Matplotlib/Seaborn (visualizaciones)
- âœ… MLOps (best practices de producciÃ³n)

**Pipeline completo de 13 pasos:**
1. Setup e inicializaciÃ³n
2. GeneraciÃ³n de datos sintÃ©ticos
3. Almacenamiento en PostgreSQL
4. Procesamiento con Spark y Parquet
5. AnÃ¡lisis exploratorio con SciPy
6. Feature engineering
7. PreparaciÃ³n de datos para ML
8. Entrenamiento de modelos
9. ComparaciÃ³n y selecciÃ³n
10. EvaluaciÃ³n detallada
11. Guardado de resultados
12. Resumen y MLOps
13. AutomatizaciÃ³n con Airflow

ğŸ“– Ver [PROYECTO_FINAL.md](PROYECTO_FINAL.md) para documentaciÃ³n completa

## InstalaciÃ³n

### Requisitos Previos
- Python 3.8+
- Java 8 o 11
- Docker y Docker Compose (opcional, para servicios)

### InstalaciÃ³n de Dependencias

```bash
pip install -r requirements.txt
```

### Iniciar Servicios con Docker

```bash
docker-compose up -d
```

Esto iniciarÃ¡:
- MLflow UI: http://localhost:5000
- PostgreSQL: localhost:5432
- Airflow: http://localhost:8080

## Uso

1. Iniciar Jupyter Notebook:
```bash
jupyter notebook
```

2. Navegar a la carpeta `notebooks/`

3. Abrir cualquier notebook y ejecutar las celdas secuencialmente

## ConfiguraciÃ³n de Spark

Los notebooks estÃ¡n configurados para ejecutarse en modo local por defecto. Para usar un cluster:

```python
spark = SparkSession.builder \
    .master("spark://master:7077") \
    .appName("MyApp") \
    .getOrCreate()
```

## MLflow Tracking

Para visualizar experimentos:

```bash
mlflow ui --backend-store-uri sqlite:///mlflow.db
```

## Notas

- Cada notebook es independiente y puede ejecutarse por separado
- Los datos de ejemplo se generan automÃ¡ticamente en cada notebook
- Los modelos se guardan en la carpeta `mlruns/`
- Los logs de Spark se almacenan en `logs/`

## Recursos Adicionales

- [Apache Spark Documentation](https://spark.apache.org/docs/latest/)
- [MLflow Documentation](https://mlflow.org/docs/latest/index.html)
- [Airflow Documentation](https://airflow.apache.org/docs/)

## Autor

Yusef GonzÃ¡lez - Proyecto de aprendizaje Spark + MLflow
