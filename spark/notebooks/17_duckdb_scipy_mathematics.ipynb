{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 17 - DuckDB + SciPy: Matem√°ticas y Series de Tiempo B√°sicas\n",
    "\n",
    "## üéØ Objetivos\n",
    "- Operaciones matem√°ticas con SciPy\n",
    "- Optimizaci√≥n num√©rica\n",
    "- Estad√≠stica inferencial\n",
    "- An√°lisis de series temporales b√°sicas\n",
    "- Integraci√≥n y diferenciaci√≥n num√©rica\n",
    "- Procesamiento de se√±ales\n",
    "- An√°lisis con DuckDB\n",
    "\n",
    "## üìö Tecnolog√≠as\n",
    "- **DuckDB**: Procesamiento y an√°lisis de datos\n",
    "- **SciPy**: Computaci√≥n cient√≠fica\n",
    "- **NumPy**: Operaciones num√©ricas\n",
    "- **Pandas**: Manipulaci√≥n de datos\n",
    "\n",
    "## ‚≠ê Complejidad: B√°sico/Intermedio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Instalaci√≥n y Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar dependencias\n",
    "!pip install duckdb pandas numpy scipy matplotlib seaborn plotly statsmodels -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# SciPy modules\n",
    "import scipy\n",
    "from scipy import stats\n",
    "from scipy import optimize\n",
    "from scipy import integrate\n",
    "from scipy import interpolate\n",
    "from scipy import signal\n",
    "from scipy import fft\n",
    "from scipy.stats import norm, t, chi2, f_oneway, pearsonr, spearmanr\n",
    "\n",
    "# Config\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(f\"‚úÖ SciPy version: {scipy.__version__}\")\n",
    "print(f\"‚úÖ NumPy version: {np.__version__}\")\n",
    "print(f\"‚úÖ DuckDB version: {duckdb.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Estad√≠stica B√°sica con SciPy y DuckDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conectar a DuckDB\n",
    "con = duckdb.connect(':memory:')\n",
    "\n",
    "# Generar datos sint√©ticos\n",
    "np.random.seed(42)\n",
    "\n",
    "# Ventas de diferentes tiendas\n",
    "n_samples = 1000\n",
    "data = {\n",
    "    'store_id': np.repeat(['Store_A', 'Store_B', 'Store_C', 'Store_D'], n_samples // 4),\n",
    "    'sales': np.concatenate([\n",
    "        np.random.normal(1000, 200, n_samples // 4),  # Store A\n",
    "        np.random.normal(1200, 180, n_samples // 4),  # Store B\n",
    "        np.random.normal(950, 250, n_samples // 4),   # Store C\n",
    "        np.random.normal(1100, 190, n_samples // 4)   # Store D\n",
    "    ]),\n",
    "    'customers': np.random.poisson(50, n_samples),\n",
    "    'avg_ticket': np.random.lognormal(3, 0.5, n_samples)\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(f\"üìä Dataset generado: {len(df)} registros\")\n",
    "print(f\"\\nüìä Primeras filas:\")\n",
    "print(df.head())\n",
    "\n",
    "# Estad√≠sticas con DuckDB\n",
    "stats_db = con.execute(\"\"\"\n",
    "    SELECT \n",
    "        store_id,\n",
    "        COUNT(*) as n,\n",
    "        ROUND(AVG(sales), 2) as mean_sales,\n",
    "        ROUND(STDDEV(sales), 2) as std_sales,\n",
    "        ROUND(MIN(sales), 2) as min_sales,\n",
    "        ROUND(MAX(sales), 2) as max_sales,\n",
    "        ROUND(MEDIAN(sales), 2) as median_sales\n",
    "    FROM df\n",
    "    GROUP BY store_id\n",
    "    ORDER BY mean_sales DESC\n",
    "\"\"\").df()\n",
    "\n",
    "print(f\"\\nüìä Estad√≠sticas por tienda (DuckDB):\")\n",
    "print(stats_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tests Estad√≠sticos con SciPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de normalidad\n",
    "print(\"üìä TEST DE NORMALIDAD (Shapiro-Wilk)\\n\")\n",
    "\n",
    "for store in df['store_id'].unique():\n",
    "    sales = df[df['store_id'] == store]['sales']\n",
    "    stat, p_value = stats.shapiro(sales)\n",
    "    \n",
    "    is_normal = \"‚úÖ Normal\" if p_value > 0.05 else \"‚ùå No normal\"\n",
    "    print(f\"{store}: W={stat:.4f}, p-value={p_value:.4f} {is_normal}\")\n",
    "\n",
    "# Test de varianza (Levene)\n",
    "print(f\"\\nüìä TEST DE VARIANZA HOMOG√âNEA (Levene)\\n\")\n",
    "\n",
    "stores_data = [df[df['store_id'] == store]['sales'].values for store in df['store_id'].unique()]\n",
    "stat, p_value = stats.levene(*stores_data)\n",
    "\n",
    "print(f\"Estad√≠stico: {stat:.4f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")\n",
    "print(f\"Conclusi√≥n: {'Varianzas homog√©neas ‚úÖ' if p_value > 0.05 else 'Varianzas heterog√©neas ‚ùå'}\")\n",
    "\n",
    "# ANOVA\n",
    "print(f\"\\nüìä AN√ÅLISIS DE VARIANZA (ANOVA)\\n\")\n",
    "\n",
    "f_stat, p_value = f_oneway(*stores_data)\n",
    "\n",
    "print(f\"F-estad√≠stico: {f_stat:.4f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")\n",
    "print(f\"Conclusi√≥n: {'Medias significativamente diferentes ‚úÖ' if p_value < 0.05 else 'No hay diferencia significativa ‚ùå'}\")\n",
    "\n",
    "# Test T de dos muestras\n",
    "print(f\"\\nüìä TEST T (Store A vs Store B)\\n\")\n",
    "\n",
    "store_a = df[df['store_id'] == 'Store_A']['sales']\n",
    "store_b = df[df['store_id'] == 'Store_B']['sales']\n",
    "\n",
    "t_stat, p_value = stats.ttest_ind(store_a, store_b)\n",
    "\n",
    "print(f\"t-estad√≠stico: {t_stat:.4f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")\n",
    "print(f\"Conclusi√≥n: {'Medias diferentes ‚úÖ' if p_value < 0.05 else 'Medias similares ‚ùå'}\")\n",
    "\n",
    "# Correlaci√≥n\n",
    "print(f\"\\nüìä CORRELACIONES\\n\")\n",
    "\n",
    "# Pearson\n",
    "corr_p, p_value_p = pearsonr(df['sales'], df['customers'])\n",
    "print(f\"Pearson (sales vs customers): r={corr_p:.4f}, p={p_value_p:.4f}\")\n",
    "\n",
    "# Spearman\n",
    "corr_s, p_value_s = spearmanr(df['sales'], df['customers'])\n",
    "print(f\"Spearman (sales vs customers): œÅ={corr_s:.4f}, p={p_value_s:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Optimizaci√≥n Num√©rica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo: Optimizar precio para maximizar ingresos\n",
    "# Modelo: Revenue = price * demand(price)\n",
    "# donde demand(price) = 1000 - 5 * price\n",
    "\n",
    "def revenue(price):\n",
    "    \"\"\"Funci√≥n de ingresos a maximizar\"\"\"\n",
    "    demand = 1000 - 5 * price\n",
    "    return -(price * demand)  # Negativo porque minimize busca m√≠nimo\n",
    "\n",
    "# Optimizar\n",
    "result = optimize.minimize_scalar(revenue, bounds=(0, 200), method='bounded')\n",
    "\n",
    "optimal_price = result.x\n",
    "max_revenue = -result.fun\n",
    "optimal_demand = 1000 - 5 * optimal_price\n",
    "\n",
    "print(\"üìä OPTIMIZACI√ìN DE PRECIO\\n\")\n",
    "print(f\"Precio √≥ptimo: ${optimal_price:.2f}\")\n",
    "print(f\"Demanda esperada: {optimal_demand:.0f} unidades\")\n",
    "print(f\"Ingresos m√°ximos: ${max_revenue:,.2f}\")\n",
    "\n",
    "# Visualizar\n",
    "prices = np.linspace(0, 200, 100)\n",
    "revenues = [price * (1000 - 5 * price) for price in prices]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(prices, revenues, linewidth=2, label='Ingresos')\n",
    "plt.axvline(optimal_price, color='r', linestyle='--', label=f'Precio √≥ptimo: ${optimal_price:.2f}')\n",
    "plt.axhline(max_revenue, color='g', linestyle='--', alpha=0.5, label=f'Ingresos m√°x: ${max_revenue:,.0f}')\n",
    "plt.xlabel('Precio ($)')\n",
    "plt.ylabel('Ingresos ($)')\n",
    "plt.title('Optimizaci√≥n de Precio')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Ejemplo 2: Ajuste de curva\n",
    "print(f\"\\nüìä AJUSTE DE CURVA (Curve Fitting)\\n\")\n",
    "\n",
    "# Datos de ejemplo\n",
    "x_data = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "y_data = np.array([1.0, 2.3, 4.2, 7.1, 11.5, 17.8, 25.9, 35.2, 46.3, 59.0, 73.1])\n",
    "\n",
    "# Funci√≥n cuadr√°tica\n",
    "def quadratic(x, a, b, c):\n",
    "    return a * x**2 + b * x + c\n",
    "\n",
    "# Ajustar\n",
    "params, covariance = optimize.curve_fit(quadratic, x_data, y_data)\n",
    "\n",
    "print(f\"Par√°metros ajustados:\")\n",
    "print(f\"   a = {params[0]:.4f}\")\n",
    "print(f\"   b = {params[1]:.4f}\")\n",
    "print(f\"   c = {params[2]:.4f}\")\n",
    "\n",
    "# Visualizar\n",
    "x_fit = np.linspace(0, 10, 100)\n",
    "y_fit = quadratic(x_fit, *params)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(x_data, y_data, s=100, alpha=0.6, label='Datos reales')\n",
    "plt.plot(x_fit, y_fit, 'r-', linewidth=2, label=f'Ajuste: {params[0]:.2f}x¬≤ + {params[1]:.2f}x + {params[2]:.2f}')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Ajuste de Curva Cuadr√°tica')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Integraci√≥n y Diferenciaci√≥n Num√©rica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integraci√≥n num√©rica\n",
    "print(\"üìä INTEGRACI√ìN NUM√âRICA\\n\")\n",
    "\n",
    "# Funci√≥n a integrar: f(x) = x^2\n",
    "def f(x):\n",
    "    return x**2\n",
    "\n",
    "# Integral definida de 0 a 3\n",
    "result, error = integrate.quad(f, 0, 3)\n",
    "\n",
    "print(f\"‚à´‚ÇÄ¬≥ x¬≤ dx = {result:.4f} (error: {error:.2e})\")\n",
    "print(f\"Valor anal√≠tico: {3**3 / 3:.4f}\")\n",
    "\n",
    "# Integraci√≥n de datos discretos\n",
    "x = np.linspace(0, 10, 100)\n",
    "y = np.sin(x)\n",
    "\n",
    "# M√©todo trapecio\n",
    "integral_trapz = integrate.trapezoid(y, x)\n",
    "# M√©todo Simpson\n",
    "integral_simps = integrate.simpson(y, x)\n",
    "\n",
    "print(f\"\\nIntegraci√≥n de sin(x) de 0 a 10:\")\n",
    "print(f\"   Trapecio: {integral_trapz:.4f}\")\n",
    "print(f\"   Simpson: {integral_simps:.4f}\")\n",
    "print(f\"   Anal√≠tico: {-np.cos(10) + np.cos(0):.4f}\")\n",
    "\n",
    "# Diferenciaci√≥n num√©rica\n",
    "print(f\"\\nüìä DIFERENCIACI√ìN NUM√âRICA\\n\")\n",
    "\n",
    "# Calcular derivada de sin(x)\n",
    "dx = x[1] - x[0]\n",
    "dy_dx = np.gradient(y, dx)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x, y, label='f(x) = sin(x)')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Funci√≥n Original')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x, dy_dx, label=\"f'(x) num√©rica\", linewidth=2)\n",
    "plt.plot(x, np.cos(x), '--', label=\"f'(x) = cos(x) anal√≠tica\", linewidth=2)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel(\"y'\")\n",
    "plt.title('Derivada')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Series de Tiempo B√°sicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar serie temporal sint√©tica\n",
    "np.random.seed(42)\n",
    "\n",
    "n_days = 365\n",
    "dates = pd.date_range(start='2024-01-01', periods=n_days, freq='D')\n",
    "\n",
    "# Componentes\n",
    "trend = np.linspace(100, 150, n_days)\n",
    "seasonal = 20 * np.sin(2 * np.pi * np.arange(n_days) / 365)\n",
    "noise = np.random.normal(0, 5, n_days)\n",
    "\n",
    "sales_ts = trend + seasonal + noise\n",
    "\n",
    "ts_df = pd.DataFrame({\n",
    "    'date': dates,\n",
    "    'sales': sales_ts,\n",
    "    'day_of_week': dates.dayofweek,\n",
    "    'month': dates.month,\n",
    "    'day_of_year': dates.dayofyear\n",
    "})\n",
    "\n",
    "print(f\"üìä Serie temporal generada: {len(ts_df)} d√≠as\")\n",
    "print(f\"\\nüìä Estad√≠sticas:\")\n",
    "print(ts_df['sales'].describe())\n",
    "\n",
    "# An√°lisis con DuckDB\n",
    "monthly_stats = con.execute(\"\"\"\n",
    "    SELECT \n",
    "        month,\n",
    "        COUNT(*) as days,\n",
    "        ROUND(AVG(sales), 2) as avg_sales,\n",
    "        ROUND(MIN(sales), 2) as min_sales,\n",
    "        ROUND(MAX(sales), 2) as max_sales,\n",
    "        ROUND(STDDEV(sales), 2) as std_sales\n",
    "    FROM ts_df\n",
    "    GROUP BY month\n",
    "    ORDER BY month\n",
    "\"\"\").df()\n",
    "\n",
    "print(f\"\\nüìä Estad√≠sticas mensuales:\")\n",
    "print(monthly_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. An√°lisis Espectral con FFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformada de Fourier para detectar periodicidades\n",
    "print(\"üìä AN√ÅLISIS DE FRECUENCIAS (FFT)\\n\")\n",
    "\n",
    "# FFT\n",
    "fft_values = fft.fft(sales_ts)\n",
    "fft_freq = fft.fftfreq(len(sales_ts), d=1)  # frecuencias (1 d√≠a)\n",
    "\n",
    "# Magnitud\n",
    "fft_magnitude = np.abs(fft_values)\n",
    "\n",
    "# Solo frecuencias positivas\n",
    "positive_freq_idx = fft_freq > 0\n",
    "\n",
    "# Encontrar picos\n",
    "peaks, _ = signal.find_peaks(fft_magnitude[positive_freq_idx], height=100)\n",
    "\n",
    "print(f\"Periodicidades detectadas:\")\n",
    "for peak in peaks[:5]:  # Top 5\n",
    "    freq = fft_freq[positive_freq_idx][peak]\n",
    "    period = 1 / freq\n",
    "    magnitude = fft_magnitude[positive_freq_idx][peak]\n",
    "    print(f\"   Per√≠odo: {period:.1f} d√≠as (magnitud: {magnitude:.2f})\")\n",
    "\n",
    "# Visualizar\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "# Serie temporal\n",
    "axes[0].plot(ts_df['date'], ts_df['sales'], linewidth=1)\n",
    "axes[0].set_title('Serie Temporal Original')\n",
    "axes[0].set_xlabel('Fecha')\n",
    "axes[0].set_ylabel('Ventas')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Espectro de frecuencias\n",
    "axes[1].plot(fft_freq[positive_freq_idx], fft_magnitude[positive_freq_idx], linewidth=1)\n",
    "axes[1].set_title('Espectro de Frecuencias')\n",
    "axes[1].set_xlabel('Frecuencia (1/d√≠a)')\n",
    "axes[1].set_ylabel('Magnitud')\n",
    "axes[1].set_xlim(0, 0.1)  # Zoom en frecuencias bajas\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Suavizado y Filtrado de Se√±ales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving average con SciPy\n",
    "window_size = 7\n",
    "weights = np.ones(window_size) / window_size\n",
    "sales_ma = np.convolve(sales_ts, weights, mode='valid')\n",
    "\n",
    "# Suavizado Savitzky-Golay\n",
    "sales_sg = signal.savgol_filter(sales_ts, window_length=21, polyorder=3)\n",
    "\n",
    "# Filtro Gaussiano\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "sales_gaussian = gaussian_filter1d(sales_ts, sigma=3)\n",
    "\n",
    "# Visualizar\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "plt.plot(ts_df['date'], sales_ts, alpha=0.5, label='Original', linewidth=1)\n",
    "plt.plot(ts_df['date'][window_size-1:], sales_ma, label=f'Moving Average ({window_size} d√≠as)', linewidth=2)\n",
    "plt.plot(ts_df['date'], sales_sg, label='Savitzky-Golay', linewidth=2)\n",
    "plt.plot(ts_df['date'], sales_gaussian, label='Gaussiano', linewidth=2)\n",
    "\n",
    "plt.title('T√©cnicas de Suavizado')\n",
    "plt.xlabel('Fecha')\n",
    "plt.ylabel('Ventas')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Comparaci√≥n de suavizado:\")\n",
    "print(f\"   Original - Std: {np.std(sales_ts):.2f}\")\n",
    "print(f\"   Moving Avg - Std: {np.std(sales_ma):.2f}\")\n",
    "print(f\"   Savitzky-Golay - Std: {np.std(sales_sg):.2f}\")\n",
    "print(f\"   Gaussiano - Std: {np.std(sales_gaussian):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Interpolaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simular datos con valores faltantes\n",
    "x_sparse = np.array([0, 30, 60, 90, 120, 150, 180, 210, 240, 270, 300, 330, 364])\n",
    "y_sparse = sales_ts[x_sparse]\n",
    "\n",
    "# Diferentes m√©todos de interpolaci√≥n\n",
    "x_dense = np.arange(365)\n",
    "\n",
    "# Lineal\n",
    "f_linear = interpolate.interp1d(x_sparse, y_sparse, kind='linear')\n",
    "y_linear = f_linear(x_dense)\n",
    "\n",
    "# C√∫bica\n",
    "f_cubic = interpolate.interp1d(x_sparse, y_sparse, kind='cubic')\n",
    "y_cubic = f_cubic(x_dense)\n",
    "\n",
    "# Spline\n",
    "tck = interpolate.splrep(x_sparse, y_sparse, s=0)\n",
    "y_spline = interpolate.splev(x_dense, tck)\n",
    "\n",
    "# Visualizar\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.scatter(x_sparse, y_sparse, s=100, c='red', zorder=3, label='Datos originales (dispersos)')\n",
    "plt.plot(x_dense, sales_ts, 'k--', alpha=0.3, label='Serie completa (referencia)', linewidth=1)\n",
    "plt.plot(x_dense, y_linear, label='Interpolaci√≥n Lineal', linewidth=2)\n",
    "plt.plot(x_dense, y_cubic, label='Interpolaci√≥n C√∫bica', linewidth=2)\n",
    "plt.plot(x_dense, y_spline, label='Spline', linewidth=2)\n",
    "\n",
    "plt.title('M√©todos de Interpolaci√≥n')\n",
    "plt.xlabel('D√≠a del a√±o')\n",
    "plt.ylabel('Ventas')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Errores de interpolaci√≥n\n",
    "print(\"üìä Error de interpolaci√≥n (RMSE vs serie completa):\")\n",
    "print(f\"   Lineal: {np.sqrt(np.mean((y_linear - sales_ts)**2)):.2f}\")\n",
    "print(f\"   C√∫bica: {np.sqrt(np.mean((y_cubic - sales_ts)**2)):.2f}\")\n",
    "print(f\"   Spline: {np.sqrt(np.mean((y_spline - sales_ts)**2)):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Descomposici√≥n de Series Temporales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# Crear serie con √≠ndice de fecha\n",
    "ts_series = pd.Series(sales_ts, index=dates)\n",
    "\n",
    "# Descomponer\n",
    "decomposition = seasonal_decompose(ts_series, model='additive', period=30)\n",
    "\n",
    "# Visualizar\n",
    "fig, axes = plt.subplots(4, 1, figsize=(14, 10))\n",
    "\n",
    "decomposition.observed.plot(ax=axes[0], title='Serie Original')\n",
    "axes[0].set_ylabel('Observado')\n",
    "\n",
    "decomposition.trend.plot(ax=axes[1], title='Tendencia')\n",
    "axes[1].set_ylabel('Tendencia')\n",
    "\n",
    "decomposition.seasonal.plot(ax=axes[2], title='Estacionalidad')\n",
    "axes[2].set_ylabel('Estacional')\n",
    "\n",
    "decomposition.resid.plot(ax=axes[3], title='Residuos')\n",
    "axes[3].set_ylabel('Residuos')\n",
    "axes[3].set_xlabel('Fecha')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Estad√≠sticas de componentes\n",
    "print(\"üìä Estad√≠sticas de componentes:\")\n",
    "print(f\"   Tendencia - Rango: [{decomposition.trend.min():.2f}, {decomposition.trend.max():.2f}]\")\n",
    "print(f\"   Estacional - Amplitud: {decomposition.seasonal.max() - decomposition.seasonal.min():.2f}\")\n",
    "print(f\"   Residuos - Std: {decomposition.resid.std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. An√°lisis con DuckDB: Queries Avanzados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregar componentes descompuestos a DataFrame\n",
    "ts_df['trend'] = decomposition.trend\n",
    "ts_df['seasonal'] = decomposition.seasonal\n",
    "ts_df['residual'] = decomposition.resid\n",
    "\n",
    "# An√°lisis con SQL\n",
    "trend_analysis = con.execute(\"\"\"\n",
    "    WITH monthly_trends AS (\n",
    "        SELECT \n",
    "            month,\n",
    "            AVG(sales) as avg_sales,\n",
    "            AVG(trend) as avg_trend,\n",
    "            AVG(seasonal) as avg_seasonal,\n",
    "            STDDEV(residual) as std_residual\n",
    "        FROM ts_df\n",
    "        WHERE trend IS NOT NULL\n",
    "        GROUP BY month\n",
    "    )\n",
    "    SELECT \n",
    "        month,\n",
    "        ROUND(avg_sales, 2) as avg_sales,\n",
    "        ROUND(avg_trend, 2) as avg_trend,\n",
    "        ROUND(avg_seasonal, 2) as avg_seasonal,\n",
    "        ROUND(std_residual, 2) as volatility,\n",
    "        ROUND((avg_trend - LAG(avg_trend) OVER (ORDER BY month)) / LAG(avg_trend) OVER (ORDER BY month) * 100, 2) as trend_growth_pct\n",
    "    FROM monthly_trends\n",
    "    ORDER BY month\n",
    "\"\"\").df()\n",
    "\n",
    "print(\"üìä An√°lisis de tendencias mensuales:\")\n",
    "print(trend_analysis)\n",
    "\n",
    "# Detectar anomal√≠as con SQL\n",
    "anomalies = con.execute(\"\"\"\n",
    "    WITH stats AS (\n",
    "        SELECT \n",
    "            AVG(residual) as mean_resid,\n",
    "            STDDEV(residual) as std_resid\n",
    "        FROM ts_df\n",
    "        WHERE residual IS NOT NULL\n",
    "    )\n",
    "    SELECT \n",
    "        date,\n",
    "        sales,\n",
    "        residual,\n",
    "        ROUND((residual - s.mean_resid) / s.std_resid, 2) as z_score\n",
    "    FROM ts_df, stats s\n",
    "    WHERE ABS((residual - s.mean_resid) / s.std_resid) > 2.5\n",
    "    AND residual IS NOT NULL\n",
    "    ORDER BY ABS((residual - s.mean_resid) / s.std_resid) DESC\n",
    "\"\"\").df()\n",
    "\n",
    "print(f\"\\nüìä Anomal√≠as detectadas (|z-score| > 2.5): {len(anomalies)}\")\n",
    "if len(anomalies) > 0:\n",
    "    print(anomalies.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Resumen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéâ RESUMEN: SciPy + DuckDB para Matem√°ticas y Series Temporales\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n‚úÖ CONCEPTOS CUBIERTOS:\")\n",
    "print(\"   1. Estad√≠stica inferencial (tests t, ANOVA, correlaciones)\")\n",
    "print(\"   2. Optimizaci√≥n num√©rica (minimizaci√≥n, ajuste de curvas)\")\n",
    "print(\"   3. Integraci√≥n y diferenciaci√≥n num√©rica\")\n",
    "print(\"   4. An√°lisis de series temporales\")\n",
    "print(\"   5. An√°lisis espectral (FFT)\")\n",
    "print(\"   6. Suavizado y filtrado de se√±ales\")\n",
    "print(\"   7. Interpolaci√≥n\")\n",
    "print(\"   8. Descomposici√≥n de series\")\n",
    "print(\"   9. Detecci√≥n de anomal√≠as\")\n",
    "print(\"   10. Integraci√≥n DuckDB para an√°lisis SQL\")\n",
    "\n",
    "print(\"\\nüí° APLICACIONES PR√ÅCTICAS:\")\n",
    "print(\"   - Forecasting de demanda\")\n",
    "print(\"   - Optimizaci√≥n de precios\")\n",
    "print(\"   - Control de calidad (detecci√≥n de anomal√≠as)\")\n",
    "print(\"   - An√°lisis financiero\")\n",
    "print(\"   - Procesamiento de se√±ales\")\n",
    "print(\"   - An√°lisis de tendencias\")\n",
    "\n",
    "print(\"\\nüìö BIBLIOTECAS CLAVE:\")\n",
    "print(\"   scipy.stats - Estad√≠stica\")\n",
    "print(\"   scipy.optimize - Optimizaci√≥n\")\n",
    "print(\"   scipy.integrate - Integraci√≥n\")\n",
    "print(\"   scipy.signal - Procesamiento de se√±ales\")\n",
    "print(\"   scipy.fft - Transformada de Fourier\")\n",
    "print(\"   scipy.interpolate - Interpolaci√≥n\")\n",
    "\n",
    "con.close()\n",
    "print(\"\\n‚úÖ Conexi√≥n DuckDB cerrada\")\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
